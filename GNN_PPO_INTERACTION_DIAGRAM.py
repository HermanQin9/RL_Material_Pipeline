#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GNN + PPO 交互架构图 / GNN + PPO Interaction Architecture Diagram
展示完整的系统如何工作 / Demonstrates how the complete system works
"""

DIAGRAM_CONTENT = """

╔════════════════════════════════════════════════════════════════════════════════╗
║                                                                                ║
║           GNN + PPO 系统架构完全指南 / Complete System Architecture             ║
║                                                                                ║
╚════════════════════════════════════════════════════════════════════════════════╝


════════════════════════════════════════════════════════════════════════════════
   第1层: 输入数据 / Layer 1: Input Data
════════════════════════════════════════════════════════════════════════════════

Materials Project数据库 (4000+ 晶体)
        ⬇
    ┌─────────────────────────────┐
    │  原始数据 / Raw Data        │
    │  ├─ 晶体结构CIF文件        │
    │  ├─ 元素成分 (matminer)    │
    │  ├─ 物性数据               │
    │  └─ 形成能 (目标值)        │
    └─────────────────────────────┘
        ⬇


════════════════════════════════════════════════════════════════════════════════
   第2层: 数据预处理 / Layer 2: Data Preprocessing
════════════════════════════════════════════════════════════════════════════════

┌──────────────────────────────┐          ┌──────────────────────────────┐
│ N0 - 数据获取节点            │          │ N2 - 特征矩阵节点            │
│ (DataFetch)                  │          │ (FeatureMatrix)              │
│                              │          │                              │
│ 输入: 材料ID列表            │          │ 输入: 晶体结构               │
│ 输出: 晶体结构 + 目标值      │   ─────→ │ 输出: [n×m]特征矩阵         │
│       结构保存在内存          │          │       特征名称 + 数据        │
└──────────────────────────────┘          └──────────────────────────────┘


════════════════════════════════════════════════════════════════════════════════
   第3层: PPO+GNN核心处理 / Layer 3: PPO+GNN Core Processing
════════════════════════════════════════════════════════════════════════════════

                    ┌─────────────────────────────┐
                    │  PPO强化学习代理            │
                    │  (Reinforcement Learning)   │
                    │                             │
                    │  ┌────────────────────────┐│
                    │  │ 策略网络 (Policy Net)  ││
                    │  │ - 输入: 状态向量      ││
                    │  │ - 输出: 动作概率      ││
                    │  └────────────────────────┘│
                    │                             │
                    │  ┌────────────────────────┐│
                    │  │ 价值网络 (Value Net)   ││
                    │  │ - 输入: 状态向量      ││
                    │  │ - 输出: 状态价值      ││
                    │  └────────────────────────┘│
                    └────────┬────────────────────┘
                             │ 
                    决策动作 / Decision
                    {method, param}
                             ⬇
        ┌─────────────────────────────────────────┐
        │        N4 - GNN处理节点                  │
        │        (GNN Processing)                  │
        │                                         │
        │  接收PPO决策:                           │
        │  ├─ method ∈ {0:GCN, 1:GAT, 2:SAGE}   │
        │  └─ param ∈ [0,1] → dim ∈ {8,16,32}   │
        │                                         │
        │  执行步骤:                              │
        │  1️⃣ 晶体结构 → 图表示                   │
        │     ├─ 节点: 各原子                    │
        │     ├─ 节点特征: [原子号, 半径, 电负性]│
        │     └─ 边: 距离<5.0Å的原子间          │
        │                                         │
        │  2️⃣ 初始化GNN模型                      │
        │     ├─ GCN:       Simple, Fast       │
        │     ├─ GAT:       Accurate, Slow    │
        │     └─ GraphSAGE: Scalable, Medium  │
        │                                         │
        │  3️⃣ 前向传播                           │
        │     └─ 每原子生成[8/16/32]维嵌入      │
        │                                         │
        │  4️⃣ 全局池化                           │
        │     └─ 获取[8/16/32]维结构特征        │
        │                                         │
        │  输出:                                 │
        │  ├─ 新特征矩阵 [n × (m + dim)]       │
        │  ├─ 处理时间 (ms/样本)                │
        │  └─ GNN信息 (元数据)                  │
        └────────────┬──────────────────────────┘
                     ⬇
        ┌─────────────────────────────────────────┐
        │        其他处理节点 / Other Nodes        │
        │                                         │
        │  N1 - 缺失值处理 (Imputation)         │
        │  N3 - 数据清理 (Cleaning)             │
        │  N5 - 知识图谱 (Knowledge Graph)      │
        │  N6 - 特征选择 (Feature Selection)    │
        │  N7 - 数据归一化 (Scaling)            │
        │                                         │
        │  PPO灵活安排这些节点的顺序和方法      │
        │  (PPO flexibly arranges order/methods) │
        └─────────────┬──────────────────────────┘
                      ⬇
        ┌─────────────────────────────────────────┐
        │        N8 - 模型训练节点                │
        │        (Model Training)                 │
        │                                         │
        │  使用增强特征训练:                     │
        │  ├─ Random Forest (RF)                │
        │  ├─ Gradient Boosting (GBR)           │
        │  ├─ XGBoost (XGB)                     │
        │  └─ CatBoost (CAT)                    │
        │                                         │
        │  输出: 训练模型 + 验证性能            │
        │  ├─ R² (决定系数)                     │
        │  ├─ MAE (平均绝对误差)                │
        │  └─ 其他指标                          │
        └────────────┬──────────────────────────┘
                     ⬇
        ┌─────────────────────────────────────────┐
        │  验证性能评估                          │
        │  (Performance Evaluation)              │
        │                                         │
        │  ⬜ 获取验证集上的预测                  │
        │  ⬜ 计算性能指标 (R², MAE, RMSE)      │
        │  ⬜ 记录总处理时间                    │
        │  ⬜ 生成奖励信号                      │
        │                                         │
        │  奖励计算:                             │
        │  reward = (α × ΔR²) - (β × Δtime)    │
        │         = 改进量 - 成本                 │
        └────────────┬──────────────────────────┘
                     ⬇


════════════════════════════════════════════════════════════════════════════════
   第4层: PPO学习与反馈 / Layer 4: PPO Learning & Feedback
════════════════════════════════════════════════════════════════════════════════

    ┌──────────────────────────────────────────────┐
    │  PPO学习循环                                 │
    │  (Proximal Policy Optimization Loop)        │
    │                                              │
    │  收集数据:                                   │
    │  ├─ 旧动作序列                              │
    │  ├─ 对应的奖励                              │
    │  ├─ 对数概率                                │
    │  └─ 状态价值估计                            │
    │                                              │
    │  计算优势:                                   │
    │  ├─ GAE (广义优势估计)                     │
    │  │  advantage = reward + γ*V(s') - V(s)   │
    │  │              └─ 显示动作相对于基线的优劣│
    │  │                                          │
    │  │  大优势 (>0) → 这个动作很好 ✓           │
    │  │  小优势 (≈0) → 这个动作中等 ≈           │
    │  │  负优势 (<0) → 这个动作不好 ✗           │
    │  └─ 计算每个动作的优势值                   │
    │                                              │
    │  更新策略:                                   │
    │  ├─ 使用PPO损失函数                        │
    │  │  L = min(rt * A, clip(rt, 1-ε, 1+ε)*A)│
    │  │      └─ rt = 新概率/旧概率              │
    │  │      └─ A = 优势                        │
    │  │      └─ ε = clip范围 (0.2)             │
    │  │                                          │
    │  │  目标: 最大化有利动作的概率              │
    │  │       最小化有害动作的概率              │
    │  │                                          │
    │  ├─ 梯度下降优化                            │
    │  ├─ 多个epoch遍历                           │
    │  └─ 防止太大更新 (clip ratio)             │
    │                                              │
    │  更新价值网络:                              │
    │  └─ 预测状态价值，用于计算优势           │
    │                                              │
    └──────────────────────┬───────────────────────┘
                           ⬇
    ┌──────────────────────────────────────────────┐
    │  迭代更新统计                                │
    │                                              │
    │  每个PPO更新后:                              │
    │  ├─ 策略变化指标 (Policy Entropy)            │
    │  ├─ 累积奖励 (Average Reward)                │
    │  ├─ 价值估计误差 (Value Loss)                │
    │  ├─ 策略网络损失 (Policy Loss)               │
    │  └─ 剪切比例 (Clip Fraction)                │
    │      └─ 检测是否更新过大                    │
    │                                              │
    │  可视化学习过程:                            │
    │  ✓ 奖励应该逐步上升                        │
    │  ✓ 损失应该逐步下降                        │
    │  ✓ 收敛应该平稳（无剧烈波动）              │
    │                                              │
    └─────────────────────────────────────────────┘


════════════════════════════════════════════════════════════════════════════════
   流程完整示意 / Complete Flow Diagram
════════════════════════════════════════════════════════════════════════════════

 初始化 / Initialization
        │
        ⬇
 数据加载 ─→ 数据预处理 ─→ 特征提取
   N0/N2      N1/N3/N5/N6/N7
        │
        ⬇
 ╔════════════════════════════════════════════════════════════════╗
 ║                   PPO 训练循环 (Episode循环)                   ║
 ║                                                                ║
 ║  Episode = 1    重置环境 ─────────────────┐                    ║
 ║  to MaxEpisodes     ⬇                    │                    ║
 ║                 状态观察 (s)             │                    ║
 ║                     ⬇                    │                    ║
 ║  ╔═══════════════════════════════════╗   │ 轨迹收集           ║
 ║  ║    内层循环 (Step循环)            ║   │ (Trajectory)        ║
 ║  ║  Step = 0 to MaxSteps             ║   │                    ║
 ║  ║      ⬇                            ║   │                    ║
 ║  ║  PPO策略选择动作 a                ║ ──┘                    ║
 ║  ║  action = {                       ║                      ║
 ║  ║    'node': N4 (GNN)               ║                      ║
 ║  ║    'method': 0/1/2                ║  GNN执行              ║
 ║  ║    'param': 0.0~1.0               ║  ┌──────────────┐     ║
 ║  ║  }                                ║  │晶体→图→GNN→特征│     ║
 ║  ║      ⬇                            ║  └──────────────┘     ║
 ║  ║  环境执行动作 / Env.step(action)  ║        ⬇              ║
 ║  ║  - 运行GNN处理                    ║  返回新观察和奖励      ║
 ║  ║  - 训练模型 (N8)                  ║        ⬇              ║
 ║  ║  - 验证性能                       ║  s', r, done          ║
 ║  ║      ⬇                            ║        ⬇              ║
 ║  ║  收集 (s, a, r, s', d)            ║  保存到缓冲区          ║
 ║  ║      ⬇                            ║        ⬇              ║
 ║  ║  如果完成或达到步数限制 → 结束    ║  done=True? 结束      ║
 ║  ║                                   ║      内层循环          ║
 ║  ╚═══════════════════════════════════╝                      ║
 ║                     ⬇                                        ║
 ║  ┌─────────────────────────────────────┐                    ║
 ║  │ PPO策略更新 (Update Network)        │                    ║
 ║  │ 基于整个Episode的轨迹                │                    ║
 ║  │ 1. 计算优势函数 (GAE)              │                    ║
 ║  │ 2. 更新策略网络                     │                    ║
 ║  │ 3. 更新价值网络                     │                    ║
 ║  │ 4. 记录指标                        │                    ║
 ║  └─────────────────────────────────────┘                    ║
 ║                     ⬇                                        ║
 ║  是否达到MaxEpisodes? 否 → 下一个Episode                     ║
 ║  (需要达到MaxEpisodes) 是 → 训练完成 ✓                      ║
 ║                                                              ║
 ║═════════════════════════════════════════════════════════════════║
        │
        ⬇
 训练完成 / Training Complete
        │
        ⬇
 ┌────────────────────────────────────────┐
 │ 学到的最优策略 (Learned Policy)       │
 │ - 什么情况下选择GCN/GAT/SAGE?        │
 │ - 最优输出维度是多少?                │
 │ - 最优的参数配置是什么?              │
 │                                      │
 │ 评估模型 (Final Evaluation)           │
 │ - 在测试集上评估最终性能             │
 │ - 对比基线方法                       │
 │ - 分析学到的策略特点                 │
 └────────────────────────────────────────┘


════════════════════════════════════════════════════════════════════════════════
   详细示例：一个完整的Episode / Detailed Example: One Complete Episode
════════════════════════════════════════════════════════════════════════════════

█ EPISODE 42 的详细过程 █

┌─ 重置环境 ─────────────────────────────────────────────────────────────┐
│ 初始状态:                                                              │
│ ├─ 特征矩阵大小: [4000, 25] (25个matminer特征)                        │
│ ├─ 验证R²: 0.820 (未使用GNN)                                         │
│ ├─ 训练进度: 0% (开始)                                                │
│ └─ 观察向量: [4000, 0.820, 0, GPU使用率, ...]                        │
└─────────────────────────────────────────────────────────────────────────┘

┌─ STEP 1: 选择GNN方法 ──────────────────────────────────────────────────┐
│                                                                        │
│ 策略网络输入:  [4000, 0.820, 0, ...]                                 │
│      ⬇                                                                │
│ 策略网络输出:  P(GCN)=0.2, P(GAT)=0.6, P(SAGE)=0.2                  │
│      ⬇ (选择最高概率或采样)                                           │
│ PPO选择:      method=1 (GAT) ← 最高概率0.6                          │
│      ⬇                                                                │
│ 参数选择:      param=0.75 ← 连续分布采样                             │
│      ⬇                                                                │
│ 维度映射:      dim = 32 (因为0.75在[0.67-1.0]范围)                  │
│                                                                        │
│ 生成的动作:                                                           │
│  action = {                                                          │
│    'node': 4,           # N4 - GNN Node                              │
│    'method': 1,         # GAT (Graph Attention)                      │
│    'param': 0.75        # Maps to 32-dim output                      │
│  }                                                                    │
└─────────────────────────────────────────────────────────────────────────┘

┌─ GNN执行 (N4处理) ──────────────────────────────────────────────────────┐
│                                                                        │
│ 输入: [4000 × 25] 特征矩阵 + 晶体结构                                │
│                                                                        │
│ ⏱️ 处理时间: 82ms (GAT + 32维)                                       │
│                                                                        │
│ 步骤1: 晶体结构转为图 (4000个图, 每个~40个原子)                     │
│   └─ 构建1000万+个节点/边的图数据                                   │
│                                                                        │
│ 步骤2: 初始化GAT模型 (32维输出)                                      │
│   ├─ 3层注意力图卷积                                                 │
│   ├─ 8个注意力头 (multi-head attention)                             │
│   ├─ 参数总数: ~50k                                                  │
│   └─ 使用adam优化器微调                                             │
│                                                                        │
│ 步骤3: 前向传播提取特征                                              │
│   ├─ 每个晶体: 4000次                                                │
│   ├─ 每个晶体的输出: [1, 32]                                        │
│   └─ 总输出: [4000, 32] GAT嵌入                                     │
│                                                                        │
│ 步骤4: 特征融合                                                       │
│   ├─ 原始特征: [4000, 25]                                            │
│   ├─ GNN特征: [4000, 32]                                             │
│   └─ 融合: concatenate → [4000, 57]                                │
│                                                                        │
│ 输出:                                                                 │
│  enhanced_data = {                                                   │
│    'X_train': [3000, 57]  # 训练集 (75%)                            │
│    'X_val': [1000, 57]    # 验证集 (25%)                            │
│    'feature_names': [25 + 32] = 57个特征名                          │
│    'gnn_info': {                                                     │
│      'method': 'GAT',                                                │
│      'output_dim': 32,                                               │
│      'processing_time_ms': 82,                                       │
│      'graph_stats': {                                                │
│        'n_graphs': 4000,                                             │
│        'avg_nodes': 38.5,                                            │
│        'avg_edges': 156.2                                            │
│      }                                                                │
│    }                                                                  │
│  }                                                                    │
└─────────────────────────────────────────────────────────────────────────┘

┌─ 模型训练 (N8节点) ─────────────────────────────────────────────────────┐
│                                                                        │
│ 输入: [3000, 57] 新特征矩阵 + 目标值                                 │
│                                                                        │
│ PPO选择的模型: XGBoost (随机选择)                                    │
│   ├─ 树的数量: 100                                                    │
│   ├─ 最大深度: 7                                                      │
│   └─ 学习率: 0.1                                                      │
│                                                                        │
│ 训练: XGBoost.fit(X_train[3000,57], y_train)                        │
│                                                                        │
│ 验证:                                                                 │
│   ├─ y_pred = model.predict(X_val[1000,57])                        │
│   ├─ R² = 0.858 ← 从0.820提升!                                      │
│   ├─ MAE = 0.082 eV/atom                                            │
│   └─ RMSE = 0.115 eV/atom                                           │
│                                                                        │
│ 模型保存:                                                             │
│   └─ models/formation_energy_xgb_gat32.joblib                      │
└─────────────────────────────────────────────────────────────────────────┘

┌─ 计算奖励 ──────────────────────────────────────────────────────────────┐
│                                                                        │
│ 之前性能 (Before): R² = 0.820                                        │
│ 之后性能 (After):  R² = 0.858                                        │
│ 改进量 (Improvement): ΔR² = 0.038 ← 3.8% 改进                      │
│                                                                        │
│ 处理成本 (Cost):                                                      │
│   ├─ GNN处理时间: 82ms                                               │
│   ├─ 基线处理时间: 20ms (无GNN)                                      │
│   └─ 额外成本: 62ms per sample                                      │
│                                                                        │
│ 奖励函数计算:                                                         │
│   α = 1.0 (改进的重要性)                                             │
│   β = 0.01 (时间成本的重要性)                                        │
│                                                                        │
│   reward = α * ΔR² - β * Δtime                                     │
│          = 1.0 * 0.038 - 0.01 * 62                                 │
│          = 0.038 - 0.62                                              │
│          = -0.582 ← 净负奖励！                                      │
│                                                                        │
│ 分析:                                                                 │
│  ✗ 虽然准确性提升了，但处理时间增加太多                              │
│  ✗ GAT + 32维有点"过度优化"                                          │
│  ✓ PPO学到: 这个配置在这种情况下不划算                              │
│                                                                        │
│ 最终奖励: reward = -0.582                                            │
└─────────────────────────────────────────────────────────────────────────┘

┌─ PPO学习反馈 ───────────────────────────────────────────────────────────┐
│                                                                        │
│ 经验收集:                                                             │
│   state:  [4000, 0.820, 0, ...]                                     │
│   action: {method:1, param:0.75}                                    │
│   reward: -0.582 ← 低奖励                                           │
│   next_state: [4000, 0.858, 1, ...]                                │
│   done: False                                                        │
│                                                                        │
│ 存储到缓冲区 (GAE Buffer)                                             │
│                                                                        │
│ 梯度信号:                                                             │
│   这个动作(GAT+32)得到了低奖励 (-0.582)                             │
│   →策略网络应该降低在此状态下选择此动作的概率                       │
│   →鼓励探索其他选择 (GCN或SAGE, 或16维)                            │
│                                                                        │
│ 策略改进方向:                                                         │
│   ├─ P(GAT|此状态) 应该下降                                         │
│   ├─ P(GCN|此状态) 可能上升                                         │
│   └─ P(SAGE|此状态) 可能上升                                        │
└─────────────────────────────────────────────────────────────────────────┘

最终结果 / Final Result for Episode 42:
├─ GNN方法选择: GAT (不是最优的选择)
├─ 输出维度: 32 (太高)
├─ 验证R²改进: +0.038 (+3.8%)
├─ 处理时间成本: +62ms
├─ 最终奖励: -0.582 (低)
├─ PPO反馈: 降低此配置的选择概率
└─ 学习意义: 在此数据集上，GAT+32维过度优化


════════════════════════════════════════════════════════════════════════════════
   对比：不同配置的结果 / Comparison: Different Configurations
════════════════════════════════════════════════════════════════════════════════

在同样的4000个材料数据上，不同的GNN配置对比:

┌─────────────┬──────────┬──────────┬──────────┬─────────┬──────────┐
│ 配置        │ 方法   │ 维度   │ R²提升 │ 时间(ms)│ 奖励   │
├─────────────┼──────────┼──────────┼──────────┼─────────┼──────────┤
│ 无GNN       │ -      │ -      │ 0%   │ 0      │ 0.0    │
│ GCN-8维     │ GCN    │ 8      │ +2.1%│ 35     │ +0.145 │ ✓推荐初期
│ GCN-16维    │ GCN    │ 16     │ +2.8%│ 45     │ +0.183 │ ✓推荐小数据
│ GCN-32维    │ GCN    │ 32     │ +3.2%│ 55     │ +0.182 │ 中等
│ GAT-8维     │ GAT    │ 8      │ +2.9%│ 50     │ +0.169 │ 中等
│ GAT-16维    │ GAT    │ 16     │ +3.8%│ 65     │ +0.172 │ ✓推荐平衡
│ GAT-32维    │ GAT    │ 32     │ +3.8%│ 82     │ -0.582 │ ✗太贵
│ SAGE-8维    │ SAGE   │ 8      │ +1.9%│ 25     │ +0.119 │ 快速
│ SAGE-16维   │ SAGE   │ 16     │ +2.4%│ 35     │ +0.157 │ ✓推荐快速
│ SAGE-32维   │ SAGE   │ 32     │ +3.1%│ 45     │ +0.180 │ 中等
└─────────────┴──────────┴──────────┴──────────┴─────────┴──────────┘

PPO应该学到的最优策略:
  ┌─ 优先顺序:
  ├─ 第1选择: GCN-16维 (+0.183 奖励) ← 最佳性价比
  ├─ 第2选择: GCN-8维 (+0.145 奖励) ← 如果时间紧
  ├─ 第3选择: GAT-16维 (+0.172 奖励) ← 如果准确性优先
  └─ 避免: GAT-32维 (-0.582 奖励) ← 代价太大

"""

if __name__ == '__main__':
    print(DIAGRAM_CONTENT)
