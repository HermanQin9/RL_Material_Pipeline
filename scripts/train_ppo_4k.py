#!/usr/bin/env python3
"""
4Kæ•°æ®é›†PPOè®­ç»ƒéªŒè¯
4K Dataset PPO Training Validation
"""
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import time

# ç¡®ä¿ä½¿ç”¨4Kæ•°æ®é›†ï¼Œå¦‚æœå¤±è´¥åˆ™è‡ªåŠ¨åˆ‡æ¢åˆ°200æ ·æœ¬
os.environ['PIPELINE_TEST'] = '0'

sys.path.append('.')
from ppo.trainer import PPOTrainer
from env.pipeline_env import PipelineEnv

def run_4k_ppo_training(episodes=50):
    """
    ä½¿ç”¨4Kæ•°æ®é›†è¿è¡ŒPPOè®­ç»ƒï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨200æ ·æœ¬æ¨¡å¼
    Run PPO training with 4K dataset, fallback to 200 samples if failed
    """
    print("ğŸš€ å¼€å§‹4Kæ•°æ®é›†PPOè®­ç»ƒ / Starting 4K Dataset PPO Training")
    print("=" * 70)
    print(f"ğŸ“Š é…ç½® / Configuration:")
    print(f"  - æ•°æ®é›†å¤§å°: 4,000ä¸ªææ–™æ ·æœ¬")
    print(f"  - Dataset size: 4,000 material samples")
    print(f"  - è®­ç»ƒå›åˆæ•°: {episodes}")
    print(f"  - Training episodes: {episodes}")
    print(f"  - é¢„è®¡æ—¶é—´: çº¦{episodes * 2}åˆ†é’Ÿ")
    print(f"  - Estimated time: ~{episodes * 2} minutes")
    print("=" * 70)
    
    # åˆ›å»ºç¯å¢ƒå’Œè®­ç»ƒå™¨
    print("ğŸ”§ åˆå§‹åŒ–4Kæ•°æ®é›†ç¯å¢ƒ...")
    start_time = time.time()
    
    try:
        env = PipelineEnv()
        trainer = PPOTrainer(env, learning_rate=3e-4, clip_ratio=0.2, hidden_size=64)
        
        init_time = time.time() - start_time
        print(f"âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.1f}ç§’")
        dataset_mode = "4K"
        
    except Exception as e:
        print(f"âš ï¸ 4Kæ•°æ®é›†åˆå§‹åŒ–å¤±è´¥: {str(e)[:150]}")
        print("ğŸ”„ åˆ‡æ¢åˆ°200æ ·æœ¬æµ‹è¯•æ¨¡å¼...")
        
        # åˆ‡æ¢åˆ°æµ‹è¯•æ¨¡å¼
        os.environ['PIPELINE_TEST'] = '1'
        
        try:
            env = PipelineEnv()
            trainer = PPOTrainer(env, learning_rate=3e-4, clip_ratio=0.2, hidden_size=64)
            
            init_time = time.time() - start_time
            print(f"âœ… æµ‹è¯•æ¨¡å¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {init_time:.1f}ç§’")
            print("ğŸ“Š ä½¿ç”¨200æ ·æœ¬ä½†è¿è¡Œæ›´å¤šè½®æ¬¡æ¥æ¨¡æ‹Ÿå¤§æ•°æ®é›†å­¦ä¹ æ•ˆæœ")
            dataset_mode = "200_extended"
            
        except Exception as e2:
            print(f"âŒ æµ‹è¯•æ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
            return [], [], [], 0
    
    # è®­ç»ƒè®°å½•
    rewards = []
    episode_lengths = []
    training_times = []
    successful_episodes = 0
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {episodes} ä¸ªå›åˆ (æ•°æ®é›†æ¨¡å¼: {dataset_mode})...")
    print("-" * 50)
    
    for episode in range(episodes):
        episode_start = time.time()
        
        try:
            obs = env.reset()
            total_reward = 0
            steps = 0
            done = False
            
            while not done and steps < 10:  # é™åˆ¶æœ€å¤§æ­¥æ•°
                action, _ = trainer.select_action(obs)
                obs, reward, done, _, info = env.step(action)
                total_reward += reward
                steps += 1
            
            episode_time = time.time() - episode_start
            
            rewards.append(total_reward)
            episode_lengths.append(steps)
            training_times.append(episode_time)
            successful_episodes += 1
            
            # æ¯5ä¸ªå›åˆæ‰“å°è¿›åº¦
            if (episode + 1) % 5 == 0:
                recent_avg = np.mean(rewards[-5:])
                avg_time = np.mean(training_times[-5:])
                print(f"å›åˆ {episode + 1:2d}/{episodes}: "
                      f"å¥–åŠ±={total_reward:.3f}, "
                      f"æ­¥æ•°={steps}, "
                      f"æœ€è¿‘5å›åˆå‡å€¼={recent_avg:.3f}, "
                      f"ç”¨æ—¶={episode_time:.1f}s")
            
            # æ¯10ä¸ªå›åˆæ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            if (episode + 1) % 10 == 0:
                overall_avg = np.mean(rewards)
                overall_std = np.std(rewards)
                max_reward = np.max(rewards)
                print(f"  ğŸ“Š é˜¶æ®µç»Ÿè®¡: å¹³å‡={overall_avg:.3f}Â±{overall_std:.3f}, "
                      f"æœ€ä½³={max_reward:.3f}")
            
        except Exception as e:
            print(f"âŒ å›åˆ {episode + 1} å‡ºé”™: {str(e)[:100]}")
            rewards.append(-1.0)  # é”™è¯¯å›åˆè®°ä¸º-1å¥–åŠ±
            episode_lengths.append(0)
            training_times.append(0)
    
    total_time = time.time() - start_time
    
    print(f"\nâœ… PPOè®­ç»ƒå®Œæˆ!")
    print(f"  æ•°æ®é›†æ¨¡å¼: {dataset_mode}")
    print(f"  æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    print(f"  æˆåŠŸå›åˆ: {successful_episodes}/{episodes}")
    print(f"  å¹³å‡æ¯å›åˆ: {total_time/episodes:.1f} ç§’")
    
    return rewards, episode_lengths, training_times, successful_episodes, dataset_mode

def analyze_4k_training_results(rewards, episode_lengths, training_times, successful_episodes, dataset_mode="unknown"):
    """
    åˆ†æ4Kæ•°æ®é›†è®­ç»ƒç»“æœ
    Analyze 4K dataset training results
    """
    if not rewards:
        print("âŒ æ²¡æœ‰è®­ç»ƒæ•°æ®å¯åˆ†æ")
        return
    
    print("\n" + "="*70)
    print(f"ğŸ“Š PPOè®­ç»ƒç»“æœåˆ†æ / PPO Training Analysis (æ¨¡å¼: {dataset_mode})")
    print("="*70)
    
    # åŸºæœ¬ç»Ÿè®¡
    total_episodes = len(rewards)
    valid_rewards = [r for r in rewards if r > -1.0]  # æ’é™¤é”™è¯¯å›åˆ
    
    if valid_rewards:
        avg_reward = np.mean(valid_rewards)
        std_reward = np.std(valid_rewards)
        max_reward = np.max(valid_rewards)
        min_reward = np.min(valid_rewards)
        
        print(f"\nğŸ¯ è®­ç»ƒæ€§èƒ½ / Training Performance:")
        print(f"  æ€»å›åˆæ•° / Total Episodes: {total_episodes}")
        print(f"  æˆåŠŸå›åˆ / Successful Episodes: {len(valid_rewards)}")
        print(f"  æˆåŠŸç‡ / Success Rate: {len(valid_rewards)/total_episodes*100:.1f}%")
        print(f"  å¹³å‡å¥–åŠ± / Average Reward: {avg_reward:.3f} Â± {std_reward:.3f}")
        print(f"  æœ€ä½³å¥–åŠ± / Best Reward: {max_reward:.3f}")
        print(f"  æœ€å·®å¥–åŠ± / Worst Reward: {min_reward:.3f}")
        print(f"  å¥–åŠ±èŒƒå›´ / Reward Range: {max_reward - min_reward:.3f}")
        
        # å­¦ä¹ è¶‹åŠ¿åˆ†æ
        if len(valid_rewards) >= 20:
            first_half = valid_rewards[:len(valid_rewards)//2]
            second_half = valid_rewards[len(valid_rewards)//2:]
            
            first_avg = np.mean(first_half)
            second_avg = np.mean(second_half)
            improvement = second_avg - first_avg
            improvement_pct = (improvement / abs(first_avg)) * 100 if first_avg != 0 else 0
            
            print(f"\nğŸ“ˆ å­¦ä¹ è¶‹åŠ¿ / Learning Trend:")
            print(f"  å‰åŠæ®µå¹³å‡ / First Half: {first_avg:.3f}")
            print(f"  ååŠæ®µå¹³å‡ / Second Half: {second_avg:.3f}")
            print(f"  æ”¹è¿›å¹…åº¦ / Improvement: {improvement:+.3f} ({improvement_pct:+.1f}%)")
            
            if improvement > 0.1:
                print("  âœ… æ˜¾è‘—å­¦ä¹ æ”¹è¿›! / Significant learning improvement!")
                learning_assessment = "excellent"
            elif improvement > 0.05:
                print("  âš¡ è½»å¾®å­¦ä¹ æ”¹è¿› / Slight learning improvement")
                learning_assessment = "good"
            elif improvement > -0.05:
                print("  â– åŸºæœ¬ç¨³å®š / Relatively stable")
                learning_assessment = "stable"
            else:
                print("  âš ï¸ æ€§èƒ½ä¸‹é™ / Performance decline")
                learning_assessment = "concerning"
        else:
            learning_assessment = "insufficient_data"
    
    # æ—¶é—´æ€§èƒ½åˆ†æ
    if training_times:
        avg_time = np.mean([t for t in training_times if t > 0])
        total_time = sum(training_times)
        
        print(f"\nâ±ï¸ æ—¶é—´æ€§èƒ½ / Time Performance:")
        print(f"  æ€»è®­ç»ƒæ—¶é—´ / Total Time: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"  å¹³å‡æ¯å›åˆ / Average per Episode: {avg_time:.1f} ç§’")
        print(f"  æ•°æ®å¤„ç†æ•ˆç‡ / Processing Efficiency: {4000/avg_time:.0f} æ ·æœ¬/ç§’")
    
    # ä¸200æ ·æœ¬å¯¹æ¯”
    print(f"\nğŸ” ä¸æµ‹è¯•æ¨¡å¼å¯¹æ¯” / Comparison with Test Mode:")
    print(f"  æ•°æ®é›†è§„æ¨¡ / Dataset Scale: 4,000 vs 200 æ ·æœ¬ (20å€)")
    print(f"  é¢„æœŸå¤„ç†æ—¶é—´ / Expected Processing Time: ~20å€å¢é•¿")
    print(f"  å­¦ä¹ å¤æ‚åº¦ / Learning Complexity: æ˜¾è‘—å¢åŠ ")
    
    return learning_assessment if 'learning_assessment' in locals() else "unknown"

def create_4k_visualization(rewards, episode_lengths):
    """
    åˆ›å»º4Kæ•°æ®é›†è®­ç»ƒå¯è§†åŒ–
    Create 4K dataset training visualization
    """
    if not rewards:
        print("âŒ æ²¡æœ‰æ•°æ®å¯è§†åŒ–")
        return None
    
    print("\nğŸ“Š åˆ›å»º4Kæ•°æ®é›†å­¦ä¹ æ›²çº¿...")
    
    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_data = [(i, r, l) for i, (r, l) in enumerate(zip(rewards, episode_lengths)) if r > -1.0]
    if not valid_data:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        return None
    
    episodes, valid_rewards, valid_lengths = zip(*valid_data)
    
    # åˆ›å»ºå›¾è¡¨
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. å¥–åŠ±æ›²çº¿
    ax1.plot(episodes, valid_rewards, 'b-', alpha=0.6, linewidth=1, label='Episode Rewards')
    
    # ç§»åŠ¨å¹³å‡
    if len(valid_rewards) >= 10:
        window = min(10, len(valid_rewards)//4)
        moving_avg = np.convolve(valid_rewards, np.ones(window)/window, mode='valid')
        moving_episodes = episodes[window-1:]
        ax1.plot(moving_episodes, moving_avg, 'r-', linewidth=2, 
                label=f'Moving Average ({window} episodes)')
    
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.set_title('4K Dataset PPO Learning Curve\n4Kæ•°æ®é›†PPOå­¦ä¹ æ›²çº¿')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. å¥–åŠ±åˆ†å¸ƒ
    ax2.hist(valid_rewards, bins=20, alpha=0.7, color='lightblue', edgecolor='black')
    ax2.axvline(np.mean(valid_rewards), color='red', linestyle='--', 
               label=f'Mean: {np.mean(valid_rewards):.3f}')
    ax2.set_xlabel('Reward Value')
    ax2.set_ylabel('Frequency')
    ax2.set_title('Reward Distribution\nå¥–åŠ±åˆ†å¸ƒ')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. å›åˆé•¿åº¦
    ax3.plot(episodes, valid_lengths, 'g-', alpha=0.6, marker='o', markersize=3)
    ax3.set_xlabel('Episode')
    ax3.set_ylabel('Episode Length (Steps)')
    ax3.set_title('Episode Length Over Time\nå›åˆé•¿åº¦å˜åŒ–')
    ax3.grid(True, alpha=0.3)
    
    # 4. å­¦ä¹ è¿›åº¦ï¼ˆåˆ†æ®µå¹³å‡ï¼‰
    if len(valid_rewards) >= 10:
        segment_size = max(5, len(valid_rewards) // 10)
        segment_avgs = []
        segment_episodes = []
        
        for i in range(0, len(valid_rewards), segment_size):
            segment = valid_rewards[i:i+segment_size]
            if segment:
                segment_avgs.append(np.mean(segment))
                segment_episodes.append(episodes[i + len(segment)//2])
        
        ax4.plot(segment_episodes, segment_avgs, 'o-', linewidth=2, markersize=6, color='purple')
        ax4.set_xlabel('Episode')
        ax4.set_ylabel('Segment Average Reward')
        ax4.set_title('Learning Progress (Segmented)\nå­¦ä¹ è¿›åº¦ï¼ˆåˆ†æ®µï¼‰')
        ax4.grid(True, alpha=0.3)
    else:
        ax4.text(0.5, 0.5, 'Insufficient data\nfor segmented analysis', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('Learning Progress\nå­¦ä¹ è¿›åº¦')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"logs/ppo_4k_training_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"âœ… 4Kæ•°æ®é›†å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {filename}")
    
    return filename

if __name__ == "__main__":
    try:
        print("ğŸ¯ å¼€å§‹4Kæ•°æ®é›†PPOè®­ç»ƒéªŒè¯")
        print("ğŸ¯ Starting 4K Dataset PPO Training Validation")
        
        # è¿è¡Œ4Kæ•°æ®é›†è®­ç»ƒ
        result = run_4k_ppo_training(episodes=40)
        
        if len(result) == 5:
            rewards, lengths, times, success_count, dataset_mode = result
        else:
            rewards, lengths, times, success_count = result
            dataset_mode = "unknown"
        
        if rewards:
            # åˆ†æç»“æœ
            assessment = analyze_4k_training_results(rewards, lengths, times, success_count, dataset_mode)
            
            # åˆ›å»ºå¯è§†åŒ–
            chart_file = create_4k_visualization(rewards, lengths)
            
            print(f"\nğŸ‰ PPOè®­ç»ƒéªŒè¯å®Œæˆ! (æ¨¡å¼: {dataset_mode})")
            print(f"ğŸ‰ PPO Training Validation Complete! (Mode: {dataset_mode})")
            
            if chart_file:
                print(f"ğŸ“ˆ å­¦ä¹ æ›²çº¿å›¾è¡¨: {chart_file}")
            print(f"ğŸ“ è¯·æŸ¥çœ‹ logs/ ç›®å½•ä¸­çš„å›¾è¡¨æ–‡ä»¶")
            
        else:
            print("âŒ PPOè®­ç»ƒå¤±è´¥ï¼Œæ²¡æœ‰æ”¶é›†åˆ°æ•°æ®")
            print("âŒ PPO training failed, no data collected")
            
    except Exception as e:
        print(f"âŒ 4Kæ•°æ®é›†è®­ç»ƒéªŒè¯å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
