#!/usr/bin/env python3
"""PPOè®­ç»ƒç»“æœåˆ†æå’Œå¯è§†åŒ– / PPO Training Results Analysis and Visualization"""
from __future__ import annotations

import argparse
from datetime import datetime
from pathlib import Path
from typing import Iterable, Sequence

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
import torch

# æ”¯æŒä¸­æ–‡æ˜¾ç¤º / Support Chinese fonts
plt.rcParams["font.sans-serif"] = ["SimHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


def find_latest_checkpoint(models_dir: Path, pattern: str = "ppo_agent*.pth") -> Path:
    """æ‰¾åˆ°æœ€æ–°çš„PPOæ£€æŸ¥ç‚¹ / Locate the most recent PPO checkpoint."""
    candidates = sorted(models_dir.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)
    if not candidates:
        raise FileNotFoundError(f"åœ¨ {models_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•PPOæ¨¡å‹ (pattern={pattern})")
    return candidates[0]


def load_training_data(checkpoint_path: Path) -> tuple[list[float], list[float]]:
    """ä»PPOæ£€æŸ¥ç‚¹è½½å…¥å¥–åŠ±ä¸é•¿åº¦ / Load rewards and lengths from a PPO checkpoint."""
    ckpt = torch.load(checkpoint_path, map_location="cpu")
    rewards = ckpt.get("episode_rewards")
    lengths = ckpt.get("episode_lengths")

    if rewards is None or len(rewards) == 0:
        raise ValueError(f"æ£€æŸ¥ç‚¹ {checkpoint_path} ä¸­æ²¡æœ‰ episode_rewards")

    rewards = list(map(float, rewards))

    if lengths is None or len(lengths) == 0:
        lengths = [0.0 for _ in rewards]
    else:
        lengths = list(map(float, lengths))

    return rewards, lengths


def rolling_mean(values: Sequence[float], window: int) -> tuple[np.ndarray, np.ndarray] | None:
    arr = np.asarray(values, dtype=float)
    if window < 2 or arr.size < window:
        return None
    kernel = np.ones(window) / window
    smoothed = np.convolve(arr, kernel, mode="valid")
    episodes = np.arange(window, arr.size + 1)
    return smoothed, episodes


def compute_success_flags(rewards: Iterable[float], failure_threshold: float = -0.95) -> list[int]:
    """åˆ¤æ–­æ¯ä¸ªå›åˆæ˜¯å¦æˆåŠŸ / Determine success for each episode."""
    arr = np.asarray(rewards, dtype=float)
    return (arr > failure_threshold).astype(int).tolist()


def create_visualizations(
    rewards: Sequence[float],
    success_flags: Sequence[int],
    episode_lengths: Sequence[float],
    output_path: Path,
    window: int,
) -> Path:
    rewards_arr = np.asarray(rewards, dtype=float)
    lengths_arr = np.asarray(episode_lengths, dtype=float)
    episodes = np.arange(1, len(rewards_arr) + 1)

    fig, axes = plt.subplots(2, 2, figsize=(14, 9))
    ax1, ax2, ax3, ax4 = axes.flatten()

    # å¥–åŠ±æ›²çº¿ / Reward curve
    ax1.plot(episodes, rewards_arr, color="#1f77b4", marker="o", markersize=3, linewidth=1, label="Episode Reward")
    smoothed = rolling_mean(rewards_arr.tolist(), window)
    if smoothed is not None:
        moving_avg, moving_eps = smoothed
        ax1.plot(moving_eps, moving_avg, color="#d62728", linewidth=2, label=f"{window}å›åˆç§»åŠ¨å¹³å‡")
    ax1.set_title("PPO Episode Rewards\næ¯å›åˆå¥–åŠ±")
    ax1.set_xlabel("Episode / å›åˆ")
    ax1.set_ylabel("Reward / å¥–åŠ±")
    ax1.grid(alpha=0.3)
    ax1.legend()

    # æˆåŠŸç‡æ›²çº¿ / Success rate curve
    success_arr = np.asarray(success_flags, dtype=float)
    cumulative_success = np.cumsum(success_arr) / np.arange(1, len(success_arr) + 1)
    ax2.plot(episodes, cumulative_success, color="#2ca02c", linewidth=2, marker="o", markersize=3)
    ax2.set_ylim(0, 1)
    ax2.set_title("Cumulative Success Rate\nç´¯è®¡æˆåŠŸç‡")
    ax2.set_xlabel("Episode / å›åˆ")
    ax2.set_ylabel("Success Rate / æˆåŠŸç‡")
    ax2.grid(alpha=0.3)

    # å¥–åŠ±åˆ†å¸ƒ / Reward distribution
    ax3.hist(rewards_arr, bins=min(20, max(5, len(rewards_arr) // 3)), color="#9467bd", alpha=0.8)
    ax3.set_title("Reward Distribution\nå¥–åŠ±åˆ†å¸ƒ")
    ax3.set_xlabel("Reward / å¥–åŠ±")
    ax3.set_ylabel("Frequency / é¢‘æ•°")
    ax3.grid(alpha=0.2)

    # å›åˆæ­¥æ•° / Episode length
    ax4.plot(episodes, lengths_arr, color="#ff7f0e", marker="o", markersize=3, linewidth=1, label="Episode Length")
    ax4.set_title("Episode Lengths\næ¯å›åˆæ­¥æ•°")
    ax4.set_xlabel("Episode / å›åˆ")
    ax4.set_ylabel("Length / æ­¥æ•°")
    ax4.grid(alpha=0.3)
    ax4.legend()

    plt.tight_layout()
    output_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(output_path, dpi=300, bbox_inches="tight")
    return output_path


def summarize_rewards(rewards: Sequence[float]) -> dict[str, float]:
    arr = np.asarray(rewards, dtype=float)
    stats = {
        "count": float(arr.size),
        "mean": float(np.mean(arr)),
        "std": float(np.std(arr)),
        "min": float(np.min(arr)),
        "max": float(np.max(arr)),
    }
    if arr.size >= 2:
        mid = arr.size // 2
        stats["first_half_mean"] = float(np.mean(arr[:mid]))
        stats["second_half_mean"] = float(np.mean(arr[mid:]))
        stats["improvement"] = stats["second_half_mean"] - stats["first_half_mean"]
    else:
        stats["first_half_mean"] = stats["second_half_mean"] = stats["improvement"] = float("nan")
    return stats


def print_summary(
    checkpoint_path: Path,
    rewards: Sequence[float],
    success_flags: Sequence[int],
    lengths: Sequence[float],
    figure_path: Path,
    window: int,
) -> None:
    stats = summarize_rewards(rewards)
    success_rate = float(np.mean(success_flags)) if len(success_flags) else 0.0
    avg_length = float(np.mean(lengths)) if len(lengths) else 0.0

    print("=" * 70)
    print("ğŸ“Š PPOè®­ç»ƒç»“æœåˆ†æ / PPO Training Results Analysis")
    print("=" * 70)
    print(f"ğŸ”– æ¨¡å‹æ£€æŸ¥ç‚¹ / Checkpoint: {checkpoint_path}")
    print(f"ğŸ“ˆ æ€»å›åˆæ•° / Total Episodes: {int(stats['count'])}")
    print(f"ğŸ¯ å¹³å‡å¥–åŠ± / Mean Reward: {stats['mean']:.3f} Â± {stats['std']:.3f}")
    print(f"ğŸ” æœ€ä½³å¥–åŠ± / Best Reward: {stats['max']:.3f}")
    print(f"ğŸ”» æœ€å·®å¥–åŠ± / Worst Reward: {stats['min']:.3f}")
    print(f"âœ… æˆåŠŸç‡(> -0.95): {success_rate * 100:.1f}% ({int(np.sum(success_flags))}/{len(success_flags)})")
    print(f"â±ï¸ å¹³å‡æ­¥æ•° / Avg Episode Length: {avg_length:.1f}")

    if not np.isnan(stats["improvement"]):
        if stats["improvement"] > 0:
            trend = "â¬†ï¸ æ”¹è¿›"
        elif abs(stats["improvement"]) < 1e-3:
            trend = "â¡ï¸ æŒå¹³"
        else:
            trend = "â¬‡ï¸ é€€åŒ–"
        print(f"ğŸ“‰ å‰åŠæ®µå¹³å‡å¥–åŠ±: {stats['first_half_mean']:.3f}")
        print(f"ğŸ“ˆ ååŠæ®µå¹³å‡å¥–åŠ±: {stats['second_half_mean']:.3f}")
        print(f"ğŸš€ å¥–åŠ±å˜åŒ– / Reward Shift: {stats['improvement']:+.3f} ({trend})")

    failures = [idx + 1 for idx, flag in enumerate(success_flags) if flag == 0]
    if failures:
        print(f"âš ï¸ å¤±è´¥å›åˆ (reward â‰¤ -0.95): {failures}")
    else:
        print("âœ… æœªæ£€æµ‹åˆ°å¤±è´¥å›åˆ / No failing episodes detected")

    print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ / Figure saved to: {figure_path}")
    print(f"ğŸª„ ç§»åŠ¨å¹³å‡çª—å£ / Moving average window: {window}")
    print("=" * 70)


def main() -> None:
    parser = argparse.ArgumentParser(description="Analyze PPO training results and generate visualizations")
    parser.add_argument("--checkpoint", type=Path, default=None, help="æŒ‡å®šPPOæ£€æŸ¥ç‚¹è·¯å¾„ / Path to checkpoint")
    parser.add_argument("--output", type=Path, default=None, help="è¾“å‡ºå›¾åƒè·¯å¾„ / Output image path")
    parser.add_argument("--window", type=int, default=10, help="ç§»åŠ¨å¹³å‡çª—å£å¤§å° / Moving average window size")
    parser.add_argument("--failure-threshold", type=float, default=-0.95, help="å¤±è´¥åˆ¤å®šé˜ˆå€¼ / Failure threshold")
    args = parser.parse_args()

    models_dir = Path("models")
    checkpoint_path = args.checkpoint or find_latest_checkpoint(models_dir)
    rewards, lengths = load_training_data(checkpoint_path)
    success_flags = compute_success_flags(rewards, failure_threshold=args.failure_threshold)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    default_output = Path("logs") / f"ppo_learning_curves_{timestamp}.png"
    output_path = args.output or default_output
    window = max(2, args.window)

    figure_path = create_visualizations(rewards, success_flags, lengths, output_path, window=window)
    print_summary(checkpoint_path, rewards, success_flags, lengths, figure_path, window)


if __name__ == "__main__":
    main()
