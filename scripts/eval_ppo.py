"""
PPO evaluation script
PPOç­–ç•¥è¯„ä¼°è„šæœ¬
"""

import sys
import os
import argparse
import torch
import numpy as np
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from env.pipeline_env import PipelineEnv
from ppo.policy import PPOPolicy
from ppo.trainer import PPOTrainer

def evaluate_policy(policy_path: str, num_episodes: int = 10, render: bool = False):
    """
    è¯„ä¼°è®­ç»ƒå¥½çš„PPOç­–ç•¥
    Evaluate trained PPO policy
    
    Args:
        policy_path: ç­–ç•¥æ¨¡å‹è·¯å¾„
        num_episodes: è¯„ä¼°å›åˆæ•°
        render: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    # åˆ›å»ºç¯å¢ƒ
    env = PipelineEnv()
    
    # åŠ è½½ç­–ç•¥
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # è·å–è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ç»´åº¦
    obs = env.reset()
    obs_dim = len(env._get_obs())
    action_dim = 3  # node + method + param
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    policy = PPOPolicy(obs_dim, action_dim).to(device)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    try:
        checkpoint = torch.load(policy_path, map_location=device)
        if 'policy_state_dict' in checkpoint:
            policy.load_state_dict(checkpoint['policy_state_dict'])
        else:
            policy.load_state_dict(checkpoint)
        print(f"âœ… æˆåŠŸåŠ è½½ç­–ç•¥: {policy_path}")
    except Exception as e:
        print(f"âŒ åŠ è½½ç­–ç•¥å¤±è´¥: {e}")
        return
    
    policy.eval()
    
    # è¯„ä¼°ç»Ÿè®¡
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    print(f"\nğŸ¯ å¼€å§‹è¯„ä¼° {num_episodes} ä¸ªå›åˆ...")
    
    for episode in range(num_episodes):
        obs = env.reset()
        total_reward = 0
        steps = 0
        done = False
        
        if render:
            print(f"\n--- å›åˆ {episode + 1} ---")
        
        while not done and steps < 100:  # é˜²æ­¢æ— é™å¾ªç¯
            with torch.no_grad():
                obs_tensor = torch.FloatTensor(env._get_obs()).unsqueeze(0).to(device)
                action_probs = policy.actor(obs_tensor)
                
                # è´ªå©ªé€‰æ‹©åŠ¨ä½œ (è¯„ä¼°æ—¶ä¸ä½¿ç”¨éšæœºæ€§)
                node_idx = torch.argmax(action_probs[:, :env.num_nodes]).item()
                
                # è®¡ç®—æ–¹æ³•ç´¢å¼•èŒƒå›´
                method_start = env.num_nodes
                node_name = env.pipeline_nodes[node_idx]
                num_methods = len(env.methods_for_node[node_name])
                method_idx = torch.argmax(action_probs[:, method_start:method_start+num_methods]).item()
                
                # å‚æ•°å€¼
                param_idx = env.num_nodes + max(len(methods) for methods in env.methods_for_node.values())
                param_value = torch.sigmoid(action_probs[:, param_idx]).item()
                
                action = {
                    'node': node_idx,
                    'method': method_idx,
                    'params': [param_value]
                }
            
            if render:
                method_name = env.methods_for_node[node_name][method_idx]
                print(f"  æ­¥éª¤ {steps}: {node_name}.{method_name}(param={param_value:.3f})")
            
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            total_reward += reward
            steps += 1
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        if total_reward > 0:  # å‡è®¾æ­£å¥–åŠ±è¡¨ç¤ºæˆåŠŸ
            success_count += 1
        
        if render:
            print(f"  å›åˆå¥–åŠ±: {total_reward:.3f}, æ­¥æ•°: {steps}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    avg_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    avg_length = np.mean(episode_lengths)
    success_rate = success_count / num_episodes
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {avg_reward:.3f} Â± {std_reward:.3f}")
    print(f"  å¹³å‡æ­¥æ•°: {avg_length:.1f}")
    print(f"  æˆåŠŸç‡: {success_rate:.1%}")
    print(f"  æœ€ä½³å¥–åŠ±: {max(episode_rewards):.3f}")
    print(f"  æœ€å·®å¥–åŠ±: {min(episode_rewards):.3f}")
    
    return {
        'avg_reward': avg_reward,
        'std_reward': std_reward,
        'avg_length': avg_length,
        'success_rate': success_rate,
        'episode_rewards': episode_rewards
    }

def compare_policies(policy_paths: list, num_episodes: int = 10):
    """
    æ¯”è¾ƒå¤šä¸ªç­–ç•¥çš„æ€§èƒ½
    Compare performance of multiple policies
    """
    results = {}
    
    for policy_path in policy_paths:
        print(f"\n{'='*50}")
        print(f"è¯„ä¼°ç­–ç•¥: {policy_path}")
        print(f"{'='*50}")
        
        result = evaluate_policy(policy_path, num_episodes, render=False)
        results[policy_path] = result
    
    # æ‰“å°æ¯”è¾ƒç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“‹ ç­–ç•¥æ¯”è¾ƒç»“æœ")
    print(f"{'='*60}")
    
    print(f"{'ç­–ç•¥':<30} {'å¹³å‡å¥–åŠ±':<12} {'æˆåŠŸç‡':<10} {'å¹³å‡æ­¥æ•°':<10}")
    print("-" * 60)
    
    for policy_path, result in results.items():
        policy_name = Path(policy_path).stem
        print(f"{policy_name:<30} {result['avg_reward']:<12.3f} {result['success_rate']:<10.1%} {result['avg_length']:<10.1f}")
    
    return results

def main():
    parser = argparse.ArgumentParser(description='Evaluate PPO policy')
    parser.add_argument('--policy-path', type=str, required=True, help='Path to trained policy')
    parser.add_argument('--episodes', type=int, default=10, help='Number of evaluation episodes')
    parser.add_argument('--render', action='store_true', help='Show detailed episode information')
    parser.add_argument('--compare', nargs='+', help='Compare multiple policies')
    
    args = parser.parse_args()
    
    if args.compare:
        # æ¯”è¾ƒå¤šä¸ªç­–ç•¥
        compare_policies(args.compare, args.episodes)
    else:
        # è¯„ä¼°å•ä¸ªç­–ç•¥
        evaluate_policy(args.policy_path, args.episodes, args.render)

if __name__ == "__main__":
    main()
