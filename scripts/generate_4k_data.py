#!/usr/bin/env python3
"""
4Kæ•°æ®é›†ç”Ÿæˆå’ŒéªŒè¯è„šæœ¬
4K Dataset Generation and Validation Script
"""
import os
import sys
import pickle
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import time

# å¼ºåˆ¶è®¾ç½®ä¸º4Kæ¨¡å¼
os.environ['PIPELINE_TEST'] = '0'

sys.path.append('..')
from config import N_TOTAL, BATCH_SIZE, CACHE_FILE, API_KEY, TARGET_PROP
from mp_api.client import MPRester

def get_value(d, key, default=None):
    """å®‰å…¨è·å–å±æ€§å€¼"""
    try:
        if hasattr(d, key):
            return getattr(d, key, default)
        elif isinstance(d, dict):
            return d.get(key, default)
        else:
            return default
    except:
        return default

def generate_4k_data_safe():
    """
    å®‰å…¨åœ°ç”Ÿæˆ4Kæ•°æ®é›†ï¼ŒåŒ…å«é”™è¯¯å¤„ç†
    Safely generate 4K dataset with error handling
    """
    print("ğŸš€ å¼€å§‹ç”Ÿæˆ4Kææ–™æ•°æ®é›†")
    print("ğŸš€ Starting 4K Material Dataset Generation")
    print("=" * 70)
    print(f"ğŸ“Š ç›®æ ‡é…ç½®:")
    print(f"  - ç›®æ ‡æ ·æœ¬æ•°: {N_TOTAL:,}")
    print(f"  - æ‰¹å¤„ç†å¤§å°: {BATCH_SIZE}")
    print(f"  - ç¼“å­˜æ–‡ä»¶: {CACHE_FILE}")
    print(f"  - APIå¯†é’¥: {'å·²è®¾ç½®' if API_KEY else 'æœªè®¾ç½®'}")
    print("=" * 70)
    
    if not API_KEY:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°Materials Project APIå¯†é’¥")
        return False
    
    dfs = []
    fetched = 0
    error_count = 0
    start_time = time.time()
    
    try:
        print(f"ğŸ”— è¿æ¥Materials Project API...")
        with MPRester(API_KEY) as mpr:
            print(f"âœ… APIè¿æ¥æˆåŠŸ")
            
            # è®¡ç®—éœ€è¦çš„æ‰¹æ¬¡æ•°
            num_chunks = (N_TOTAL // BATCH_SIZE) + 2  # é¢å¤–è·å–ä¸€äº›ä»¥é˜²æ•°æ®ä¸è¶³
            print(f"ğŸ“¦ é¢„è®¡æ‰¹æ¬¡æ•°: {num_chunks}")
            
            print(f"ğŸ“¥ å¼€å§‹è·å–æ•°æ®...")
            docs_iter = mpr.materials.summary.search(
                fields=["material_id", "structure", "elements", "formula_pretty", TARGET_PROP],
                chunk_size=BATCH_SIZE,
                num_chunks=num_chunks,
            )
            
            batch_count = 0
            for docs in tqdm(docs_iter, desc="è·å–MPæ•°æ®", total=num_chunks):
                batch_count += 1
                try:
                    # ç¡®ä¿docsæ˜¯åˆ—è¡¨
                    if not isinstance(docs, list):
                        docs = [docs]
                    
                    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
                    valid_docs = []
                    for d in docs:
                        try:
                            # æ£€æŸ¥å¿…éœ€å­—æ®µ
                            target_value = get_value(d, TARGET_PROP)
                            structure = get_value(d, "structure")
                            
                            if target_value is not None and structure is not None:
                                valid_docs.append(d)
                        except Exception as e:
                            error_count += 1
                            continue
                    
                    if not valid_docs:
                        print(f"âš ï¸ æ‰¹æ¬¡ {batch_count} æ— æœ‰æ•ˆæ•°æ®")
                        continue
                    
                    # åˆ›å»ºDataFrame
                    batch_data = []
                    for d in valid_docs:
                        try:
                            # å®‰å…¨æå–composition
                            structure = get_value(d, "structure")
                            composition = None
                            if structure and hasattr(structure, 'composition'):
                                composition = structure.composition
                            
                            row_data = {
                                "material_id": get_value(d, "material_id"),
                                "structure": structure,
                                "elements": get_value(d, "elements"),
                                "formula_pretty": get_value(d, "formula_pretty"),
                                TARGET_PROP: get_value(d, TARGET_PROP),
                                "composition": composition
                            }
                            batch_data.append(row_data)
                            
                        except Exception as e:
                            error_count += 1
                            continue
                    
                    if batch_data:
                        df_batch = pd.DataFrame(batch_data)
                        # åˆ é™¤structureä¸ºNoneçš„è¡Œ
                        df_batch = df_batch.dropna(subset=["structure"]).reset_index(drop=True)
                        
                        if len(df_batch) > 0:
                            dfs.append(df_batch)
                            fetched += len(df_batch)
                            
                            if batch_count % 5 == 0:
                                print(f"ğŸ“Š å·²è·å– {fetched:,} / {N_TOTAL:,} æ ·æœ¬ (æ‰¹æ¬¡ {batch_count})")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡
                    if fetched >= N_TOTAL:
                        print(f"âœ… å·²è¾¾åˆ°ç›®æ ‡æ ·æœ¬æ•°: {fetched:,}")
                        break
                        
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡ {batch_count} å¤„ç†é”™è¯¯: {str(e)[:100]}")
                    error_count += 1
                    continue
            
    except Exception as e:
        print(f"âŒ APIè¿æ¥æˆ–æ•°æ®è·å–å¤±è´¥: {e}")
        return False
    
    # åˆå¹¶æ•°æ®
    if not dfs:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®")
        return False
    
    print(f"\nğŸ“Š åˆå¹¶æ•°æ®...")
    full_df = pd.concat(dfs, ignore_index=True)
    
    # æˆªå–åˆ°ç›®æ ‡å¤§å°
    if len(full_df) > N_TOTAL:
        full_df = full_df.iloc[:N_TOTAL].reset_index(drop=True)
    
    actual_size = len(full_df)
    elapsed_time = time.time() - start_time
    
    print(f"âœ… æ•°æ®è·å–å®Œæˆ!")
    print(f"  å®é™…æ ·æœ¬æ•°: {actual_size:,}")
    print(f"  ç›®æ ‡æ ·æœ¬æ•°: {N_TOTAL:,}")
    print(f"  å®Œæˆç‡: {actual_size/N_TOTAL*100:.1f}%")
    print(f"  é”™è¯¯æ•°é‡: {error_count}")
    print(f"  æ€»è€—æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ")
    
    # éªŒè¯æ•°æ®è´¨é‡
    print(f"\nğŸ” æ•°æ®è´¨é‡æ£€æŸ¥...")
    print(f"  åˆ—æ•°: {len(full_df.columns)}")
    print(f"  åˆ—å: {list(full_df.columns)}")
    print(f"  ç¼ºå¤±å€¼: {full_df.isnull().sum().sum()}")
    
    # æ£€æŸ¥ç›®æ ‡å˜é‡
    target_stats = full_df[TARGET_PROP].describe()
    print(f"  ç›®æ ‡å˜é‡ ({TARGET_PROP}):")
    print(f"    å‡å€¼: {target_stats['mean']:.3f}")
    print(f"    æ ‡å‡†å·®: {target_stats['std']:.3f}")
    print(f"    èŒƒå›´: {target_stats['min']:.3f} ~ {target_stats['max']:.3f}")
    
    # ä¿å­˜ç¼“å­˜
    print(f"\nğŸ’¾ ä¿å­˜ç¼“å­˜æ–‡ä»¶...")
    try:
        cache_path = Path(CACHE_FILE)
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºç®€å•çš„DataFrameæ ¼å¼ï¼ˆé¿å…å¤æ‚çš„dictç»“æ„ï¼‰
        with open(cache_path, "wb") as f:
            pickle.dump(full_df, f)
        
        print(f"âœ… ç¼“å­˜å·²ä¿å­˜: {cache_path}")
        print(f"  æ–‡ä»¶å¤§å°: {cache_path.stat().st_size / (1024*1024):.1f} MB")
        
        # éªŒè¯ç¼“å­˜æ–‡ä»¶
        print(f"ğŸ” éªŒè¯ç¼“å­˜æ–‡ä»¶...")
        with open(cache_path, "rb") as f:
            test_load = pickle.load(f)
        
        if isinstance(test_load, pd.DataFrame) and len(test_load) == actual_size:
            print(f"âœ… ç¼“å­˜æ–‡ä»¶éªŒè¯æˆåŠŸ")
            return True
        else:
            print(f"âŒ ç¼“å­˜æ–‡ä»¶éªŒè¯å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
        return False

def test_4k_data_loading():
    """
    æµ‹è¯•4Kæ•°æ®åŠ è½½
    Test 4K data loading
    """
    print(f"\nğŸ§ª æµ‹è¯•4Kæ•°æ®åŠ è½½...")
    
    try:
        # æµ‹è¯•é€šè¿‡data_methodsåŠ è½½
        from methods.data_methods import fetch_data
        
        start_time = time.time()
        df = fetch_data(cache=True)
        load_time = time.time() - start_time
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
        print(f"  åŠ è½½æ—¶é—´: {load_time:.1f} ç§’")
        print(f"  æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"  å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / (1024*1024):.1f} MB")
        
        # æ£€æŸ¥æ•°æ®å†…å®¹
        print(f"  å‰5è¡Œé¢„è§ˆ:")
        print(df.head()[['material_id', 'formula_pretty', TARGET_PROP]])
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ¯ 4Kæ•°æ®é›†ç”Ÿæˆå’ŒéªŒè¯")
    print("ğŸ¯ 4K Dataset Generation and Validation")
    
    try:
        # ç”Ÿæˆ4Kæ•°æ®
        success = generate_4k_data_safe()
        
        if success:
            # æµ‹è¯•æ•°æ®åŠ è½½
            test_success = test_4k_data_loading()
            
            if test_success:
                print(f"\nğŸ‰ 4Kæ•°æ®é›†ç”Ÿæˆå’ŒéªŒè¯å®Œæˆ!")
                print(f"ğŸ‰ 4K Dataset Generation and Validation Complete!")
                print(f"ğŸ“ ç°åœ¨å¯ä»¥ä½¿ç”¨4Kæ•°æ®é›†è¿›è¡ŒPPOè®­ç»ƒ")
            else:
                print(f"\nâš ï¸ æ•°æ®ç”ŸæˆæˆåŠŸä½†åŠ è½½æµ‹è¯•å¤±è´¥")
        else:
            print(f"\nâŒ 4Kæ•°æ®é›†ç”Ÿæˆå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
