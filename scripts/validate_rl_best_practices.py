"""
éªŒè¯RL agentèƒ½å¦é‡æ–°å‘ç°æ ‡å‡†æœ€ä½³å®è·µ
Validate that RL agent can re-discover standard best practices

è¿™ä¸ªè„šæœ¬å°†ï¼š
1. å®šä¹‰æ ‡å‡†çš„MLæµæ°´çº¿æœ€ä½³å®è·µåºåˆ—
2. è®­ç»ƒPPO agentï¼Œè®©å®ƒè‡ªä¸»æ¢ç´¢
3. æ”¶é›†PPOå‘ç°çš„top-5æœ€ä¼˜åºåˆ—
4. å¯¹æ¯”PPOå‘ç°çš„åºåˆ—ä¸æ ‡å‡†æœ€ä½³å®è·µçš„ç›¸ä¼¼åº¦
5. ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple
from collections import Counter, defaultdict
import json
from datetime import datetime

from env.pipeline_env import PipelineEnv
from ppo.trainer import PPOTrainer
from pipeline import run_pipeline_config


# ===================== æ ‡å‡†æœ€ä½³å®è·µå®šä¹‰ =====================

BEST_PRACTICE_SEQUENCES = {
    "minimal": {
        "sequence": ['N0', 'N2', 'N8', 'N9'],
        "description": "æœ€å°æµç¨‹ï¼šæ•°æ®è·å– â†’ ç‰¹å¾æ„å»º â†’ æ¨¡å‹è®­ç»ƒ â†’ ç»“æŸ",
        "expected_performance": "baseline"
    },
    "standard": {
        "sequence": ['N0', 'N2', 'N1', 'N6', 'N7', 'N8', 'N9'],
        "description": "æ ‡å‡†æµç¨‹ï¼šè·å– â†’ ç‰¹å¾ â†’ å¡«å…… â†’ é€‰æ‹© â†’ ç¼©æ”¾ â†’ è®­ç»ƒ â†’ ç»“æŸ",
        "expected_performance": "good"
    },
    "standard_with_cleaning": {
        "sequence": ['N0', 'N2', 'N3', 'N1', 'N6', 'N7', 'N8', 'N9'],
        "description": "å¸¦æ¸…æ´—ï¼šè·å– â†’ ç‰¹å¾ â†’ æ¸…æ´— â†’ å¡«å…… â†’ é€‰æ‹© â†’ ç¼©æ”¾ â†’ è®­ç»ƒ â†’ ç»“æŸ",
        "expected_performance": "better"
    },
    "advanced_with_gnn": {
        "sequence": ['N0', 'N2', 'N4', 'N1', 'N6', 'N7', 'N8', 'N9'],
        "description": "GNNå¢å¼ºï¼šè·å– â†’ ç‰¹å¾ â†’ GNN â†’ å¡«å…… â†’ é€‰æ‹© â†’ ç¼©æ”¾ â†’ è®­ç»ƒ â†’ ç»“æŸ",
        "expected_performance": "advanced"
    },
    "full_with_kg": {
        "sequence": ['N0', 'N2', 'N3', 'N4', 'N5', 'N1', 'N6', 'N7', 'N8', 'N9'],
        "description": "å®Œæ•´æµç¨‹ï¼šè·å– â†’ ç‰¹å¾ â†’ æ¸…æ´— â†’ GNN â†’ çŸ¥è¯†å›¾è°± â†’ å¡«å…… â†’ é€‰æ‹© â†’ ç¼©æ”¾ â†’ è®­ç»ƒ â†’ ç»“æŸ",
        "expected_performance": "best"
    }
}


# ===================== PPOè®­ç»ƒå’Œåºåˆ—æ”¶é›† =====================

def train_ppo_and_collect_sequences(
    episodes: int = 100,
    max_steps: int = 15,
    verbose: bool = True
) -> Tuple[List[List[str]], List[float], PPOTrainer]:
    """
    è®­ç»ƒPPOå¹¶æ”¶é›†æ‰€æœ‰å°è¯•çš„åºåˆ—
    
    Returns:
        sequences: æ‰€æœ‰å®Œæ•´çš„åºåˆ—åˆ—è¡¨
        rewards: å¯¹åº”çš„å¥–åŠ±åˆ—è¡¨
        trainer: è®­ç»ƒå¥½çš„PPO trainer
    """
    print("=" * 70)
    print("ğŸš€ å¼€å§‹PPOè®­ç»ƒï¼Œæ¢ç´¢æœ€ä¼˜æµæ°´çº¿åºåˆ—")
    print("=" * 70)
    
    env = PipelineEnv()
    trainer = PPOTrainer(
        env,
        learning_rate=3e-4,
        clip_ratio=0.2,
        hidden_size=128,
        max_steps_per_episode=max_steps
    )
    
    sequences = []
    rewards = []
    
    for episode in range(episodes):
        obs = env.reset()
        episode_sequence = []
        episode_reward = 0.0
        done = False
        steps = 0
        
        while not done and steps < max_steps:
            action, _ = trainer.select_action(obs)
            node_idx = action['node']
            node_name = env.pipeline_nodes[node_idx]
            episode_sequence.append(node_name)
            
            obs, reward, done, _, _ = env.step(action)
            episode_reward += reward
            steps += 1
            
            if done:
                break
        
        # åªè®°å½•å®Œæ•´åºåˆ—ï¼ˆä»¥N9ç»“æŸï¼‰
        if episode_sequence and episode_sequence[-1] == 'N9':
            sequences.append(episode_sequence)
            rewards.append(episode_reward)
            
            if verbose and (episode + 1) % 10 == 0:
                recent_avg = np.mean(rewards[-10:]) if len(rewards) >= 10 else np.mean(rewards)
                print(f"Episode {episode + 1}/{episodes}: "
                      f"Reward={episode_reward:.3f}, "
                      f"Length={len(episode_sequence)}, "
                      f"Recent Avg={recent_avg:.3f}")
        
        # è¿›è¡ŒPPOæ›´æ–°
        if episode > 0 and episode % 10 == 0:
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è°ƒç”¨trainerçš„updateæ–¹æ³•
            pass
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ”¶é›†åˆ° {len(sequences)} ä¸ªå®Œæ•´åºåˆ—")
    return sequences, rewards, trainer


# ===================== åºåˆ—åˆ†æå’Œå¯¹æ¯” =====================

def calculate_sequence_similarity(seq1: List[str], seq2: List[str]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªåºåˆ—çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºç¼–è¾‘è·ç¦»å’Œå…³é”®èŠ‚ç‚¹ä½ç½®ï¼‰
    
    Returns:
        similarity: 0.0-1.0ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°
    """
    # 1. ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦ï¼ˆLevenshteinï¼‰
    def levenshtein_distance(s1, s2):
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)
        
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    edit_dist = levenshtein_distance(seq1, seq2)
    max_len = max(len(seq1), len(seq2))
    edit_similarity = 1.0 - (edit_dist / max_len) if max_len > 0 else 0.0
    
    # 2. å…³é”®èŠ‚ç‚¹é¡ºåºç›¸ä¼¼åº¦
    key_nodes = ['N0', 'N2', 'N8', 'N9']
    seq1_key_positions = [seq1.index(n) if n in seq1 else -1 for n in key_nodes]
    seq2_key_positions = [seq2.index(n) if n in seq2 else -1 for n in key_nodes]
    
    key_order_matches = sum(1 for p1, p2 in zip(seq1_key_positions, seq2_key_positions) 
                           if p1 >= 0 and p2 >= 0 and p1 == p2)
    key_similarity = key_order_matches / len(key_nodes)
    
    # 3. èŠ‚ç‚¹å­˜åœ¨æ€§ç›¸ä¼¼åº¦
    set1 = set(seq1)
    set2 = set(seq2)
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    set_similarity = intersection / union if union > 0 else 0.0
    
    # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
    similarity = (0.4 * edit_similarity + 
                  0.4 * key_similarity + 
                  0.2 * set_similarity)
    
    return similarity


def find_top_sequences(
    sequences: List[List[str]], 
    rewards: List[float], 
    top_k: int = 5
) -> List[Tuple[List[str], float]]:
    """
    æ‰¾å‡ºtop-kæœ€ä¼˜åºåˆ—
    """
    # æŒ‰å¥–åŠ±æ’åº
    paired = list(zip(sequences, rewards))
    paired.sort(key=lambda x: x[1], reverse=True)
    
    # å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„ï¼‰
    seen = set()
    unique_top = []
    for seq, reward in paired:
        seq_tuple = tuple(seq)
        if seq_tuple not in seen:
            seen.add(seq_tuple)
            unique_top.append((seq, reward))
            if len(unique_top) >= top_k:
                break
    
    return unique_top


def analyze_node_usage(sequences: List[List[str]]) -> Dict[str, Dict]:
    """
    åˆ†æèŠ‚ç‚¹ä½¿ç”¨é¢‘ç‡å’Œä½ç½®
    """
    node_count = Counter()
    node_positions = defaultdict(list)
    
    for seq in sequences:
        for pos, node in enumerate(seq):
            node_count[node] += 1
            node_positions[node].append(pos)
    
    analysis = {}
    for node in node_count:
        positions = node_positions[node]
        analysis[node] = {
            'count': node_count[node],
            'frequency': node_count[node] / len(sequences),
            'avg_position': np.mean(positions),
            'std_position': np.std(positions),
            'typical_position': int(np.median(positions))
        }
    
    return analysis


def compare_with_best_practices(
    ppo_sequences: List[List[str]],
    ppo_rewards: List[float]
) -> pd.DataFrame:
    """
    å¯¹æ¯”PPOå‘ç°çš„åºåˆ—ä¸æ ‡å‡†æœ€ä½³å®è·µ
    """
    print("\n" + "=" * 70)
    print("ğŸ“Š å¯¹æ¯”PPOå‘ç°çš„åºåˆ—ä¸æ ‡å‡†æœ€ä½³å®è·µ")
    print("=" * 70)
    
    # è·å–PPOçš„top-5åºåˆ—
    top_ppo = find_top_sequences(ppo_sequences, ppo_rewards, top_k=5)
    
    results = []
    
    # å¯¹æ¯”æ¯ä¸ªæœ€ä½³å®è·µ
    for bp_name, bp_info in BEST_PRACTICE_SEQUENCES.items():
        bp_seq = bp_info['sequence']
        
        print(f"\nğŸ“Œ æœ€ä½³å®è·µ: {bp_name}")
        print(f"   æè¿°: {bp_info['description']}")
        print(f"   åºåˆ—: {' â†’ '.join(bp_seq)}")
        
        # æ‰¾å‡ºä¸æ­¤æœ€ä½³å®è·µæœ€ç›¸ä¼¼çš„PPOåºåˆ—
        best_match = None
        best_similarity = 0.0
        best_reward = 0.0
        
        for ppo_seq, ppo_reward in top_ppo:
            similarity = calculate_sequence_similarity(bp_seq, ppo_seq)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = ppo_seq
                best_reward = ppo_reward
        
        print(f"   âœ“ æœ€ç›¸ä¼¼çš„PPOåºåˆ—: {' â†’ '.join(best_match) if best_match else 'None'}")
        print(f"   âœ“ ç›¸ä¼¼åº¦: {best_similarity:.2%}")
        print(f"   âœ“ PPOå¥–åŠ±: {best_reward:.3f}")
        
        results.append({
            'best_practice': bp_name,
            'description': bp_info['description'],
            'bp_sequence': ' â†’ '.join(bp_seq),
            'bp_length': len(bp_seq),
            'ppo_match_sequence': ' â†’ '.join(best_match) if best_match else 'None',
            'ppo_match_length': len(best_match) if best_match else 0,
            'similarity': best_similarity,
            'ppo_reward': best_reward,
            'discovered': best_similarity >= 0.7  # 70%ç›¸ä¼¼åº¦è®¤ä¸º"é‡æ–°å‘ç°"
        })
    
    return pd.DataFrame(results)


# ===================== å¯è§†åŒ– =====================

def visualize_results(
    comparison_df: pd.DataFrame,
    node_usage: Dict[str, Dict],
    top_sequences: List[Tuple[List[str], float]],
    output_dir: Path
):
    """
    ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å›¾1: æœ€ä½³å®è·µé‡æ–°å‘ç°ç‡
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # å­å›¾1: ç›¸ä¼¼åº¦å¯¹æ¯”
    ax = axes[0, 0]
    bp_names = comparison_df['best_practice']
    similarities = comparison_df['similarity']
    colors = ['green' if s >= 0.7 else 'orange' if s >= 0.5 else 'red' for s in similarities]
    ax.barh(bp_names, similarities, color=colors, alpha=0.7)
    ax.axvline(x=0.7, color='green', linestyle='--', label='Discovery Threshold (70%)')
    ax.set_xlabel('Similarity Score')
    ax.set_title('PPO Sequence Similarity to Best Practices')
    ax.legend()
    ax.grid(axis='x', alpha=0.3)
    
    # å­å›¾2: èŠ‚ç‚¹ä½¿ç”¨é¢‘ç‡
    ax = axes[0, 1]
    nodes = list(node_usage.keys())
    frequencies = [node_usage[n]['frequency'] for n in nodes]
    ax.bar(nodes, frequencies, color='steelblue', alpha=0.7)
    ax.set_ylabel('Usage Frequency')
    ax.set_title('Node Usage Frequency in PPO Sequences')
    ax.grid(axis='y', alpha=0.3)
    
    # å­å›¾3: èŠ‚ç‚¹å¹³å‡ä½ç½®
    ax = axes[1, 0]
    avg_positions = [node_usage[n]['avg_position'] for n in nodes]
    std_positions = [node_usage[n]['std_position'] for n in nodes]
    ax.errorbar(nodes, avg_positions, yerr=std_positions, fmt='o', 
                capsize=5, capthick=2, color='coral', ecolor='gray')
    ax.set_ylabel('Average Position in Sequence')
    ax.set_title('Node Typical Position (with std)')
    ax.invert_yaxis()
    ax.grid(alpha=0.3)
    
    # å­å›¾4: Top-5åºåˆ—å¥–åŠ±
    ax = axes[1, 1]
    seq_labels = [f"Seq {i+1}\n{len(seq)} nodes" for i, (seq, _) in enumerate(top_sequences)]
    seq_rewards = [reward for _, reward in top_sequences]
    ax.bar(seq_labels, seq_rewards, color='purple', alpha=0.7)
    ax.set_ylabel('Reward')
    ax.set_title('Top-5 PPO Sequences Performance')
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'rl_best_practices_validation.png', dpi=150, bbox_inches='tight')
    print(f"\nğŸ“Š å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜: {output_dir / 'rl_best_practices_validation.png'}")


# ===================== ä¸»å‡½æ•° =====================

def main():
    """
    ä¸»éªŒè¯æµç¨‹
    """
    print("\n" + "=" * 70)
    print("ğŸ¯ RL Agent æœ€ä½³å®è·µé‡æ–°å‘ç°éªŒè¯")
    print("   Validate RL Agent Re-discovery of Best Practices")
    print("=" * 70)
    
    # é…ç½®
    EPISODES = 100
    MAX_STEPS = 15
    OUTPUT_DIR = Path(__file__).parent.parent / 'logs' / f'validation_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    
    # Step 1: è®­ç»ƒPPOå¹¶æ”¶é›†åºåˆ—
    print("\nğŸ“ Step 1: è®­ç»ƒPPO agentå¹¶æ”¶é›†æ¢ç´¢åºåˆ—")
    sequences, rewards, trainer = train_ppo_and_collect_sequences(
        episodes=EPISODES,
        max_steps=MAX_STEPS,
        verbose=True
    )
    
    if not sequences:
        print("\nâŒ é”™è¯¯ï¼šæ²¡æœ‰æ”¶é›†åˆ°å®Œæ•´åºåˆ—ï¼")
        return
    
    # Step 2: åˆ†æèŠ‚ç‚¹ä½¿ç”¨
    print("\nğŸ“ Step 2: åˆ†æèŠ‚ç‚¹ä½¿ç”¨æ¨¡å¼")
    node_usage = analyze_node_usage(sequences)
    print("\nèŠ‚ç‚¹ä½¿ç”¨ç»Ÿè®¡:")
    for node, stats in sorted(node_usage.items()):
        print(f"  {node}: ä½¿ç”¨ {stats['count']} æ¬¡ "
              f"({stats['frequency']:.1%}), "
              f"å¹³å‡ä½ç½® {stats['avg_position']:.1f}")
    
    # Step 3: å¯¹æ¯”æœ€ä½³å®è·µ
    print("\nğŸ“ Step 3: å¯¹æ¯”PPOå‘ç°çš„åºåˆ—ä¸æ ‡å‡†æœ€ä½³å®è·µ")
    comparison_df = compare_with_best_practices(sequences, rewards)
    
    # Step 4: ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ Step 4: ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_df.to_csv(OUTPUT_DIR / 'best_practices_comparison.csv', index=False)
    print(f"   âœ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜: {OUTPUT_DIR / 'best_practices_comparison.csv'}")
    
    # ä¿å­˜topåºåˆ—
    top_sequences = find_top_sequences(sequences, rewards, top_k=5)
    with open(OUTPUT_DIR / 'top_5_sequences.json', 'w') as f:
        json.dump([
            {
                'sequence': seq,
                'reward': float(reward),
                'length': len(seq)
            }
            for seq, reward in top_sequences
        ], f, indent=2)
    print(f"   âœ“ Top-5åºåˆ—å·²ä¿å­˜: {OUTPUT_DIR / 'top_5_sequences.json'}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    visualize_results(comparison_df, node_usage, top_sequences, OUTPUT_DIR)
    
    # Step 5: æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ“ˆ éªŒè¯ç»“æœæ€»ç»“")
    print("=" * 70)
    
    discovered_count = comparison_df['discovered'].sum()
    total_practices = len(comparison_df)
    discovery_rate = discovered_count / total_practices
    
    print(f"\nâœ… é‡æ–°å‘ç°ç‡: {discovered_count}/{total_practices} ({discovery_rate:.1%})")
    print(f"   - å¹³å‡ç›¸ä¼¼åº¦: {comparison_df['similarity'].mean():.2%}")
    print(f"   - æœ€é«˜ç›¸ä¼¼åº¦: {comparison_df['similarity'].max():.2%}")
    print(f"   - æœ€ä½³PPOå¥–åŠ±: {comparison_df['ppo_reward'].max():.3f}")
    
    if discovery_rate >= 0.6:
        print("\nğŸ‰ ç»“è®º: PPO agentæˆåŠŸé‡æ–°å‘ç°äº†æ ‡å‡†æœ€ä½³å®è·µï¼")
    elif discovery_rate >= 0.4:
        print("\nâš ï¸  ç»“è®º: PPO agentéƒ¨åˆ†é‡æ–°å‘ç°äº†æœ€ä½³å®è·µï¼Œéœ€è¦ç»§ç»­è®­ç»ƒ")
    else:
        print("\nâŒ ç»“è®º: PPO agentæœªèƒ½æœ‰æ•ˆé‡æ–°å‘ç°æœ€ä½³å®è·µï¼Œéœ€è¦è°ƒæ•´è®­ç»ƒç­–ç•¥")
    
    print(f"\nğŸ“ å®Œæ•´æŠ¥å‘Šä¿å­˜åœ¨: {OUTPUT_DIR}")
    print("=" * 70)


if __name__ == '__main__':
    main()
