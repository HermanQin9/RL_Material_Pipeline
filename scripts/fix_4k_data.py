#!/usr/bin/env python3
"""
ä¿®å¤4Kæ•°æ®é›†çš„ç”Ÿæˆé—®é¢˜
Fix 4K Dataset Generation Issues
"""
import os
import sys
import numpy as np
import pandas as pd
import pickle
from pathlib import Path
from tqdm import tqdm
from mp_api.client import MPRester

# Add project root to path
sys.path.append('..')
from config import API_KEY, TARGET_PROP

def fix_4k_data_generation():
    """
    é‡æ–°ç”Ÿæˆå®‰å…¨çš„4Kæ•°æ®é›†ï¼Œé¿å…featurizationé”™è¯¯
    Regenerate safe 4K dataset avoiding featurization errors
    """
    print("ğŸ”§ ä¿®å¤4Kæ•°æ®é›†ç”Ÿæˆ")
    print("ğŸ”§ Fixing 4K Dataset Generation")
    print("=" * 60)
    
    # é…ç½®å‚æ•°
    N_TARGET = 4000
    BATCH_SIZE = 100
    cache_file = Path("data/processed/mp_data_cache_4k.pkl")
    
    print(f"ğŸ“Š ç›®æ ‡æ ·æœ¬æ•°: {N_TARGET}")
    print(f"ğŸ“Š Target samples: {N_TARGET}")
    print(f"ğŸ”— API Key: {'å·²è®¾ç½®' if API_KEY else 'æœªè®¾ç½®'}")
    print(f"ğŸ”— API Key: {'Set' if API_KEY else 'Not set'}")
    print(f"ğŸ’¾ ç¼“å­˜æ–‡ä»¶: {cache_file}")
    print(f"ğŸ’¾ Cache file: {cache_file}")
    print()
    
    if not API_KEY:
        print("âŒ é”™è¯¯: API_KEY æœªè®¾ç½®")
        print("âŒ Error: API_KEY not set")
        return False
    
    try:
        # è·å–æ•°æ®
        print("ğŸ“¥ è·å–ææ–™æ•°æ®...")
        print("ğŸ“¥ Fetching material data...")
        
        all_data = []
        with MPRester(API_KEY) as mpr:
            # åˆ†æ‰¹è·å–æ•°æ®
            docs_iter = mpr.materials.summary.search(
                fields=["material_id", "structure", "elements", "formula_pretty", TARGET_PROP],
                chunk_size=BATCH_SIZE,
                num_chunks=(N_TARGET // BATCH_SIZE) + 2  # å¤šè·å–ä¸€äº›ä»¥é˜²è¿‡æ»¤åä¸å¤Ÿ
            )
            
            for docs in tqdm(docs_iter, desc="è·å–MPæ•°æ®"):
                if not docs:
                    continue
                    
                docs = docs if isinstance(docs, list) else [docs]
                
                for doc in docs:
                    # å®‰å…¨åœ°æå–æ•°æ®
                    try:
                        # æ£€æŸ¥å¿…éœ€å­—æ®µ
                        if not hasattr(doc, 'material_id') or not hasattr(doc, 'structure'):
                            continue
                        if not hasattr(doc, TARGET_PROP) or getattr(doc, TARGET_PROP) is None:
                            continue
                            
                        material_id = getattr(doc, 'material_id', None)
                        structure = getattr(doc, 'structure', None)
                        elements = getattr(doc, 'elements', None)
                        formula_pretty = getattr(doc, 'formula_pretty', None)
                        formation_energy = getattr(doc, TARGET_PROP, None)
                        
                        # éªŒè¯å…³é”®å­—æ®µ
                        if not all([material_id, structure, formation_energy is not None]):
                            continue
                        
                        # éªŒè¯ç»“æ„å¯¹è±¡
                        if not hasattr(structure, 'composition'):
                            continue
                            
                        composition = structure.composition
                        if composition is None:
                            continue
                        
                        # æ£€æŸ¥æ‰€æœ‰å…ƒç´ éƒ½æœ‰atomic_radiuså±æ€§
                        skip_this = False
                        for element in composition.elements:
                            if not hasattr(element, 'atomic_radius') or element.atomic_radius is None:
                                skip_this = True
                                break
                        
                        if skip_this:
                            continue
                        
                        # æ·»åŠ åˆ°åˆ—è¡¨
                        all_data.append({
                            'material_id': material_id,
                            'structure': structure,
                            'elements': elements,
                            'formula_pretty': formula_pretty,
                            TARGET_PROP: formation_energy,
                            'composition': composition
                        })
                        
                        # æ£€æŸ¥æ˜¯å¦å·²ç»è·å¾—è¶³å¤Ÿçš„æ•°æ®
                        if len(all_data) >= N_TARGET:
                            break
                            
                    except Exception as e:
                        # è·³è¿‡æœ‰é—®é¢˜çš„æ¡ç›®
                        continue
                
                if len(all_data) >= N_TARGET:
                    break
        
        print(f"âœ… æˆåŠŸè·å– {len(all_data)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        print(f"âœ… Successfully fetched {len(all_data)} valid samples")
        
        if len(all_data) < N_TARGET:
            print(f"âš ï¸ è·å–çš„æ ·æœ¬æ•° ({len(all_data)}) å°‘äºç›®æ ‡ ({N_TARGET})")
            print(f"âš ï¸ Fetched samples ({len(all_data)}) less than target ({N_TARGET})")
        
        # æˆªå–åˆ°ç›®æ ‡æ•°é‡
        all_data = all_data[:N_TARGET]
        
        # è½¬æ¢ä¸ºDataFrame
        print("ğŸ”„ è½¬æ¢ä¸ºDataFrame...")
        print("ğŸ”„ Converting to DataFrame...")
        df = pd.DataFrame(all_data)
        
        # åŸºæœ¬æ•°æ®éªŒè¯
        print("ğŸ” æ•°æ®éªŒè¯...")
        print("ğŸ” Data validation...")
        print(f"  æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"  Data shape: {df.shape}")
        print(f"  åˆ—å: {list(df.columns)}")
        print(f"  Column names: {list(df.columns)}")
        print(f"  ç¼ºå¤±å€¼æ•°é‡: {df.isnull().sum().sum()}")
        print(f"  Missing values: {df.isnull().sum().sum()}")
        
        # ç›®æ ‡å˜é‡ç»Ÿè®¡
        target_values = df[TARGET_PROP].values
        print(f"  ç›®æ ‡å˜é‡ç»Ÿè®¡:")
        print(f"  Target variable statistics:")
        print(f"    å‡å€¼: {np.mean(target_values):.3f}")
        print(f"    Mean: {np.mean(target_values):.3f}")
        print(f"    æ ‡å‡†å·®: {np.std(target_values):.3f}")
        print(f"    Std: {np.std(target_values):.3f}")
        print(f"    èŒƒå›´: {np.min(target_values):.3f} ~ {np.max(target_values):.3f}")
        print(f"    Range: {np.min(target_values):.3f} ~ {np.max(target_values):.3f}")
        
        # ä¿å­˜ç¼“å­˜æ–‡ä»¶
        print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜æ–‡ä»¶...")
        print(f"ğŸ’¾ Saving cache file...")
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'wb') as f:
            pickle.dump(df, f)
        
        # éªŒè¯æ–‡ä»¶
        file_size = cache_file.stat().st_size / (1024 * 1024)  # MB
        print(f"âœ… ç¼“å­˜æ–‡ä»¶å·²ä¿å­˜: {cache_file}")
        print(f"âœ… Cache file saved: {cache_file}")
        print(f"  æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
        print(f"  File size: {file_size:.1f} MB")
        
        # æµ‹è¯•åŠ è½½
        print("ğŸ§ª æµ‹è¯•æ–‡ä»¶åŠ è½½...")
        print("ğŸ§ª Testing file loading...")
        with open(cache_file, 'rb') as f:
            test_df = pickle.load(f)
        print(f"âœ… åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {test_df.shape}")
        print(f"âœ… Loading successful, shape: {test_df.shape}")
        
        print()
        print("ğŸ‰ 4Kæ•°æ®é›†ä¿®å¤å®Œæˆ!")
        print("ğŸ‰ 4K Dataset Fix Complete!")
        print(f"ğŸ“ ç¼“å­˜æ–‡ä»¶ä½ç½®: {cache_file}")
        print(f"ğŸ“ Cache file location: {cache_file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ 4Kæ•°æ®é›†ä¿®å¤å¤±è´¥: {e}")
        print(f"âŒ 4K Dataset fix failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = fix_4k_data_generation()
    if success:
        print("\nâœ… ç°åœ¨å¯ä»¥å°è¯•ä½¿ç”¨4Kæ•°æ®é›†è¿›è¡ŒPPOè®­ç»ƒ")
        print("âœ… Now you can try PPO training with 4K dataset")
    else:
        print("\nâŒ ä¿®å¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        print("âŒ Fix failed, please check error messages")
