#!/usr/bin/env python3
"""
Pipelineæµ‹è¯•è„šæœ¬ / Pipeline testing script

æµ‹è¯•åŸºç¡€pipelineæ‰§è¡ŒåŠŸèƒ½ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹æ‰§è¡Œã€çŠ¶æ€ç®¡ç†å’Œæ•°æ®æµè½¬ã€‚
Tests basic pipeline execution, including node execution, state management, and data flow.
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
import logging
from typing import Dict, Any

from pipeline import run_pipeline, run_pipeline_config
from nodes import (
    DataFetchNode,
    FeatureMatrixNode,
    ImputeNode,
    FeatureSelectionNode,
    ScalingNode,
    ModelTrainingNode
)
from env.pipeline_env import PipelineEnv

# é…ç½®æ—¥å¿— / Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_basic_pipeline():
    """
    æµ‹è¯•åŸºç¡€pipelineæ‰§è¡Œ / Test basic pipeline execution
    ä½¿ç”¨é»˜è®¤å‚æ•°æ‰§è¡Œå®Œæ•´pipeline
    """
    print("\n" + "="*60)
    print("æµ‹è¯•1: åŸºç¡€Pipelineæ‰§è¡Œ / Test 1: Basic Pipeline Execution")
    print("="*60)
    
    try:
        result = run_pipeline(
            cache=True,
            impute_strategy='mean',
            nan_thresh=0.5,
            train_val_ratio=0.8,
            selection_strategy='none',
            scaling_strategy='standard',
            model_strategy='rf',
            model_params={'n_estimators': 10}
        )
        
        # éªŒè¯ç»“æœ / Verify results
        assert result is not None, "Pipelineè¿”å›None"
        assert 'model' in result, "ç»“æœä¸­ç¼ºå°‘model"
        assert 'y_val_pred' in result, "ç»“æœä¸­ç¼ºå°‘y_val_pred"
        
        # æ£€æŸ¥é¢„æµ‹ç²¾åº¦ / Check prediction accuracy
        mae = result.get('mae')
        r2 = result.get('r2')
        
        print(f"\nâœ… åŸºç¡€Pipelineæµ‹è¯•é€šè¿‡")
        print(f"   MAE: {mae:.4f}" if mae else "   MAE: N/A")
        print(f"   RÂ²: {r2:.4f}" if r2 else "   RÂ²: N/A")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ åŸºç¡€Pipelineæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_node_execution():
    """
    æµ‹è¯•å•ä¸ªèŠ‚ç‚¹æ‰§è¡Œ / Test individual node execution
    éªŒè¯å„èŠ‚ç‚¹çš„è¾“å…¥è¾“å‡ºæ­£ç¡®æ€§
    """
    print("\n" + "="*60)
    print("æµ‹è¯•2: å•ä¸ªèŠ‚ç‚¹æ‰§è¡Œ / Test 2: Individual Node Execution")
    print("="*60)
    
    try:
        # æµ‹è¯•DataFetchNode
        print("\nğŸ“¦ æµ‹è¯•DataFetchNode...")
        n0 = DataFetchNode()
        data = n0.execute('api', {'cache': True}, {})
        assert 'X_train' in data, "DataFetchNodeè¾“å‡ºç¼ºå°‘X_train"
        assert 'y_train' in data, "DataFetchNodeè¾“å‡ºç¼ºå°‘y_train"
        print(f"   âœ“ DataFetchNode: train={data['X_train'].shape[0]} samples")
        
        # æµ‹è¯•FeatureMatrixNode
        print("\nğŸ”§ æµ‹è¯•FeatureMatrixNode...")
        n2 = FeatureMatrixNode()
        features = n2.execute('construct', {
            'nan_thresh': 0.5,
            'train_val_ratio': 0.8,
            'verbose': False
        }, data)
        assert 'X_train' in features, "FeatureMatrixNodeè¾“å‡ºç¼ºå°‘X_train"
        assert 'feature_names' in features, "FeatureMatrixNodeè¾“å‡ºç¼ºå°‘feature_names"
        print(f"   âœ“ FeatureMatrixNode: {features['X_train'].shape[1]} features")
        
        # æµ‹è¯•ImputeNode
        print("\nğŸ©¹ æµ‹è¯•ImputeNode...")
        n1 = ImputeNode()
        imputed = n1.execute('impute', {'strategy': 'mean'}, features)
        assert 'X_train' in imputed, "ImputeNodeè¾“å‡ºç¼ºå°‘X_train"
        assert not np.isnan(imputed['X_train']).any(), "ImputeNodeæœªèƒ½æ¶ˆé™¤æ‰€æœ‰NaN"
        print(f"   âœ“ ImputeNode: æ— NaNå€¼")
        
        # æµ‹è¯•ScalingNode
        print("\nğŸ“ æµ‹è¯•ScalingNode...")
        n4 = ScalingNode()
        scaled = n4.execute('scale', {'strategy': 'standard'}, imputed)
        assert 'X_train' in scaled, "ScalingNodeè¾“å‡ºç¼ºå°‘X_train"
        assert 'scaler' in scaled, "ScalingNodeè¾“å‡ºç¼ºå°‘scaler"
        print(f"   âœ“ ScalingNode: meanâ‰ˆ{np.mean(scaled['X_train']):.2f}, stdâ‰ˆ{np.std(scaled['X_train']):.2f}")
        
        # æµ‹è¯•ModelTrainingNode
        print("\nğŸ¤– æµ‹è¯•ModelTrainingNode...")
        n5 = ModelTrainingNode()
        trained = n5.execute('train_rf', {'n_estimators': 10}, scaled)
        assert 'model' in trained, "ModelTrainingNodeè¾“å‡ºç¼ºå°‘model"
        assert 'y_val_pred' in trained, "ModelTrainingNodeè¾“å‡ºç¼ºå°‘y_val_pred"
        print(f"   âœ“ ModelTrainingNode: æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        print(f"\nâœ… å•ä¸ªèŠ‚ç‚¹æ‰§è¡Œæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"\nâŒ å•ä¸ªèŠ‚ç‚¹æ‰§è¡Œæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_pipeline_config():
    """
    æµ‹è¯•10èŠ‚ç‚¹çµæ´»pipelineé…ç½® / Test 10-node flexible pipeline configuration
    ä½¿ç”¨run_pipeline_configæ‰§è¡Œä¸åŒçš„èŠ‚ç‚¹åºåˆ—
    """
    print("\n" + "="*60)
    print("æµ‹è¯•3: çµæ´»Pipelineé…ç½® / Test 3: Flexible Pipeline Configuration")
    print("="*60)
    
    try:
        # å®šä¹‰ä¸€ä¸ªç®€å•çš„åºåˆ—: N0 â†’ N2 â†’ N1 â†’ N7 â†’ N8 â†’ N9
        config = {
            'sequence': ['N0', 'N2', 'N1', 'N7', 'N8', 'N9'],
            'N1_method': 'impute',
            'N1_params': {'strategy': 'mean'},
            'N7_method': 'scale',
            'N7_params': {'strategy': 'standard'},
            'N8_method': 'train_rf',
            'N8_params': {'n_estimators': 10},
            'cache': True,
            'nan_thresh': 0.5,
            'train_val_ratio': 0.8
        }
        
        print(f"\nğŸ”„ æ‰§è¡Œåºåˆ—: {' â†’ '.join(config['sequence'])}")
        result = run_pipeline_config(**config)
        
        # éªŒè¯ç»“æœ
        assert result is not None, "Pipelineè¿”å›None"
        assert 'model' in result, "ç»“æœä¸­ç¼ºå°‘model"
        
        print(f"\nâœ… çµæ´»Pipelineé…ç½®æµ‹è¯•é€šè¿‡")
        print(f"   æ‰§è¡Œæ—¶é—´: {result.get('total_time', 'N/A'):.2f}s" if 'total_time' in result else "")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ çµæ´»Pipelineé…ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_different_strategies():
    """
    æµ‹è¯•ä¸åŒçš„ç­–ç•¥ç»„åˆ / Test different strategy combinations
    éªŒè¯pipelineå¯¹ä¸åŒå‚æ•°çš„é€‚åº”æ€§
    """
    print("\n" + "="*60)
    print("æµ‹è¯•4: ä¸åŒç­–ç•¥ç»„åˆ / Test 4: Different Strategy Combinations")
    print("="*60)
    
    strategies = [
        {
            'name': 'Mean + Standard + RF',
            'impute': 'mean',
            'scaling': 'standard',
            'model': 'rf'
        },
        {
            'name': 'Median + Robust + GBR',
            'impute': 'median',
            'scaling': 'robust',
            'model': 'gbr'
        },
        {
            'name': 'KNN + MinMax + XGB',
            'impute': 'knn',
            'scaling': 'minmax',
            'model': 'xgb'
        }
    ]
    
    results = []
    
    for strategy in strategies:
        print(f"\nğŸ”§ æµ‹è¯•ç­–ç•¥: {strategy['name']}")
        try:
            result = run_pipeline(
                cache=True,
                impute_strategy=strategy['impute'],
                impute_params={'n_neighbors': 5} if strategy['impute'] == 'knn' else None,
                scaling_strategy=strategy['scaling'],
                model_strategy=strategy['model'],
                model_params={'n_estimators': 10}
            )
            
            mae = result.get('mae', float('inf'))
            r2 = result.get('r2', 0.0)
            
            print(f"   âœ“ MAE: {mae:.4f}, RÂ²: {r2:.4f}")
            results.append({'strategy': strategy['name'], 'mae': mae, 'r2': r2, 'success': True})
            
        except Exception as e:
            print(f"   âœ— å¤±è´¥: {e}")
            results.append({'strategy': strategy['name'], 'success': False, 'error': str(e)})
    
    # ç»Ÿè®¡æˆåŠŸç‡
    success_count = sum(1 for r in results if r.get('success', False))
    print(f"\nâœ… ç­–ç•¥æµ‹è¯•å®Œæˆ: {success_count}/{len(strategies)} æˆåŠŸ")
    
    return success_count == len(strategies)


def test_pipeline_with_ppo_env():
    """
    æµ‹è¯•Pipelineä¸PPOç¯å¢ƒçš„é›†æˆ / Test pipeline integration with PPO environment
    éªŒè¯ç¯å¢ƒåˆå§‹åŒ–å’ŒåŸºæœ¬stepæ“ä½œ
    """
    print("\n" + "="*60)
    print("æµ‹è¯•5: PPOç¯å¢ƒé›†æˆ / Test 5: PPO Environment Integration")
    print("="*60)
    
    try:
        print("\nğŸ¤– åˆå§‹åŒ–PPOç¯å¢ƒ...")
        env = PipelineEnv()
        
        print("\nğŸ”„ é‡ç½®ç¯å¢ƒ...")
        obs = env.reset()
        
        # éªŒè¯è§‚å¯Ÿç©ºé—´
        assert 'fingerprint' in obs, "è§‚å¯Ÿç¼ºå°‘fingerprint"
        assert 'node_visited' in obs, "è§‚å¯Ÿç¼ºå°‘node_visited"
        assert 'action_mask' in obs, "è§‚å¯Ÿç¼ºå°‘action_mask"
        
        print(f"   âœ“ è§‚å¯Ÿç©ºé—´ç»´åº¦: fingerprint={len(obs['fingerprint'])}, node_visited={len(obs['node_visited'])}")
        print(f"   âœ“ å¯ç”¨åŠ¨ä½œæ•°: {np.sum(obs['action_mask'])}")
        
        # æ‰§è¡Œä¸€ä¸ªéšæœºåŠ¨ä½œ
        print("\nğŸ® æ‰§è¡ŒéšæœºåŠ¨ä½œ...")
        valid_actions = np.where(obs['action_mask'])[0]
        if len(valid_actions) > 0:
            action_idx = np.random.choice(valid_actions)
            
            # éœ€è¦æ„å»ºå®Œæ•´çš„action dict
            node_idx = action_idx % 10  # å‡è®¾10ä¸ªèŠ‚ç‚¹
            method_idx = 0
            params = [0.5, 0.5, 0.5]
            
            action = {
                'node': node_idx,
                'method': method_idx,
                'params': params
            }
            
            obs, reward, done, truncated, info = env.step(action)
            print(f"   âœ“ åŠ¨ä½œæ‰§è¡ŒæˆåŠŸ: reward={reward:.3f}, done={done}")
        
        print(f"\nâœ… PPOç¯å¢ƒé›†æˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"\nâŒ PPOç¯å¢ƒé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_error_handling():
    """
    æµ‹è¯•é”™è¯¯å¤„ç† / Test error handling
    éªŒè¯pipelineå¯¹å¼‚å¸¸è¾“å…¥çš„é²æ£’æ€§
    """
    print("\n" + "="*60)
    print("æµ‹è¯•6: é”™è¯¯å¤„ç† / Test 6: Error Handling")
    print("="*60)
    
    test_cases = [
        {
            'name': 'æ— æ•ˆçš„imputeç­–ç•¥',
            'params': {'impute_strategy': 'invalid_strategy'},
            'should_fail': True
        },
        {
            'name': 'æ— æ•ˆçš„modelç­–ç•¥',
            'params': {'model_strategy': 'invalid_model'},
            'should_fail': True
        },
        {
            'name': 'è´Ÿæ•°nan_thresh',
            'params': {'nan_thresh': -0.5},
            'should_fail': False  # åº”è¯¥è‡ªåŠ¨çº æ­£
        }
    ]
    
    passed = 0
    
    for case in test_cases:
        print(f"\nğŸ§ª æµ‹è¯•: {case['name']}")
        try:
            params = {
                'cache': True,
                'model_params': {'n_estimators': 10}
            }
            params.update(case['params'])
            
            result = run_pipeline(**params)
            
            if case['should_fail']:
                print(f"   âš ï¸ é¢„æœŸå¤±è´¥ä½†æˆåŠŸäº†")
            else:
                print(f"   âœ“ æ­£ç¡®å¤„ç†")
                passed += 1
                
        except Exception as e:
            if case['should_fail']:
                print(f"   âœ“ æŒ‰é¢„æœŸå¤±è´¥: {type(e).__name__}")
                passed += 1
            else:
                print(f"   âœ— æ„å¤–å¤±è´¥: {e}")
    
    print(f"\nâœ… é”™è¯¯å¤„ç†æµ‹è¯•: {passed}/{len(test_cases)} é€šè¿‡")
    return passed >= len(test_cases) * 0.5  # è‡³å°‘50%é€šè¿‡


def run_all_tests():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯• / Run all tests
    """
    print("\n" + "="*70)
    print("ğŸ§ª å¼€å§‹Pipelineæµ‹è¯•å¥—ä»¶ / Starting Pipeline Test Suite")
    print("="*70)
    
    tests = [
        ("åŸºç¡€Pipelineæ‰§è¡Œ", test_basic_pipeline),
        ("å•ä¸ªèŠ‚ç‚¹æ‰§è¡Œ", test_node_execution),
        ("çµæ´»Pipelineé…ç½®", test_pipeline_config),
        ("ä¸åŒç­–ç•¥ç»„åˆ", test_different_strategies),
        ("PPOç¯å¢ƒé›†æˆ", test_pipeline_with_ppo_env),
        ("é”™è¯¯å¤„ç†", test_error_handling)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            success = test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"\nâŒ {test_name} å´©æºƒ: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“ / Test Results Summary")
    print("="*70)
    
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{status} - {test_name}")
    
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    print("\n" + "="*70)
    print(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡ ({passed/total*100:.1f}%)")
    print("="*70)
    
    return passed == total


if __name__ == "__main__":
    import warnings
    warnings.filterwarnings('ignore')
    
    success = run_all_tests()
    
    if success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        sys.exit(1)
