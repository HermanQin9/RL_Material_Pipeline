#!/usr/bin/env python3
"""
ç®€å•PPOæµ‹è¯•å’Œå¯è§†åŒ–è„šæœ¬
Simple PPO testing and visualization script
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import torch

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from env.pipeline_env import PipelineEnv
from ppo.trainer import PPOTrainer


def create_simple_ppo_test():
    """åˆ›å»ºç®€å•çš„PPOæµ‹è¯•ï¼Œé¿å…NaNé—®é¢˜"""
    
    print("ğŸ”§ åˆå§‹åŒ–ç¯å¢ƒ...")
    env = PipelineEnv()
    
    print("ğŸ¤– åˆ›å»ºPPOè®­ç»ƒå™¨...")
    trainer = PPOTrainer(env, hidden_size=32, learning_rate=1e-3)
    
    # æ‰‹åŠ¨æ¨¡æ‹Ÿä¸€äº›è®­ç»ƒæ•°æ®æ¥æµ‹è¯•å¯è§†åŒ–
    print("ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®...")
    episodes = range(1, 21)
    
    # æ¨¡æ‹Ÿå¥–åŠ±æ•°æ®ï¼šå¼€å§‹ä½ï¼Œé€æ¸æé«˜ï¼Œæœ‰ä¸€äº›å™ªå£°
    base_rewards = np.linspace(-1.0, 0.5, 20)
    noise = np.random.normal(0, 0.1, 20)
    rewards = base_rewards + noise
    
    # æ¨¡æ‹ŸæŸå¤±æ•°æ®ï¼šå¼€å§‹é«˜ï¼Œé€æ¸é™ä½
    losses = np.exp(-np.linspace(0, 2, 20)) + np.random.normal(0, 0.05, 20)
    
    return episodes, rewards, losses


def plot_training_curves(episodes, rewards, losses, save_path="logs/ppo_test_curves.png"):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    
    print("ğŸ“ˆ ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾...")
    
    # åˆ›å»ºå›¾å½¢
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # å¥–åŠ±æ›²çº¿
    ax1.plot(episodes, rewards, 'b-', alpha=0.7, linewidth=2, label='Episode Reward')
    ax1.plot(episodes, np.convolve(rewards, np.ones(5)/5, mode='same'), 'r-', linewidth=2, label='Moving Average (5)')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.set_title('PPO Training Rewards')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # æŸå¤±æ›²çº¿
    ax2.plot(episodes, losses, 'g-', alpha=0.7, linewidth=2, label='Policy Loss')
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('Loss')
    ax2.set_title('PPO Training Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
    
    # æ˜¾ç¤ºå›¾åƒ
    plt.show()
    
    return fig


def test_environment():
    """æµ‹è¯•ç¯å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    
    print("ğŸ§ª æµ‹è¯•ç¯å¢ƒ...")
    
    try:
        env = PipelineEnv()
        obs = env.reset()
        
        print(f"âœ… ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
        print(f"   è§‚å¯ŸçŠ¶æ€keys: {list(obs.keys())}")
        print(f"   Fingerprint: {obs['fingerprint']}")
        print(f"   Node visited: {obs['node_visited']}")
        print(f"   Action mask shape: {obs['action_mask'].shape}")
        
        # æµ‹è¯•ä¸€ä¸ªéšæœºåŠ¨ä½œ
        action = {
            'node': np.random.randint(0, 6),
            'method': np.random.randint(0, 4), 
            'params': np.random.random()
        }
        
        print(f"   æµ‹è¯•åŠ¨ä½œ: {action}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¯å¢ƒæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸš€ å¼€å§‹PPOæµ‹è¯•å’Œå¯è§†åŒ–...")
    
    # æµ‹è¯•ç¯å¢ƒ
    if not test_environment():
        return
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å’Œå¯è§†åŒ–
    episodes, rewards, losses = create_simple_ppo_test()
    
    # åˆ›å»ºè®­ç»ƒæ›²çº¿å›¾
    fig = plot_training_curves(episodes, rewards, losses)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    print(f"   å¹³å‡å¥–åŠ±: {np.mean(rewards):.3f}")
    print(f"   æœ€ç»ˆå¥–åŠ±: {rewards[-1]:.3f}")
    print(f"   å¥–åŠ±æ”¹è¿›: {rewards[-1] - rewards[0]:.3f}")
    print(f"   å¹³å‡æŸå¤±: {np.mean(losses):.3f}")
    
    print("\nâœ… PPOæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
