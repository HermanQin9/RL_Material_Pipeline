#!/usr/bin/env python3
"""
ç®€å•PPOæµ‹è¯•å’Œå¯è§†åŒ–è„šæœ¬
Simple PPO testing and visualization script
"""

import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import torch

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from env.pipeline_env import PipelineEnv
from ppo.trainer import PPOTrainer


def create_simple_ppo_test():
    """
    åˆ›å»ºç®€å•çš„PPOæµ‹è¯•ï¼Œé¿å…NaNé—®é¢˜ / Create simple PPO test, avoiding NaN issues
    
    ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®ç”¨äºæµ‹è¯•å¯è§†åŒ–åŠŸèƒ½
    Generates mock training data for testing visualization features
    """
    print("ğŸ”§ åˆå§‹åŒ–ç¯å¢ƒ... / Initializing environment...")
    env = PipelineEnv()
    
    print("ğŸ¤– åˆ›å»ºPPOè®­ç»ƒå™¨... / Creating PPO trainer...")
    trainer = PPOTrainer(env, hidden_size=32, learning_rate=1e-3)
    
    # æ‰‹åŠ¨æ¨¡æ‹Ÿä¸€äº›è®­ç»ƒæ•°æ®æ¥æµ‹è¯•å¯è§†åŒ–
    # Manually simulate some training data to test visualization
    print("ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®... / Generating mock training data...")
    episodes = range(1, 21)
    
    # æ¨¡æ‹Ÿå¥–åŠ±æ•°æ®ï¼šå¼€å§‹ä½ï¼Œé€æ¸æé«˜ï¼Œæœ‰ä¸€äº›å™ªå£°
    # Mock reward data: starts low, gradually improves, with some noise
    base_rewards = np.linspace(-1.0, 0.5, 20)
    noise = np.random.normal(0, 0.1, 20)
    rewards = base_rewards + noise
    
    # æ¨¡æ‹ŸæŸå¤±æ•°æ®ï¼šå¼€å§‹é«˜ï¼Œé€æ¸é™ä½
    # Mock loss data: starts high, gradually decreases
    losses = np.exp(-np.linspace(0, 2, 20)) + np.random.normal(0, 0.05, 20)
    
    return episodes, rewards, losses


def plot_training_curves(episodes, rewards, losses, save_path="logs/ppo_test_curves.png"):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿ / Plot training curves
    
    åˆ›å»ºå¥–åŠ±å’ŒæŸå¤±çš„å¯è§†åŒ–å›¾è¡¨
    Creates visualization charts for rewards and losses
    """
    print("ğŸ“ˆ ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾... / Generating training curve charts...")
    
    # åˆ›å»ºå›¾å½¢ / Create figure
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # å¥–åŠ±æ›²çº¿ / Reward curve
    ax1.plot(episodes, rewards, 'b-', alpha=0.7, linewidth=2, label='å›åˆå¥–åŠ± / Episode Reward')
    ax1.plot(episodes, np.convolve(rewards, np.ones(5)/5, mode='same'), 'r-', linewidth=2, 
            label='ç§»åŠ¨å¹³å‡(5) / Moving Avg (5)')
    ax1.set_xlabel('å›åˆ / Episode')
    ax1.set_ylabel('å¥–åŠ± / Reward')
    ax1.set_title('PPOè®­ç»ƒå¥–åŠ± / PPO Training Rewards')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # æŸå¤±æ›²çº¿ / Loss curve
    ax2.plot(episodes, losses, 'g-', alpha=0.7, linewidth=2, label='ç­–ç•¥æŸå¤± / Policy Loss')
    ax2.set_xlabel('å›åˆ / Episode')
    ax2.set_ylabel('æŸå¤± / Loss')
    ax2.set_title('PPOè®­ç»ƒæŸå¤± / PPO Training Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ / Save figure
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° / Training curves saved to: {save_path}")
    
    # æ˜¾ç¤ºå›¾åƒ / Show figure
    plt.show()
    
    return fig


def test_environment():
    """
    æµ‹è¯•ç¯å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œ / Test if environment works properly
    
    éªŒè¯ç¯å¢ƒåˆå§‹åŒ–å’ŒåŸºæœ¬åŠŸèƒ½
    Validates environment initialization and basic functionality
    """
    print("ğŸ§ª æµ‹è¯•ç¯å¢ƒ... / Testing environment...")
    
    try:
        env = PipelineEnv()
        obs = env.reset()
        
        print(f"âœ… ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ / Environment initialized successfully")
        print(f"   è§‚å¯ŸçŠ¶æ€keys / Observation keys: {list(obs.keys())}")
        print(f"   Fingerprint: {obs['fingerprint']}")
        print(f"   Node visited: {obs['node_visited']}")
        print(f"   Action mask shape: {obs['action_mask'].shape}")
        
        # æµ‹è¯•ä¸€ä¸ªéšæœºåŠ¨ä½œ / Test a random action
        action = {
            'node': np.random.randint(0, 6),
            'method': np.random.randint(0, 4), 
            'params': np.random.random()
        }
        
        print(f"   æµ‹è¯•åŠ¨ä½œ / Test action: {action}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¯å¢ƒæµ‹è¯•å¤±è´¥ / Environment test failed: {e}")
        return False


def main():
    """
    ä¸»å‡½æ•° / Main function
    
    æ‰§è¡Œå®Œæ•´çš„PPOæµ‹è¯•å’Œå¯è§†åŒ–æµç¨‹
    Executes complete PPO testing and visualization workflow
    """
    print("ğŸš€ å¼€å§‹PPOæµ‹è¯•å’Œå¯è§†åŒ–... / Starting PPO test and visualization...")
    
    # æµ‹è¯•ç¯å¢ƒ / Test environment
    if not test_environment():
        print("âš ï¸ ç¯å¢ƒæµ‹è¯•æœªé€šè¿‡ï¼Œé€€å‡º / Environment test failed, exiting")
        return
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å’Œå¯è§†åŒ– / Generate mock data and visualization
    episodes, rewards, losses = create_simple_ppo_test()
    
    # åˆ›å»ºè®­ç»ƒæ›²çº¿å›¾ / Create training curve charts
    fig = plot_training_curves(episodes, rewards, losses)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯ / Print statistics
    print("\nğŸ“Š è®­ç»ƒç»Ÿè®¡ / Training Statistics:")
    print(f"   å¹³å‡å¥–åŠ± / Average reward: {np.mean(rewards):.3f}")
    print(f"   æœ€ç»ˆå¥–åŠ± / Final reward: {rewards[-1]:.3f}")
    print(f"   å¥–åŠ±æ”¹è¿› / Reward improvement: {rewards[-1] - rewards[0]:.3f}")
    print(f"   å¹³å‡æŸå¤± / Average loss: {np.mean(losses):.3f}")
    
    print("\nâœ… PPOæµ‹è¯•å®Œæˆï¼ / PPO test completed!")


if __name__ == "__main__":
    main()
