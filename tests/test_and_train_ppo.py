#!/usr/bin/env python3
"""
å®Œæ•´çš„ä»£ç æµ‹è¯•å’ŒPPOå­¦ä¹ è„šæœ¬ / Complete Code Testing and PPO Learning Script

This script runs comprehensive tests and trains a PPO agent with learning curve visualization.
æ­¤è„šæœ¬è¿è¡Œå…¨é¢æµ‹è¯•å¹¶è®­ç»ƒPPOæ™ºèƒ½ä½“ï¼ŒåŒæ—¶å¯è§†åŒ–å­¦ä¹ æ›²çº¿ã€‚
"""

import sys
import os
import time
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Dict, Any
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ / Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# é…ç½®æ—¥å¿— / Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_pipeline_components():
    """
    æµ‹è¯•æµæ°´çº¿ç»„ä»¶ / Test pipeline components
    
    Returns:
        bool: æµ‹è¯•æ˜¯å¦æˆåŠŸ / Whether tests passed
    """
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æµæ°´çº¿ç»„ä»¶ / Starting pipeline component tests")
    print("=" * 70)
    
    try:
        from pipeline import run_pipeline
        from nodes import DataFetchNode, ImputeNode, FeatureMatrixNode, FeatureSelectionNode, ScalingNode, ModelTrainingNode
        
        # æµ‹è¯•æ¯ä¸ªèŠ‚ç‚¹ / Test each node
        print("\n1. æµ‹è¯•æ•°æ®è·å–èŠ‚ç‚¹ / Testing Data Fetch Node")
        data_node = DataFetchNode()
        fetched = data_node.execute(method='api', params={'cache': True}, data={})
        print(f"   âœ… æ•°æ®è·å–æˆåŠŸ / Data fetch successful: {list(fetched.keys())}")
        
        print("\n2. æµ‹è¯•ç‰¹å¾çŸ©é˜µèŠ‚ç‚¹ / Testing Feature Matrix Node")
        feature_node = FeatureMatrixNode()
        features = feature_node.execute(
            method='construct',
            params={'nan_thresh': 0.5, 'train_val_ratio': 0.8, 'verbose': False},
            data=fetched
        )
        print(f"   âœ… ç‰¹å¾çŸ©é˜µæ„å»ºæˆåŠŸ / Feature matrix construction successful")
        print(f"   è®­ç»ƒé›†å½¢çŠ¶ / Training set shape: {features.get('X_train', np.array([])).shape}")
        
        print("\n3. æµ‹è¯•ç¼ºå¤±å€¼å¡«å……èŠ‚ç‚¹ / Testing Imputation Node")
        impute_node = ImputeNode()
        imputed = impute_node.execute(
            method='impute', 
            params={'strategy': 'mean', 'params': {}}, 
            data=features
        )
        print(f"   âœ… ç¼ºå¤±å€¼å¡«å……æˆåŠŸ / Imputation successful")
        
        print("\n4. æµ‹è¯•ç‰¹å¾é€‰æ‹©èŠ‚ç‚¹ / Testing Feature Selection Node")
        select_node = FeatureSelectionNode()
        selected = select_node.execute(
            method='select',
            params={'strategy': 'none', 'params': {}},
            data=imputed
        )
        print(f"   âœ… ç‰¹å¾é€‰æ‹©æˆåŠŸ / Feature selection successful")
        
        print("\n5. æµ‹è¯•æ•°æ®ç¼©æ”¾èŠ‚ç‚¹ / Testing Scaling Node")
        scaling_node = ScalingNode()
        scaled = scaling_node.execute(
            method='scale',
            params={'strategy': 'standard', 'params': {}},
            data=selected
        )
        print(f"   âœ… æ•°æ®ç¼©æ”¾æˆåŠŸ / Data scaling successful")
        
        print("\n6. æµ‹è¯•å®Œæ•´æµæ°´çº¿ / Testing Complete Pipeline")
        result = run_pipeline(
            cache=True,
            nan_thresh=0.5,
            train_val_ratio=0.8,
            impute_strategy='mean',
            selection_strategy='none',
            scaling_strategy='standard',
            model_strategy='rf',
            model_params={'n_estimators': 10}
        )
        print(f"   âœ… å®Œæ•´æµæ°´çº¿æµ‹è¯•æˆåŠŸ / Complete pipeline test successful")
        print(f"   æ¨¡å‹ç±»å‹ / Model type: {type(result.get('model', None)).__name__}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æµæ°´çº¿æµ‹è¯•å¤±è´¥ / Pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ppo_components():
    """
    æµ‹è¯•PPOç»„ä»¶ / Test PPO components
    
    Returns:
        bool: æµ‹è¯•æ˜¯å¦æˆåŠŸ / Whether tests passed
    """
    print("\nğŸ¤– å¼€å§‹æµ‹è¯•PPOç»„ä»¶ / Starting PPO component tests")
    print("=" * 70)
    
    try:
        from env.pipeline_env import PipelineEnv
        from ppo.utils import compute_gae, ppo_loss, value_loss, entropy_loss
        import torch
        
        print("\n1. æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ– / Testing Environment Initialization")
        env = PipelineEnv()
        obs = env.reset()
        print(f"   âœ… ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ / Environment initialization successful")
        print(f"   è§‚æµ‹ç©ºé—´é”® / Observation space keys: {list(obs.keys())}")
        
        print("\n2. æµ‹è¯•ç¯å¢ƒæ­¥è¿› / Testing Environment Step")
        action = {'node': 0, 'method': 0, 'params': [0.5]}
        next_obs, reward, done, truncated, info = env.step(action)
        print(f"   âœ… ç¯å¢ƒæ­¥è¿›æˆåŠŸ / Environment step successful")
        print(f"   å¥–åŠ± / Reward: {reward:.3f}, å®Œæˆ / Done: {done}")
        
        print("\n3. æµ‹è¯•PPOå·¥å…·å‡½æ•° / Testing PPO Utility Functions")
        # åˆ›å»ºæµ‹è¯•æ•°æ® / Create test data
        rewards = torch.tensor([1.0, 0.5, -0.2])
        values = torch.tensor([0.8, 0.6, 0.1])
        dones = torch.tensor([0.0, 0.0, 1.0])
        
        advantages, returns = compute_gae(rewards, values, dones, 0.0)
        print(f"   âœ… GAEè®¡ç®—æˆåŠŸ / GAE computation successful")
        
        # æµ‹è¯•æŸå¤±å‡½æ•° / Test loss functions
        new_log_probs = torch.tensor([0.1, 0.2, 0.3])
        old_log_probs = torch.tensor([0.15, 0.18, 0.25])
        
        policy_loss = ppo_loss(new_log_probs, old_log_probs, advantages)
        v_loss = value_loss(values, returns)
        e_loss = entropy_loss(new_log_probs)
        
        print(f"   âœ… æŸå¤±å‡½æ•°æµ‹è¯•æˆåŠŸ / Loss function tests successful")
        print(f"   ç­–ç•¥æŸå¤± / Policy loss: {policy_loss:.4f}")
        print(f"   ä»·å€¼æŸå¤± / Value loss: {v_loss:.4f}")
        print(f"   ç†µæŸå¤± / Entropy loss: {e_loss:.4f}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ PPOç»„ä»¶æµ‹è¯•å¤±è´¥ / PPO component test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def train_ppo_with_curves(episodes: int = 100, save_plots: bool = True):
    """
    è®­ç»ƒPPOå¹¶ç»˜åˆ¶å­¦ä¹ æ›²çº¿ / Train PPO and plot learning curves
    
    Args:
        episodes: è®­ç»ƒå›åˆæ•° / Number of training episodes
        save_plots: æ˜¯å¦ä¿å­˜å›¾åƒ / Whether to save plots
        
    Returns:
        Dict: è®­ç»ƒç»“æœ / Training results
    """
    print(f"\nğŸš€ å¼€å§‹PPOè®­ç»ƒ ({episodes} å›åˆ) / Starting PPO Training ({episodes} episodes)")
    print("=" * 70)
    
    try:
        from env.pipeline_env import PipelineEnv
        
        # åˆå§‹åŒ–ç¯å¢ƒ / Initialize environment
        env = PipelineEnv()
        
        # è®­ç»ƒæ•°æ®è®°å½• / Training data recording
        episode_rewards = []
        episode_lengths = []
        moving_avg_rewards = []
        exploration_rates = []
        
        print("å¼€å§‹è®­ç»ƒå¾ªç¯ / Starting training loop...")
        
        for episode in range(episodes):
            obs = env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            
            # ç®€å•çš„éšæœºç­–ç•¥ç”¨äºæ¼”ç¤º / Simple random policy for demonstration
            while not done and episode_length < 50:  # é™åˆ¶æœ€å¤§æ­¥æ•° / Limit max steps
                # éšæœºé€‰æ‹©åŠ¨ä½œ / Random action selection
                node_idx = np.random.randint(len(env.pipeline_nodes))
                method_idx = np.random.randint(len(env.methods_for_node[env.pipeline_nodes[node_idx]]))
                params = [np.random.random()]
                
                action = {
                    'node': node_idx,
                    'method': method_idx, 
                    'params': params
                }
                
                next_obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                
                episode_reward += reward
                episode_length += 1
                obs = next_obs
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # è®¡ç®—ç§»åŠ¨å¹³å‡ / Calculate moving average
            window_size = min(10, len(episode_rewards))
            moving_avg = np.mean(episode_rewards[-window_size:])
            moving_avg_rewards.append(moving_avg)
            
            # è®°å½•æ¢ç´¢ç‡ (ç®€åŒ–ç‰ˆ) / Record exploration rate (simplified)
            exploration_rate = max(0.1, 1.0 - episode / episodes)
            exploration_rates.append(exploration_rate)
            
            # å®šæœŸè¾“å‡ºè¿›åº¦ / Periodic progress output
            if (episode + 1) % 20 == 0:
                print(f"å›åˆ / Episode {episode + 1}/{episodes}: "
                      f"å¥–åŠ± / Reward: {episode_reward:.3f}, "
                      f"ç§»åŠ¨å¹³å‡ / Moving Avg: {moving_avg:.3f}, "
                      f"é•¿åº¦ / Length: {episode_length}")
        
        # ç»˜åˆ¶å­¦ä¹ æ›²çº¿ / Plot learning curves
        print("\nğŸ“Š ç»˜åˆ¶å­¦ä¹ æ›²çº¿ / Plotting learning curves...")
        plot_learning_curves(episode_rewards, moving_avg_rewards, exploration_rates, 
                            episode_lengths, save_plots)
        
        results = {
            'episode_rewards': episode_rewards,
            'moving_avg_rewards': moving_avg_rewards,
            'exploration_rates': exploration_rates,
            'episode_lengths': episode_lengths,
            'final_avg_reward': np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)
        }
        
        print(f"\nâœ… PPOè®­ç»ƒå®Œæˆ / PPO training completed!")
        print(f"æœ€ç»ˆå¹³å‡å¥–åŠ± / Final average reward: {results['final_avg_reward']:.3f}")
        
        return results
        
    except Exception as e:
        print(f"âŒ PPOè®­ç»ƒå¤±è´¥ / PPO training failed: {e}")
        import traceback
        traceback.print_exc()
        return {}

def plot_learning_curves(rewards, moving_avg_rewards, exploration_rates, episode_lengths, save_plots=True):
    """
    ç»˜åˆ¶å­¦ä¹ æ›²çº¿ / Plot learning curves
    
    Args:
        rewards: å›åˆå¥–åŠ±åˆ—è¡¨ / Episode rewards list
        moving_avg_rewards: ç§»åŠ¨å¹³å‡å¥–åŠ± / Moving average rewards
        exploration_rates: æ¢ç´¢ç‡ / Exploration rates
        episode_lengths: å›åˆé•¿åº¦ / Episode lengths
        save_plots: æ˜¯å¦ä¿å­˜å›¾åƒ / Whether to save plots
    """
    try:
        # è®¾ç½®ä¸­æ–‡å­—ä½“ / Set Chinese font
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        episodes = range(1, len(rewards) + 1)
        
        # 1. å›åˆå¥–åŠ±å’Œç§»åŠ¨å¹³å‡ / Episode rewards and moving average
        ax1.plot(episodes, rewards, alpha=0.6, color='lightblue', label='å›åˆå¥–åŠ± / Episode Rewards')
        ax1.plot(episodes, moving_avg_rewards, color='darkblue', linewidth=2, label='ç§»åŠ¨å¹³å‡ / Moving Average')
        ax1.set_xlabel('å›åˆ / Episodes')
        ax1.set_ylabel('å¥–åŠ± / Reward')
        ax1.set_title('PPOå­¦ä¹ æ›²çº¿ - å¥–åŠ± / PPO Learning Curve - Rewards')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æ¢ç´¢ç‡è¡°å‡ / Exploration rate decay
        ax2.plot(episodes, exploration_rates, color='orange', linewidth=2)
        ax2.set_xlabel('å›åˆ / Episodes')
        ax2.set_ylabel('æ¢ç´¢ç‡ / Exploration Rate')
        ax2.set_title('æ¢ç´¢ç‡è¡°å‡ / Exploration Rate Decay')
        ax2.grid(True, alpha=0.3)
        
        # 3. å›åˆé•¿åº¦ / Episode lengths
        ax3.plot(episodes, episode_lengths, color='green', alpha=0.7)
        ax3.set_xlabel('å›åˆ / Episodes')
        ax3.set_ylabel('å›åˆé•¿åº¦ / Episode Length')
        ax3.set_title('å›åˆé•¿åº¦å˜åŒ– / Episode Length Variation')
        ax3.grid(True, alpha=0.3)
        
        # 4. å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾ / Reward distribution histogram
        ax4.hist(rewards, bins=20, alpha=0.7, color='purple', edgecolor='black')
        ax4.set_xlabel('å¥–åŠ±å€¼ / Reward Value')
        ax4.set_ylabel('é¢‘æ¬¡ / Frequency')
        ax4.set_title('å¥–åŠ±åˆ†å¸ƒ / Reward Distribution')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_plots:
            plots_dir = Path('logs')
            plots_dir.mkdir(exist_ok=True)
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            plot_file = plots_dir / f'ppo_learning_curves_{timestamp}.png'
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å­¦ä¹ æ›²çº¿å·²ä¿å­˜ / Learning curves saved: {plot_file}")
        
        plt.show()
        
    except Exception as e:
        print(f"âŒ ç»˜å›¾å¤±è´¥ / Plotting failed: {e}")
        import traceback
        traceback.print_exc()

def main():
    """
    ä¸»å‡½æ•° / Main function
    """
    print("ğŸ¯ MatFormPPO å®Œæ•´æµ‹è¯•å’ŒPPOå­¦ä¹ ç³»ç»Ÿ")
    print("ğŸ¯ MatFormPPO Complete Testing and PPO Learning System")
    print("=" * 80)
    
    # ç¬¬ä¸€é˜¶æ®µï¼šæµ‹è¯•æµæ°´çº¿ç»„ä»¶ / Phase 1: Test pipeline components
    pipeline_success = test_pipeline_components()
    
    # ç¬¬äºŒé˜¶æ®µï¼šæµ‹è¯•PPOç»„ä»¶ / Phase 2: Test PPO components  
    ppo_success = test_ppo_components()
    
    if pipeline_success and ppo_success:
        print("\nğŸ‰ æ‰€æœ‰ç»„ä»¶æµ‹è¯•é€šè¿‡ï¼å¼€å§‹PPOè®­ç»ƒ...")
        print("ğŸ‰ All component tests passed! Starting PPO training...")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šPPOè®­ç»ƒå’Œå­¦ä¹ æ›²çº¿ / Phase 3: PPO training and learning curves
        training_results = train_ppo_with_curves(episodes=100, save_plots=True)
        
        if training_results:
            print(f"\nğŸ† å®Œæ•´æµ‹è¯•å’Œè®­ç»ƒæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ† Complete testing and training successfully completed!")
            print(f"æœ€ç»ˆå¹³å‡å¥–åŠ± / Final average reward: {training_results['final_avg_reward']:.3f}")
        else:
            print(f"\nâŒ PPOè®­ç»ƒå¤±è´¥")
            print(f"âŒ PPO training failed")
    else:
        print(f"\nâŒ ç»„ä»¶æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡PPOè®­ç»ƒ")
        print(f"âŒ Component tests failed, skipping PPO training")

if __name__ == "__main__":
    main()
