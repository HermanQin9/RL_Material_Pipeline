#!/usr/bin/env python3
"""
æ‰©å±•PPOè®­ç»ƒéªŒè¯ - å¤šè½®è®­ç»ƒåˆ†æ
Extended PPO Training Validation - Multi-round Training Analysis
"""
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei',        if results:
            # åˆ›å»ºç»¼åˆåˆ†æ
            analysis_result = create_comprehensive_analysis(results)
            if analysis_result:
                chart_file, all_rewards, all_episodes = analysis_result
                
                # æ‰“å°åˆ†ææŠ¥å‘Š
                print_comprehensive_analysis(results, all_rewards)
                
                print(f"\nğŸ‰ PPOæ‰©å±•è®­ç»ƒéªŒè¯å®Œæˆ! / Extended PPO Training Validation Complete!")
                print(f"ğŸ“ˆ ç»¼åˆåˆ†æå›¾è¡¨: {chart_file}")
                print(f"ğŸ“ è¯·æŸ¥çœ‹ logs/ ç›®å½•ä¸­çš„è¯¦ç»†å›¾è¡¨æ–‡ä»¶")
            else:
                print("âš ï¸ å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼Œä½†è®­ç»ƒæ•°æ®å¯ç”¨")
                all_rewards = []
                for r in results:
                    all_rewards.extend(r['rewards'])
                print_comprehensive_analysis(results, all_rewards)ans']
matplotlib.rcParams['axes.unicode_minus'] = False

# ç¡®ä¿ä½¿ç”¨æµ‹è¯•æ¨¡å¼
os.environ['PIPELINE_TEST'] = '1'

sys.path.append('.')

def run_extended_ppo_training():
    """è¿è¡Œæ‰©å±•çš„PPOè®­ç»ƒéªŒè¯"""
    print("ğŸš€ å¼€å§‹æ‰©å±•PPOè®­ç»ƒéªŒè¯ / Starting Extended PPO Training Validation")
    print("=" * 80)
    
    # è¿è¡Œå¤šè½®ä¸åŒé…ç½®çš„è®­ç»ƒ
    training_configs = [
        {"episodes": 20, "name": "ç¬¬1è½®è®­ç»ƒ", "description": "Round 1"},
        {"episodes": 25, "name": "ç¬¬2è½®è®­ç»ƒ", "description": "Round 2"}, 
        {"episodes": 30, "name": "ç¬¬3è½®è®­ç»ƒ", "description": "Round 3"},
        {"episodes": 35, "name": "ç¬¬4è½®è®­ç»ƒ", "description": "Round 4"}
    ]
    
    all_results = []
    cumulative_episodes = 0
    
    for round_idx, config in enumerate(training_configs, 1):
        print(f"\nğŸ”„ {config['name']} / {config['description']}")
        print(f"ğŸ“Š å›åˆæ•°: {config['episodes']}")
        print("-" * 60)
        
        # è¿è¡Œè®­ç»ƒ
        try:
            import subprocess
            result = subprocess.run([
                "D:\\conda_envs\\summer_project_2025\\python.exe", 
                "train_ppo_safe.py", 
                "--episodes", str(config['episodes'])
            ], capture_output=True, text=True, cwd=".")
            
            if result.returncode == 0:
                # è§£æè¾“å‡ºè·å–å¥–åŠ±ä¿¡æ¯
                output_lines = result.stdout.split('\\n')
                rewards = []
                
                for line in output_lines:
                    if "å¥–åŠ±:" in line and "é•¿åº¦:" in line:
                        try:
                            reward_str = line.split("å¥–åŠ±:")[1].split(",")[0].strip()
                            reward = float(reward_str)
                            rewards.append(reward)
                        except:
                            continue
                
                if rewards:
                    round_stats = {
                        'round': round_idx,
                        'episodes': config['episodes'],
                        'rewards': rewards,
                        'avg_reward': np.mean(rewards),
                        'std_reward': np.std(rewards),
                        'max_reward': np.max(rewards),
                        'min_reward': np.min(rewards),
                        'cumulative_start': cumulative_episodes,
                        'cumulative_end': cumulative_episodes + len(rewards)
                    }
                    all_results.append(round_stats)
                    cumulative_episodes += len(rewards)
                    
                    print(f"âœ… {config['name']}å®Œæˆ:")
                    print(f"   ğŸ“ˆ å¹³å‡å¥–åŠ±: {round_stats['avg_reward']:.3f} Â± {round_stats['std_reward']:.3f}")
                    print(f"   ğŸ“Š å¥–åŠ±èŒƒå›´: {round_stats['min_reward']:.3f} ~ {round_stats['max_reward']:.3f}")
                    print(f"   ğŸ¯ å®é™…å›åˆæ•°: {len(rewards)}")
                else:
                    print(f"âš ï¸ {config['name']}æœªè·å–åˆ°å¥–åŠ±æ•°æ®")
            else:
                print(f"âŒ {config['name']}è®­ç»ƒå¤±è´¥: {result.stderr}")
                
        except Exception as e:
            print(f"âŒ {config['name']}è¿è¡Œå‡ºé”™: {e}")
    
    return all_results

def create_comprehensive_analysis(results):
    """åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨"""
    if not results:
        print("âŒ æ²¡æœ‰è®­ç»ƒç»“æœå¯åˆ†æ")
        return
    
    print("\\nğŸ“Š ç»˜åˆ¶ç»¼åˆå­¦ä¹ æ›²çº¿...")
    
    # åˆ›å»ºç»¼åˆå›¾è¡¨
    fig = plt.figure(figsize=(16, 12))
    
    # 1. æ‰€æœ‰å¥–åŠ±çš„æ—¶é—´åºåˆ—å›¾
    ax1 = plt.subplot(2, 3, 1)
    all_rewards = []
    all_episodes = []
    colors = ['blue', 'red', 'green', 'purple']
    
    for i, result in enumerate(results):
        episode_range = range(result['cumulative_start'], result['cumulative_end'])
        all_rewards.extend(result['rewards'])
        all_episodes.extend(episode_range)
        
        plt.plot(episode_range, result['rewards'], 'o-', 
                alpha=0.7, color=colors[i % len(colors)], 
                label=f"ç¬¬{result['round']}è½®")
    
    # æ·»åŠ æ€»ä½“ç§»åŠ¨å¹³å‡
    if len(all_rewards) >= 10:
        window = 10
        moving_avg = np.convolve(all_rewards, np.ones(window)/window, mode='valid')
        moving_episodes = all_episodes[window-1:]
        plt.plot(moving_episodes, moving_avg, 'k-', linewidth=3, 
                label=f'{window}å›åˆç§»åŠ¨å¹³å‡', alpha=0.8)
    
    plt.xlabel('ç´¯ç§¯å›åˆæ•° / Cumulative Episodes')
    plt.ylabel('å¥–åŠ± / Reward')
    plt.title('PPOå­¦ä¹ æ›²çº¿ - å¤šè½®è®­ç»ƒ\\nPPO Learning Curve - Multi-Round Training')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 2. æ¯è½®å¹³å‡å¥–åŠ±å¯¹æ¯”
    ax2 = plt.subplot(2, 3, 2)
    round_nums = [r['round'] for r in results]
    avg_rewards = [r['avg_reward'] for r in results]
    std_rewards = [r['std_reward'] for r in results]
    
    bars = plt.bar(round_nums, avg_rewards, yerr=std_rewards, 
                  capsize=5, alpha=0.7, color=colors[:len(results)])
    plt.xlabel('è®­ç»ƒè½®æ¬¡ / Training Round')
    plt.ylabel('å¹³å‡å¥–åŠ± / Average Reward')
    plt.title('å„è½®å¹³å‡å¥–åŠ±å¯¹æ¯”\\nAverage Reward per Round')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (x, y) in enumerate(zip(round_nums, avg_rewards)):
        plt.text(x, y + std_rewards[i] + 0.02, f'{y:.3f}', 
                ha='center', va='bottom', fontweight='bold')
    
    plt.grid(True, alpha=0.3)
    
    # 3. å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
    ax3 = plt.subplot(2, 3, 3)
    plt.hist(all_rewards, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    plt.xlabel('å¥–åŠ±å€¼ / Reward Value')
    plt.ylabel('é¢‘æ¬¡ / Frequency')
    plt.title('å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾\\nReward Distribution')
    plt.axvline(float(np.mean(all_rewards)), color='red', linestyle='--', 
               label=f'å¹³å‡å€¼: {np.mean(all_rewards):.3f}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 4. å­¦ä¹ è¶‹åŠ¿åˆ†æ
    ax4 = plt.subplot(2, 3, 4)
    round_improvements = []
    for i in range(1, len(results)):
        improvement = results[i]['avg_reward'] - results[i-1]['avg_reward']
        round_improvements.append(improvement)
    
    if round_improvements:
        plt.plot(range(2, len(results)+1), round_improvements, 'o-', 
                linewidth=2, markersize=8, color='green')
        plt.axhline(0, color='red', linestyle='--', alpha=0.5)
        plt.xlabel('è½®æ¬¡ / Round')
        plt.ylabel('å¥–åŠ±æ”¹è¿› / Reward Improvement')
        plt.title('è½®æ¬¡é—´æ”¹è¿›è¶‹åŠ¿\\nImprovement Trend Between Rounds')
        plt.grid(True, alpha=0.3)
    
    # 5. ç¨³å®šæ€§åˆ†æ
    ax5 = plt.subplot(2, 3, 5)
    stability_scores = [1.0 / (1.0 + r['std_reward']) for r in results]
    plt.plot(round_nums, stability_scores, 'o-', linewidth=2, markersize=8, color='orange')
    plt.xlabel('è®­ç»ƒè½®æ¬¡ / Training Round')
    plt.ylabel('ç¨³å®šæ€§è¯„åˆ† / Stability Score')
    plt.title('è®­ç»ƒç¨³å®šæ€§è¶‹åŠ¿\\nTraining Stability Trend')
    plt.ylim(0, 1)
    plt.grid(True, alpha=0.3)
    
    # 6. ç´¯ç§¯æ€§èƒ½æŒ‡æ ‡
    ax6 = plt.subplot(2, 3, 6)
    cumulative_avg = []
    cumulative_rewards = []
    
    for result in results:
        cumulative_rewards.extend(result['rewards'])
        cumulative_avg.append(np.mean(cumulative_rewards))
    
    plt.plot(round_nums, cumulative_avg, 'o-', linewidth=2, markersize=8, color='purple')
    plt.xlabel('è®­ç»ƒè½®æ¬¡ / Training Round')
    plt.ylabel('ç´¯ç§¯å¹³å‡å¥–åŠ± / Cumulative Average Reward')
    plt.title('ç´¯ç§¯å­¦ä¹ æ•ˆæœ\\nCumulative Learning Effect')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"logs/extended_ppo_analysis_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"âœ… ç»¼åˆåˆ†æå›¾è¡¨å·²ä¿å­˜: {filename}")
    
    return filename, all_rewards, all_episodes

def print_comprehensive_analysis(results, all_rewards):
    """æ‰“å°ç»¼åˆåˆ†ææŠ¥å‘Š"""
    print("\\n" + "="*80)
    print("ğŸ“Š PPOæ‰©å±•è®­ç»ƒç»¼åˆåˆ†ææŠ¥å‘Š / Comprehensive PPO Training Analysis")
    print("="*80)
    
    if not results:
        print("âŒ æ²¡æœ‰è®­ç»ƒç»“æœå¯åˆ†æ")
        return
    
    # æ€»ä½“ç»Ÿè®¡
    total_episodes = sum(len(r['rewards']) for r in results)
    overall_avg = np.mean(all_rewards)
    overall_std = np.std(all_rewards)
    overall_max = np.max(all_rewards)
    overall_min = np.min(all_rewards)
    
    print(f"\\nğŸ¯ æ€»ä½“æ€§èƒ½ / Overall Performance:")
    print(f"  è®­ç»ƒè½®æ•° / Training Rounds: {len(results)}")
    print(f"  æ€»å›åˆæ•° / Total Episodes: {total_episodes}")
    print(f"  å¹³å‡å¥–åŠ± / Average Reward: {overall_avg:.3f} Â± {overall_std:.3f}")
    print(f"  æœ€ä½³å¥–åŠ± / Best Reward: {overall_max:.3f}")
    print(f"  æœ€å·®å¥–åŠ± / Worst Reward: {overall_min:.3f}")
    print(f"  å¥–åŠ±èŒƒå›´ / Reward Range: {overall_max - overall_min:.3f}")
    
    # å„è½®å¯¹æ¯”
    print(f"\\nğŸ“ˆ å„è½®è®­ç»ƒå¯¹æ¯” / Round-by-Round Comparison:")
    for result in results:
        improvement = ""
        if result['round'] > 1:
            prev_avg = results[result['round']-2]['avg_reward']
            change = result['avg_reward'] - prev_avg
            improvement = f" ({change:+.3f})"
        
        print(f"  ç¬¬{result['round']}è½®: {result['avg_reward']:.3f} Â± {result['std_reward']:.3f}{improvement}")
    
    # å­¦ä¹ è¶‹åŠ¿åˆ†æ
    if len(results) >= 2:
        first_round_avg = results[0]['avg_reward']
        last_round_avg = results[-1]['avg_reward']
        total_improvement = last_round_avg - first_round_avg
        improvement_pct = (total_improvement / abs(first_round_avg)) * 100 if first_round_avg != 0 else 0
        
        print(f"\\nğŸš€ å­¦ä¹ è¶‹åŠ¿ / Learning Trend:")
        print(f"  é¦–è½®å¹³å‡å¥–åŠ± / First Round Average: {first_round_avg:.3f}")
        print(f"  æœ«è½®å¹³å‡å¥–åŠ± / Last Round Average: {last_round_avg:.3f}")
        print(f"  æ€»ä½“æ”¹è¿› / Total Improvement: {total_improvement:+.3f} ({improvement_pct:+.1f}%)")
        
        if total_improvement > 0.1:
            print("  âœ… æ˜¾è‘—å­¦ä¹ æ”¹è¿› / Significant learning improvement!")
        elif total_improvement > 0:
            print("  âš¡ è½»å¾®å­¦ä¹ æ”¹è¿› / Slight learning improvement")
        else:
            print("  âš ï¸ æœªè§‚å¯Ÿåˆ°æ˜æ˜¾æ”¹è¿› / No significant improvement observed")
    
    # ç¨³å®šæ€§åˆ†æ
    avg_stability = np.mean([1.0 / (1.0 + r['std_reward']) for r in results])
    print(f"\\nğŸ² è®­ç»ƒç¨³å®šæ€§ / Training Stability:")
    print(f"  å¹³å‡ç¨³å®šæ€§è¯„åˆ† / Average Stability Score: {avg_stability:.3f}")
    
    if avg_stability > 0.7:
        print("  âœ… è®­ç»ƒæ•´ä½“ç¨³å®š / Training is generally stable")
    elif avg_stability > 0.5:
        print("  âš¡ è®­ç»ƒè¾ƒç¨³å®š / Training is moderately stable")
    else:
        print("  âš ï¸ è®­ç»ƒä¸ç¨³å®š / Training is unstable")
    
    # æ¨è
    print(f"\\nğŸ’¡ è®­ç»ƒå»ºè®® / Training Recommendations:")
    if overall_avg < -0.5:
        print("  ğŸ”§ å»ºè®®è°ƒæ•´è¶…å‚æ•°ä»¥æé«˜å¥–åŠ±")
        print("  ğŸ”§ Consider adjusting hyperparameters to improve rewards")
    if overall_std > 0.5:
        print("  ğŸ“Š å»ºè®®å¢åŠ è®­ç»ƒç¨³å®šæ€§æªæ–½")
        print("  ğŸ“Š Consider adding measures to improve training stability")
    if total_improvement > 0:
        print("  âœ… å­¦ä¹ ç®—æ³•æœ‰æ•ˆï¼Œå¯ç»§ç»­æ‰©å±•è®­ç»ƒ")
        print("  âœ… Learning algorithm is effective, can extend training")

if __name__ == "__main__":
    try:
        # è¿è¡Œæ‰©å±•è®­ç»ƒ
        results = run_extended_ppo_training()
        
        if results:
            # åˆ›å»ºç»¼åˆåˆ†æ
            chart_file, all_rewards, all_episodes = create_comprehensive_analysis(results)
            
            # æ‰“å°åˆ†ææŠ¥å‘Š
            print_comprehensive_analysis(results, all_rewards)
            
            print(f"\\nğŸ‰ PPOæ‰©å±•è®­ç»ƒéªŒè¯å®Œæˆ! / Extended PPO Training Validation Complete!")
            print(f"ğŸ“ˆ ç»¼åˆåˆ†æå›¾è¡¨: {chart_file}")
            print(f"ğŸ“ è¯·æŸ¥çœ‹ logs/ ç›®å½•ä¸­çš„è¯¦ç»†å›¾è¡¨æ–‡ä»¶")
            
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°è®­ç»ƒç»“æœ / No training results collected")
            
    except Exception as e:
        print(f"âŒ æ‰©å±•è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
