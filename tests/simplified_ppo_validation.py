#!/usr/bin/env python3
"""
ç®€åŒ–PPOå¤šè½®è®­ç»ƒéªŒè¯
Simplified PPO Multi-Round Training Validation
"""
import os
import sys
import subprocess
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

# ç¡®ä¿ä½¿ç”¨æµ‹è¯•æ¨¡å¼
os.environ['PIPELINE_TEST'] = '1'
sys.path.append('.')

def run_ppo_multiple_rounds():
    """è¿è¡Œå¤šè½®PPOè®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹å¤šè½®PPOè®­ç»ƒéªŒè¯ / Starting Multi-Round PPO Training")
    print("=" * 70)
    
    rounds_config = [
        {"episodes": 20, "name": "ç¬¬1è½®", "desc": "Round 1"},
        {"episodes": 25, "name": "ç¬¬2è½®", "desc": "Round 2"},
        {"episodes": 30, "name": "ç¬¬3è½®", "desc": "Round 3"}
    ]
    
    all_rewards = []
    round_summaries = []
    
    for i, config in enumerate(rounds_config):
        print(f"\nğŸ”„ {config['name']} ({config['desc']}) - {config['episodes']} ä¸ªå›åˆ")
        print("-" * 50)
        
        try:
            # è¿è¡Œè®­ç»ƒ
            result = subprocess.run([
                "D:\\conda_envs\\summer_project_2025\\python.exe",
                "train_ppo_safe.py", 
                "--episodes", str(config['episodes'])
            ], capture_output=True, text=True, cwd=".")
            
            if result.returncode == 0:
                # ä»è¾“å‡ºä¸­æå–å¥–åŠ±
                rewards = extract_rewards_from_output(result.stdout)
                
                if rewards:
                    round_summary = {
                        'round': i + 1,
                        'name': config['name'],
                        'rewards': rewards,
                        'avg': np.mean(rewards),
                        'std': np.std(rewards),
                        'max': np.max(rewards),
                        'min': np.min(rewards)
                    }
                    round_summaries.append(round_summary)
                    all_rewards.extend(rewards)
                    
                    print(f"âœ… {config['name']}å®Œæˆ:")
                    print(f"   ğŸ“Š å®é™…å›åˆæ•°: {len(rewards)}")
                    print(f"   ğŸ“ˆ å¹³å‡å¥–åŠ±: {round_summary['avg']:.3f} Â± {round_summary['std']:.3f}")
                    print(f"   ğŸ“Š å¥–åŠ±èŒƒå›´: {round_summary['min']:.3f} ~ {round_summary['max']:.3f}")
                else:
                    print(f"âš ï¸ {config['name']}æœªèƒ½æå–å¥–åŠ±æ•°æ®")
            else:
                print(f"âŒ {config['name']}å¤±è´¥: {result.stderr[:200]}")
                
        except Exception as e:
            print(f"âŒ {config['name']}å‡ºé”™: {e}")
    
    return round_summaries, all_rewards

def extract_rewards_from_output(output):
    """ä»è®­ç»ƒè¾“å‡ºä¸­æå–å¥–åŠ±å€¼"""
    rewards = []
    lines = output.split('\\n')
    
    for line in lines:
        if "å¥–åŠ±:" in line and "é•¿åº¦:" in line:
            try:
                # æå– "å¥–åŠ±: -1.000" è¿™æ ·çš„æ ¼å¼
                reward_part = line.split("å¥–åŠ±:")[1].split(",")[0].strip()
                reward = float(reward_part)
                rewards.append(reward)
            except (ValueError, IndexError):
                continue
    
    return rewards

def create_training_visualization(round_summaries, all_rewards):
    """åˆ›å»ºè®­ç»ƒå¯è§†åŒ–"""
    if not round_summaries:
        print("âŒ æ²¡æœ‰æ•°æ®å¯è§†åŒ–")
        return None
    
    print("\\nğŸ“Š åˆ›å»ºå­¦ä¹ æ›²çº¿å¯è§†åŒ–...")
    
    # åˆ›å»ºå›¾è¡¨
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. å„è½®å¥–åŠ±æ—¶é—´åºåˆ—
    colors = ['blue', 'red', 'green', 'purple', 'orange']
    episode_counter = 0
    
    for i, summary in enumerate(round_summaries):
        episodes = range(episode_counter, episode_counter + len(summary['rewards']))
        ax1.plot(episodes, summary['rewards'], 'o-', 
                alpha=0.7, color=colors[i % len(colors)], 
                label=summary['name'])
        episode_counter += len(summary['rewards'])
    
    # æ·»åŠ ç§»åŠ¨å¹³å‡
    if len(all_rewards) >= 8:
        window = 8
        moving_avg = np.convolve(all_rewards, np.ones(window)/window, mode='valid')
        moving_episodes = range(window-1, len(all_rewards))
        ax1.plot(moving_episodes, moving_avg, 'k-', linewidth=3, 
                label=f'{window}å›åˆç§»åŠ¨å¹³å‡', alpha=0.8)
    
    ax1.set_xlabel('ç´¯ç§¯å›åˆæ•°')
    ax1.set_ylabel('å¥–åŠ±')
    ax1.set_title('PPOå¤šè½®å­¦ä¹ æ›²çº¿\\nPPO Multi-Round Learning Curve')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. å„è½®å¹³å‡å¥–åŠ±å¯¹æ¯”
    round_nums = [s['round'] for s in round_summaries]
    avg_rewards = [s['avg'] for s in round_summaries]
    std_rewards = [s['std'] for s in round_summaries]
    
    bars = ax2.bar(round_nums, avg_rewards, yerr=std_rewards, 
                  capsize=5, alpha=0.7, color=colors[:len(round_summaries)])
    ax2.set_xlabel('è®­ç»ƒè½®æ¬¡')
    ax2.set_ylabel('å¹³å‡å¥–åŠ±')
    ax2.set_title('å„è½®å¹³å‡å¥–åŠ±å¯¹æ¯”\\nAverage Reward Comparison')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (x, y) in enumerate(zip(round_nums, avg_rewards)):
        ax2.text(x, y + std_rewards[i] + 0.02, f'{y:.3f}', 
                ha='center', va='bottom', fontweight='bold')
    ax2.grid(True, alpha=0.3)
    
    # 3. å¥–åŠ±åˆ†å¸ƒ
    ax3.hist(all_rewards, bins=15, alpha=0.7, color='lightblue', edgecolor='black')
    ax3.axvline(np.mean(all_rewards), color='red', linestyle='--', 
               label=f'å¹³å‡å€¼: {np.mean(all_rewards):.3f}')
    ax3.set_xlabel('å¥–åŠ±å€¼')
    ax3.set_ylabel('é¢‘æ¬¡')
    ax3.set_title('å¥–åŠ±åˆ†å¸ƒ\\nReward Distribution')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. å­¦ä¹ æ”¹è¿›è¶‹åŠ¿
    if len(round_summaries) > 1:
        improvements = []
        for i in range(1, len(round_summaries)):
            improvement = round_summaries[i]['avg'] - round_summaries[i-1]['avg']
            improvements.append(improvement)
        
        ax4.plot(range(2, len(round_summaries)+1), improvements, 'o-', 
                linewidth=2, markersize=8, color='green')
        ax4.axhline(0, color='red', linestyle='--', alpha=0.5)
        ax4.set_xlabel('è½®æ¬¡')
        ax4.set_ylabel('å¥–åŠ±æ”¹è¿›')
        ax4.set_title('è½®æ¬¡é—´æ”¹è¿›è¶‹åŠ¿\\nImprovement Trend')
        ax4.grid(True, alpha=0.3)
    else:
        ax4.text(0.5, 0.5, 'éœ€è¦æ›´å¤šè½®æ¬¡\\næ˜¾ç¤ºæ”¹è¿›è¶‹åŠ¿', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.set_title('æ”¹è¿›è¶‹åŠ¿\\nImprovement Trend')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"logs/ppo_multi_round_analysis_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"âœ… å¤šè½®å­¦ä¹ åˆ†æå›¾è¡¨å·²ä¿å­˜: {filename}")
    
    return filename

def analyze_multi_round_results(round_summaries, all_rewards):
    """åˆ†æå¤šè½®è®­ç»ƒç»“æœ"""
    print("\\n" + "="*70)
    print("ğŸ“Š PPOå¤šè½®è®­ç»ƒç»“æœåˆ†æ / Multi-Round Training Analysis")
    print("="*70)
    
    if not round_summaries:
        print("âŒ æ²¡æœ‰è®­ç»ƒç»“æœ")
        return
    
    # æ€»ä½“ç»Ÿè®¡
    total_episodes = sum(len(s['rewards']) for s in round_summaries)
    overall_avg = np.mean(all_rewards)
    overall_std = np.std(all_rewards)
    overall_max = np.max(all_rewards)
    overall_min = np.min(all_rewards)
    
    print(f"\\nğŸ¯ æ€»ä½“è¡¨ç° / Overall Performance:")
    print(f"  è®­ç»ƒè½®æ•°: {len(round_summaries)} è½®")
    print(f"  æ€»å›åˆæ•°: {total_episodes} ä¸ª")
    print(f"  å¹³å‡å¥–åŠ±: {overall_avg:.3f} Â± {overall_std:.3f}")
    print(f"  æœ€ä½³å¥–åŠ±: {overall_max:.3f}")
    print(f"  æœ€å·®å¥–åŠ±: {overall_min:.3f}")
    print(f"  å¥–åŠ±èŒƒå›´: {overall_max - overall_min:.3f}")
    
    # å„è½®å¯¹æ¯”
    print(f"\\nğŸ“ˆ å„è½®è¯¦ç»†å¯¹æ¯” / Round-by-Round Details:")
    for i, summary in enumerate(round_summaries):
        improvement = ""
        if i > 0:
            prev_avg = round_summaries[i-1]['avg']
            change = summary['avg'] - prev_avg
            improvement = f" ({change:+.3f})"
        
        print(f"  {summary['name']}: {summary['avg']:.3f} Â± {summary['std']:.3f}{improvement}")
        print(f"    â””â”€ èŒƒå›´: {summary['min']:.3f} ~ {summary['max']:.3f}, å›åˆæ•°: {len(summary['rewards'])}")
    
    # å­¦ä¹ è¶‹åŠ¿
    if len(round_summaries) >= 2:
        first_avg = round_summaries[0]['avg']
        last_avg = round_summaries[-1]['avg']
        total_improvement = last_avg - first_avg
        improvement_pct = (total_improvement / abs(first_avg)) * 100 if first_avg != 0 else 0
        
        print(f"\\nğŸš€ å­¦ä¹ è¶‹åŠ¿åˆ†æ / Learning Trend Analysis:")
        print(f"  é¦–è½®å¹³å‡: {first_avg:.3f}")
        print(f"  æœ«è½®å¹³å‡: {last_avg:.3f}")
        print(f"  æ€»ä½“æ”¹è¿›: {total_improvement:+.3f} ({improvement_pct:+.1f}%)")
        
        if total_improvement > 0.15:
            print("  âœ… æ˜¾è‘—å­¦ä¹ æ”¹è¿›! / Significant improvement!")
            assessment = "excellent"
        elif total_improvement > 0.05:
            print("  âš¡ è½»å¾®å­¦ä¹ æ”¹è¿› / Slight improvement")
            assessment = "good"
        elif total_improvement > -0.05:
            print("  â– åŸºæœ¬ç¨³å®š / Relatively stable")
            assessment = "stable"
        else:
            print("  âš ï¸ æ€§èƒ½ä¸‹é™ / Performance decline")
            assessment = "concerning"
    else:
        assessment = "insufficient_data"
    
    # ç¨³å®šæ€§è¯„ä¼°
    avg_stability = np.mean([1.0 / (1.0 + s['std']) for s in round_summaries])
    print(f"\\nğŸ² è®­ç»ƒç¨³å®šæ€§ / Training Stability:")
    print(f"  å¹³å‡ç¨³å®šæ€§è¯„åˆ†: {avg_stability:.3f} (0-1, è¶Šé«˜è¶Šç¨³å®š)")
    
    if avg_stability > 0.7:
        print("  âœ… è®­ç»ƒéå¸¸ç¨³å®š / Very stable training")
        stability = "high"
    elif avg_stability > 0.5:
        print("  âš¡ è®­ç»ƒè¾ƒç¨³å®š / Moderately stable")
        stability = "medium"
    else:
        print("  âš ï¸ è®­ç»ƒä¸ç¨³å®š / Unstable training")
        stability = "low"
    
    # ç»¼åˆè¯„ä¼°å’Œå»ºè®®
    print(f"\\nğŸ’¡ ç»¼åˆè¯„ä¼°ä¸å»ºè®® / Assessment & Recommendations:")
    
    if assessment == "excellent":
        print("  ğŸŒŸ PPOå­¦ä¹ æ•ˆæœä¼˜ç§€!")
        print("  ğŸŒŸ Excellent PPO learning performance!")
        print("  ğŸ“ˆ å»ºè®®ç»§ç»­æ‰©å±•è®­ç»ƒå›åˆæ•°ä»¥è¿›ä¸€æ­¥æå‡")
    elif assessment == "good":
        print("  âœ… PPOå­¦ä¹ æ•ˆæœè‰¯å¥½")
        print("  âœ… Good PPO learning performance")
        print("  ğŸ”§ å¯è€ƒè™‘å¾®è°ƒè¶…å‚æ•°ä¼˜åŒ–")
    elif assessment == "stable":
        print("  â– PPOå­¦ä¹ æ•ˆæœç¨³å®š")
        print("  â– Stable PPO learning performance")
        print("  ğŸ”§ å»ºè®®è°ƒæ•´å­¦ä¹ ç‡æˆ–å¥–åŠ±æœºåˆ¶")
    else:
        print("  âš ï¸ PPOå­¦ä¹ æ•ˆæœéœ€è¦æ”¹è¿›")
        print("  âš ï¸ PPO learning needs improvement")
        print("  ğŸ”§ å»ºè®®æ£€æŸ¥ç¯å¢ƒè®¾è®¡å’Œå¥–åŠ±å‡½æ•°")
    
    if stability == "low":
        print("  ğŸ“Š å»ºè®®å¢åŠ è®­ç»ƒç¨³å®šæ€§æªæ–½")
        print("  ğŸ“Š Consider adding stability measures")

if __name__ == "__main__":
    try:
        print("ğŸ¯ PPOå¤šè½®è®­ç»ƒéªŒè¯å¼€å§‹ / Multi-Round PPO Training Validation")
        
        # è¿è¡Œå¤šè½®è®­ç»ƒ
        round_summaries, all_rewards = run_ppo_multiple_rounds()
        
        if round_summaries and all_rewards:
            # åˆ›å»ºå¯è§†åŒ–
            chart_file = create_training_visualization(round_summaries, all_rewards)
            
            # åˆ†æç»“æœ
            analyze_multi_round_results(round_summaries, all_rewards)
            
            print(f"\\nğŸ‰ å¤šè½®PPOè®­ç»ƒéªŒè¯å®Œæˆ! / Multi-Round Training Complete!")
            if chart_file:
                print(f"ğŸ“ˆ åˆ†æå›¾è¡¨: {chart_file}")
            print(f"ğŸ“ è¯·æŸ¥çœ‹ logs/ ç›®å½•ä¸­çš„å›¾è¡¨æ–‡ä»¶")
            
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°è®­ç»ƒæ•°æ® / No training data collected")
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒéªŒè¯å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
