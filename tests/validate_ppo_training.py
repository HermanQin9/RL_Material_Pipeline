#!/usr/bin/env python3
"""
PPOè®­ç»ƒæ¨¡å¼éªŒè¯ - å¤šè½®è®­ç»ƒå¹¶ç»˜åˆ¶å­¦ä¹ æ›²çº¿
PPO Training Mode Validation - Multiple rounds with learning curves
"""
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import json

# è®¾ç½®è®­ç»ƒæ¨¡å¼ç¯å¢ƒå˜é‡
os.environ['PIPELINE_TEST'] = '0'

sys.path.append('.')
from ppo.trainer import PPOTrainer
from env.pipeline_env import PipelineEnv

def run_multiple_training_rounds(num_rounds=3, episodes_per_round=30):
    """
    è¿è¡Œå¤šè½®PPOè®­ç»ƒ
    Run multiple rounds of PPO training
    """
    print("ğŸš€ å¼€å§‹PPOè®­ç»ƒæ¨¡å¼éªŒè¯ / Starting PPO Training Mode Validation")
    print(f"ğŸ“Š é…ç½® / Configuration:")
    print(f"  - è®­ç»ƒè½®æ•° / Training Rounds: {num_rounds}")
    print(f"  - æ¯è½®å›åˆæ•° / Episodes per Round: {episodes_per_round}")
    print(f"  - æ€»å›åˆæ•° / Total Episodes: {num_rounds * episodes_per_round}")
    print("=" * 60)
    
    all_rewards = []
    all_episode_numbers = []
    round_summaries = []
    
    for round_num in range(1, num_rounds + 1):
        print(f"\nğŸ”„ ç¬¬ {round_num} è½®è®­ç»ƒ / Round {round_num} Training")
        print("-" * 40)
        
        # åˆ›å»ºç¯å¢ƒå’Œè®­ç»ƒå™¨
        env = PipelineEnv()
        trainer = PPOTrainer(env, learning_rate=3e-4, clip_ratio=0.2, hidden_size=64)
        
        round_rewards = []
        round_episodes = []
        
        try:
            # è®­ç»ƒæŒ‡å®šå›åˆæ•°
            for episode in range(episodes_per_round):
                obs = env.reset()
                total_reward = 0
                steps = 0
                done = False
                
                while not done and steps < 10:  # é™åˆ¶æœ€å¤§æ­¥æ•°
                    action, _ = trainer.select_action(obs)  # ä¿®æ­£ï¼šè·å–actionå’Œlog_prob
                    obs, reward, done, _, info = env.step(action)
                    total_reward += reward
                    steps += 1
                
                round_rewards.append(total_reward)
                episode_num = (round_num - 1) * episodes_per_round + episode + 1
                round_episodes.append(episode_num)
                
                # æ¯5ä¸ªå›åˆæ‰“å°ä¸€æ¬¡è¿›åº¦
                if (episode + 1) % 5 == 0:
                    avg_reward = np.mean(round_rewards[-5:])
                    print(f"  å›åˆ {episode + 1:2d}/30: å¹³å‡å¥–åŠ± = {avg_reward:.3f}")
            
            # è®°å½•æœ¬è½®ç»“æœ
            round_avg = np.mean(round_rewards)
            round_std = np.std(round_rewards)
            round_max = np.max(round_rewards)
            round_min = np.min(round_rewards)
            
            round_summary = {
                'round': round_num,
                'avg_reward': round_avg,
                'std_reward': round_std,
                'max_reward': round_max,
                'min_reward': round_min,
                'episodes': len(round_rewards)
            }
            round_summaries.append(round_summary)
            
            print(f"\nğŸ“ˆ ç¬¬ {round_num} è½®ç»“æœ / Round {round_num} Results:")
            print(f"  å¹³å‡å¥–åŠ± / Average Reward: {round_avg:.3f} Â± {round_std:.3f}")
            print(f"  æœ€å¤§å¥–åŠ± / Max Reward: {round_max:.3f}")
            print(f"  æœ€å°å¥–åŠ± / Min Reward: {round_min:.3f}")
            
            # ç´¯ç§¯æ‰€æœ‰æ•°æ®
            all_rewards.extend(round_rewards)
            all_episode_numbers.extend(round_episodes)
            
        except Exception as e:
            print(f"âŒ ç¬¬ {round_num} è½®è®­ç»ƒå‡ºé”™: {e}")
            continue
    
    return all_rewards, all_episode_numbers, round_summaries

def plot_learning_curves(rewards, episodes, round_summaries):
    """
    ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    Plot learning curves
    """
    print("\nğŸ“Š ç»˜åˆ¶å­¦ä¹ æ›²çº¿ / Plotting Learning Curves...")
    
    # åˆ›å»ºå›¾å½¢
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
    
    # å›¾1: åŸå§‹å¥–åŠ±æ›²çº¿
    ax1.plot(episodes, rewards, 'b-', alpha=0.6, linewidth=1, label='Episode Rewards')
    
    # è®¡ç®—ç§»åŠ¨å¹³å‡
    window_size = 10
    if len(rewards) >= window_size:
        moving_avg = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')
        moving_episodes = episodes[window_size-1:]
        ax1.plot(moving_episodes, moving_avg, 'r-', linewidth=2, label=f'Moving Average ({window_size} episodes)')
    
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('Reward')
    ax1.set_title('PPO Learning Curve - Training Mode (4000 samples)\nPPOå­¦ä¹ æ›²çº¿ - è®­ç»ƒæ¨¡å¼ (4000æ ·æœ¬)', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å›¾2: æ¯è½®å¹³å‡å¥–åŠ±
    if round_summaries:
        round_nums = [r['round'] for r in round_summaries]
        round_avgs = [r['avg_reward'] for r in round_summaries]
        round_stds = [r['std_reward'] for r in round_summaries]
        
        ax2.errorbar(round_nums, round_avgs, yerr=round_stds, 
                    marker='o', linewidth=2, markersize=8, capsize=5)
        ax2.set_xlabel('Training Round / è®­ç»ƒè½®æ¬¡')
        ax2.set_ylabel('Average Reward / å¹³å‡å¥–åŠ±')
        ax2.set_title('Average Reward per Training Round\næ¯è½®è®­ç»ƒçš„å¹³å‡å¥–åŠ±', fontsize=14)
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (x, y) in enumerate(zip(round_nums, round_avgs)):
            ax2.annotate(f'{y:.3f}', (x, y), textcoords="offset points", 
                        xytext=(0,10), ha='center', fontsize=10)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"logs/ppo_training_curves_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"âœ… å­¦ä¹ æ›²çº¿å·²ä¿å­˜: {filename}")
    
    return filename

def analyze_results(rewards, round_summaries):
    """
    åˆ†æè®­ç»ƒç»“æœ
    Analyze training results
    """
    print("\n" + "="*60)
    print("ğŸ“Š PPOè®­ç»ƒç»“æœåˆ†æ / PPO Training Results Analysis")
    print("="*60)
    
    # æ•´ä½“ç»Ÿè®¡
    overall_avg = np.mean(rewards)
    overall_std = np.std(rewards)
    overall_max = np.max(rewards)
    overall_min = np.min(rewards)
    
    print(f"\nğŸ¯ æ•´ä½“æ€§èƒ½ / Overall Performance:")
    print(f"  æ€»å›åˆæ•° / Total Episodes: {len(rewards)}")
    print(f"  å¹³å‡å¥–åŠ± / Average Reward: {overall_avg:.3f} Â± {overall_std:.3f}")
    print(f"  æœ€ä½³å¥–åŠ± / Best Reward: {overall_max:.3f}")
    print(f"  æœ€å·®å¥–åŠ± / Worst Reward: {overall_min:.3f}")
    print(f"  å¥–åŠ±èŒƒå›´ / Reward Range: {overall_max - overall_min:.3f}")
    
    # å­¦ä¹ è¶‹åŠ¿åˆ†æ
    if len(rewards) >= 20:
        first_half = rewards[:len(rewards)//2]
        second_half = rewards[len(rewards)//2:]
        
        improvement = np.mean(second_half) - np.mean(first_half)
        improvement_pct = (improvement / abs(np.mean(first_half))) * 100 if np.mean(first_half) != 0 else 0
        
        print(f"\nğŸ“ˆ å­¦ä¹ è¶‹åŠ¿ / Learning Trend:")
        print(f"  å‰åŠæ®µå¹³å‡å¥–åŠ± / First Half Average: {np.mean(first_half):.3f}")
        print(f"  ååŠæ®µå¹³å‡å¥–åŠ± / Second Half Average: {np.mean(second_half):.3f}")
        print(f"  æ”¹è¿›å¹…åº¦ / Improvement: {improvement:+.3f} ({improvement_pct:+.1f}%)")
        
        if improvement > 0.05:
            print("  âœ… æ˜¾è‘—å­¦ä¹ æ”¹è¿› / Significant learning improvement detected!")
        elif improvement > 0:
            print("  âš¡ è½»å¾®å­¦ä¹ æ”¹è¿› / Slight learning improvement detected")
        else:
            print("  âš ï¸ æœªè§‚å¯Ÿåˆ°æ˜æ˜¾æ”¹è¿› / No significant improvement observed")
    
    # æ¯è½®å¯¹æ¯”
    if len(round_summaries) > 1:
        print(f"\nğŸ”„ è½®æ¬¡å¯¹æ¯” / Round Comparison:")
        for i, summary in enumerate(round_summaries):
            print(f"  ç¬¬{summary['round']}è½® / Round {summary['round']}: "
                  f"{summary['avg_reward']:.3f} Â± {summary['std_reward']:.3f}")
    
    # ç¨³å®šæ€§åˆ†æ
    stability = 1.0 / (1.0 + overall_std)  # ç®€å•çš„ç¨³å®šæ€§æŒ‡æ ‡
    print(f"\nğŸ² è®­ç»ƒç¨³å®šæ€§ / Training Stability:")
    print(f"  å¥–åŠ±æ–¹å·® / Reward Variance: {overall_std**2:.3f}")
    print(f"  ç¨³å®šæ€§è¯„åˆ† / Stability Score: {stability:.3f} (0-1, è¶Šé«˜è¶Šç¨³å®š)")
    
    if stability > 0.7:
        print("  âœ… è®­ç»ƒç¨³å®š / Training is stable")
    elif stability > 0.5:
        print("  âš¡ è®­ç»ƒè¾ƒç¨³å®š / Training is moderately stable")
    else:
        print("  âš ï¸ è®­ç»ƒä¸ç¨³å®š / Training is unstable")

if __name__ == "__main__":
    try:
        # è¿è¡Œå¤šè½®è®­ç»ƒ
        rewards, episodes, summaries = run_multiple_training_rounds(num_rounds=3, episodes_per_round=30)
        
        if len(rewards) > 0:
            # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
            curve_file = plot_learning_curves(rewards, episodes, summaries)
            
            # åˆ†æç»“æœ
            analyze_results(rewards, summaries)
            
            print(f"\nğŸ‰ PPOè®­ç»ƒéªŒè¯å®Œæˆ! / PPO Training Validation Complete!")
            print(f"ğŸ“ˆ å­¦ä¹ æ›²çº¿æ–‡ä»¶: {curve_file}")
            
        else:
            print("âŒ æ²¡æœ‰æ”¶é›†åˆ°è®­ç»ƒæ•°æ® / No training data collected")
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
