# Machine Learning Pipeline with PPO RL 

### Author: Herman Qin

## Project Overview

This project implements a Reinforcement Learning (RL)-based automated Machine Learning (AutoML) pipeline. Utilizing the Proximal Policy Optimization (PPO) algorithm, the pipeline automatically selects and tunes preprocessing methods, feature engineering techniques, and machine learning models to optimize predictive performance for formation energy prediction.

### Key Features

- ðŸ¤– **Automated ML Pipeline**: PPO-driven automatic selection of preprocessing and ML methods
- ðŸ“Š **Scalable Data Processing**: Support for both 200-sample testing and 4K production datasets
- ðŸ”¬ **Materials Science Focus**: Specialized for formation energy prediction using Materials Project data
- ðŸŽ¯ **Reinforcement Learning**: PPO algorithm for intelligent pipeline optimization
- ðŸ“ˆ **Comprehensive Analysis**: Built-in tools for training analysis and visualization
- ðŸ§ª **Extensive Testing**: Complete test suite for all components
- ðŸ“š **Rich Documentation**: Detailed documentation for all aspects of the project

### Technical Highlights

- **Multi-scale Dataset Support**: Seamless switching between test (200) and production (4K) datasets
- **Robust Error Handling**: Safe data processing with comprehensive error recovery
- **Modular Architecture**: Clean separation of concerns with dedicated modules for each component
- **Advanced Featurization**: Integration with matminer for materials property features
- **Flexible Configuration**: Environment-based configuration for different deployment scenarios

## Repository Structure

```
.
â”œâ”€â”€ config.py            # Global configuration (paths, API keys, hyperparameters)
â”œâ”€â”€ nodes.py             # Node base class and all node implementations
â”œâ”€â”€ pipeline.py          # Main pipeline (run_pipeline)
â”œâ”€â”€ methods/             # Data and model methods
â”‚   â”œâ”€â”€ data_methods.py  # Data processing functions
â”‚   â””â”€â”€ model_methods.py # Model training and prediction functions
â”œâ”€â”€ env/                 # Environment-related code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline_env.py  # PipelineEnv class definition
â”‚   â””â”€â”€ utils.py         # Helper functions (_get_obs, _compute_action_mask, reward calculation)
â”œâ”€â”€ ppo/                 # PPO algorithm components
â”‚   â”œâ”€â”€ __init__.py      #
â”‚   â”œâ”€â”€ policy.py        # PPOPolicy implementation
â”‚   â”œâ”€â”€ buffer.py        # RolloutBuffer (stores transitions)
â”‚   â”œâ”€â”€ utils.py         # Utility functions (GAE, loss calculation)
â”‚   â””â”€â”€ trainer.py       # Training loop and optimization logic
â”œâ”€â”€ scripts/             # Command-line scripts
â”‚   â”œâ”€â”€ train_ppo.py     # PPO training script
â”‚   â”œâ”€â”€ eval_ppo.py      # Policy evaluation script
â”‚   â”œâ”€â”€ example_usage.py # Example usage demonstration
â”‚   â”œâ”€â”€ debug_pipeline.py # Pipeline debugging utilities
â”‚   â”œâ”€â”€ train_ppo_4k.py  # 4K dataset PPO training
â”‚   â”œâ”€â”€ train_ppo_safe.py # Safe PPO training with error handling
â”‚   â”œâ”€â”€ generate_4k_data.py # 4K dataset generation
â”‚   â”œâ”€â”€ fix_4k_data.py   # Data fixing utilities
â”‚   â”œâ”€â”€ main.py          # Main execution script
â”‚   â”œâ”€â”€ run.py           # Alternative run script
â”‚   â”œâ”€â”€ analysis/        # Analysis and visualization scripts
â”‚   â”‚   â”œâ”€â”€ analyze_ppo_results.py # PPO results analysis
â”‚   â”‚   â””â”€â”€ reward_analysis.py     # Reward function analysis
â”‚   â””â”€â”€ debug/           # Debugging utilities
â”‚       â””â”€â”€ check_training_mode.py # Training mode checker
â”œâ”€â”€ tests/               # Unit tests and validation scripts
â”‚   â”œâ”€â”€ test_all_files.py
â”‚   â”œâ”€â”€ test_all_models.py
â”‚   â”œâ”€â”€ test_and_train_ppo.py
â”‚   â”œâ”€â”€ test_components.py
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â”œâ”€â”€ test_ppo.py
â”‚   â”œâ”€â”€ test_data_methods.py
â”‚   â”œâ”€â”€ test_4k_data.py  # 4K dataset testing
â”‚   â”œâ”€â”€ test_ppo_simple.py # Simple PPO testing
â”‚   â”œâ”€â”€ validate_ppo_training.py # PPO training validation
â”‚   â”œâ”€â”€ extended_ppo_validation.py # Extended validation
â”‚   â””â”€â”€ simplified_ppo_validation.py # Simplified validation
â”œâ”€â”€ utils/               # Utility functions
â”‚   â””â”€â”€ pipeline_utils.py # Pipeline utilities
â”œâ”€â”€ notebooks/           # Jupyter notebooks
â”‚   â”œâ”€â”€ PPO_Testing_and_Debugging.ipynb
â”‚   â””â”€â”€ _setup.ipynb
â”œâ”€â”€ docs/                # Documentation files
â”‚   â”œâ”€â”€ DATASET_INFO.md          # Dataset information and analysis
â”‚   â”œâ”€â”€ GITHUB_UPLOAD_REPORT.md  # GitHub upload status report
â”‚   â”œâ”€â”€ PPO_TRAINING_ANALYSIS.md # PPO training analysis results
â”‚   â”œâ”€â”€ PPO_VALIDATION_REPORT.md # PPO validation results
â”‚   â”œâ”€â”€ PROJECT_ORGANIZATION.md  # Project organization guide
â”‚   â”œâ”€â”€ PROJECT_ORGANIZATION_COMPLETION.md # Organization completion report
â”‚   â””â”€â”€ STRUCTURE_ANALYSIS.md    # Project structure analysis
â”œâ”€â”€ data/                # Data storage
â”‚   â”œâ”€â”€ raw/             # Original datasets
â”‚   â””â”€â”€ processed/       # Processed datasets
â”œâ”€â”€ models/              # Trained model checkpoints cache
â”œâ”€â”€ logs/                # Training and evaluation logs
â””â”€â”€ dash_app/            # Visualization dashboard application
    â””â”€â”€ data/            # Dashboard-specific data
```

## Key Components

### Nodes and Methods

The pipeline consists of the following nodes and methods:

| Node | Name            | Available Methods               |
| ---- | --------------- | ------------------------------- |
| N0   | Data\_Fetch     | api                             |
| N1   | Impute          | mean, median, knn, none         |
| N2   | Feature\_Matrix | default                         |
| N3   | Feat\_Select    | none, variance, univariate, pca |
| N4   | Scale           | std, robust, minmax, none       |
| N5   | Learner         | rf, gbr, lgbm, xgb, cat         |
| N6   | END             | Pipeline termination node       |

### PPO Reinforcement Learning

The PPO algorithm automatically selects:

* **Nodes**: The sequence of steps in the pipeline.
* **Methods**: Specific methods at each node.
* **Hyperparameters**: Optimal parameter settings for each method.

## Installation

### Prerequisites
- Python 3.8+ 
- Conda (recommended) or pip
- Git
- Materials Project API key (for data access)

### Setup Instructions

1. **Clone the repository:**
   ```bash
   git clone https://github.com/HermanQin9/Summer_Project_Clear_Version.git
   cd Summer_Project_Clear_Version
   git checkout 2025-07-24
   ```

2. **Create and activate the environment:**
   ```bash
   # Using conda (recommended)
   conda env create -f environment.yml
   conda activate summer_project_2025
   
   # Or using pip
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure API access:**
   - Get your Materials Project API key from https://materialsproject.org/api
   - Add it to `config.py`:
     ```python
     API_KEY = "your_api_key_here"
     ```

5. **Verify installation:**
   ```bash
   python scripts/debug/check_training_mode.py
   ```

## Quick Start

### 1. Basic Material Property Prediction
```bash
# Run the pipeline with test data
python scripts/example_usage.py

# Or use interactive mode
python -c "from scripts.example_usage import run_example; run_example()"
```

### 2. PPO Reinforcement Learning Training
```bash
# Quick PPO training test
python scripts/train_ppo.py

# For extended training with 4K dataset
export PIPELINE_TEST=4k  # On Windows: set PIPELINE_TEST=4k
python scripts/train_ppo.py
```

### 3. Environment Testing
```bash
# Test the RL environment
python scripts/debug/debug_pipeline.py

# Validate all components
python tests/test_components.py
```

### 4. Analysis and Visualization
```bash
# Analyze PPO training results
python scripts/analysis/analyze_ppo_results.py

# Generate detailed reward analysis
python scripts/analysis/reward_analysis.py
```

## Running the Project

### Training PPO

**Standard training (200 samples):**
```bash
python scripts/train_ppo.py
```

**4K dataset training:**
```bash
# Windows
set PIPELINE_TEST=4k
python scripts/train_ppo.py

# Linux/Mac
export PIPELINE_TEST=4k
python scripts/train_ppo.py
```

**Monitor training progress:**
- Training logs are saved to `logs/` directory
- Learning curves are automatically generated
- Check console output for real-time statistics

**Expected Results:**
- Standard training: ~40 episodes, 85% completion rate
- 4K training: Extended episodes with comprehensive learning
- Processing speed: ~695K samples/second

### Data Generation and Testing

**Test pipeline components:**
```bash
# Test all core components
python tests/test_components.py

# Test specific models
python tests/test_all_models.py

# Test pipeline functionality
python tests/test_pipeline.py
```

**Environment debugging:**
```bash
# Debug pipeline environment
python scripts/debug/debug_pipeline.py

# Check training mode configuration
python scripts/debug/check_training_mode.py
```

**Data validation:**
- Raw data is cached in `data/processed/`
- 4K dataset: `mp_data_cache_200_test.pkl`
- Feature data: `all_data_feat.csv`
- Model files: `models/` directory (RF, XGBoost, scaler)

### Analysis and Visualization

**Comprehensive PPO analysis:**
```bash
# Generate learning curves and performance metrics
python scripts/analysis/analyze_ppo_results.py
```

**Reward function analysis:**
```bash
# Detailed reward mechanism evaluation
python scripts/analysis/reward_analysis.py
```

**Generated outputs:**
- Learning curves: `logs/ppo_learning_curves_[timestamp].png`
- Performance reports: Console output with detailed metrics
- Comparative analysis: Episode success rates, reward distributions

### Example Usage

**Basic pipeline demonstration:**
```bash
# Run complete example workflow
python scripts/example_usage.py
```

**Interactive usage:**
```python
from scripts.example_usage import run_example
from methods.data_methods import MaterialsData

# Quick demonstration
run_example()

# Custom data processing
data_processor = MaterialsData(api_key="your_key")
materials = data_processor.get_materials_data(limit=10)
```

**Key demonstration features:**
- Material property prediction workflow
- PPO training integration
- Real-time performance metrics
- Error handling and validation

### Debugging

**Check training mode:**
```bash
python scripts/debug/check_training_mode.py
```

## Testing

### Unit Tests
Run comprehensive unit tests from the project root:

```bash
# Test all core components
python tests/test_components.py

# Test pipeline functionality
python tests/test_pipeline.py

# Test PPO implementation
python tests/test_ppo.py

# Test all models
python tests/test_all_models.py

# Test all files (comprehensive)
python tests/test_all_files.py
```

### Integration Testing
```bash
# Complete pipeline validation
python tests/test_pipeline.py

# Environment functionality
python scripts/debug/debug_pipeline.py
```

### Performance Validation
- **PPO Training**: 85% completion rate on 4K dataset
- **Processing Speed**: ~695,122 samples/second
- **Memory Usage**: Optimized for large dataset processing
- **API Integration**: Robust Materials Project data fetching

### Jupyter Notebooks
Interactive testing and development:

* `PPO_Testing_and_Debugging.ipynb` - PPO development and validation
* `_setup.ipynb` - Environment configuration and setup

## Visualization

### Dashboard (Future Development)
Launch the dashboard app:

```bash
cd dash_app
python app.py
```

*Note: Dashboard functionality is planned for future development*

## Data and Models

### Dataset Support
- **Standard Dataset**: 200 material samples for quick testing
- **4K Dataset**: 4,000+ material samples for production training
- **Data Source**: Materials Project (MP) API
- **Target Property**: Formation energy per atom

### Model Storage
- **Checkpoints**: Stored in `models/` directory
- **Training Logs**: Stored in `logs/` directory  
- **Cached Data**: Stored in `data/processed/`

## Documentation

Comprehensive documentation available in the `docs/` directory:

- **Dataset Info**: `DATASET_INFO.md` - Detailed dataset information
- **PPO Analysis**: `PPO_TRAINING_ANALYSIS.md` - Training results analysis
- **Validation Report**: `PPO_VALIDATION_REPORT.md` - Validation results
- **Project Organization**: `PROJECT_ORGANIZATION.md` - Development guide
- **Structure Analysis**: `STRUCTURE_ANALYSIS.md` - Architecture comparison

## Contributing

1. Clone the repository:
   ```bash
   git clone https://github.com/HermanQin9/Summer_Project_Clear_Version.git
   cd Summer_Project_Clear_Version
   git checkout 2025-07-24
   ```

2. Set up the environment:
   ```bash
   conda env create -f environment.yml
   conda activate base
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Configure your Materials Project API key in `config.py`

## Performance Benchmarks

### PPO Training Results (4K Dataset)
- **Success Rate**: 85% (34/40 episodes completed)
- **Processing Speed**: 695,122 samples/second
- **Training Time**: ~40 episodes for convergence
- **Memory Efficiency**: Optimized for large dataset processing

### Model Performance
- **Random Forest**: Robust formation energy prediction
- **XGBoost**: High-performance gradient boosting
- **Feature Engineering**: Advanced material property extraction
- **Scalability**: Supports datasets from 200 to 4K+ samples

## Troubleshooting

### Common Issues

1. **API Key Error**: Ensure your Materials Project API key is correctly set in `config.py`
2. **Memory Issues**: For large datasets, monitor RAM usage during training
3. **Import Errors**: Ensure all dependencies are installed via `requirements.txt`
4. **Training Slowdown**: Use `PIPELINE_TEST` environment variable for dataset size control

### Debug Mode
```bash
# Enable verbose logging
export DEBUG=1  # Windows: set DEBUG=1
python scripts/train_ppo.py
```

## License

This project is part of a summer research program. Please contact the author for usage permissions.

## Author

**Herman Qin**  
Summer Research Project 2025

For questions or contributions, please open an issue on GitHub.

## Project Status

- âœ… **4K Dataset Support**: Complete with generation and validation tools
- âœ… **PPO Training**: Functional with both 200 and 4K datasets
- âœ… **Analysis Tools**: Comprehensive result analysis and visualization
- âœ… **Testing Suite**: Complete test coverage for all components
- ðŸš§ **Dashboard**: Planned for future development

---

**Author**: Herman Qin  
**Repository**: https://github.com/HermanQin9/Summer_Project_Clear_Version  
**License**: MIT (if applicable)  
**Last Updated**: July 24, 2025
