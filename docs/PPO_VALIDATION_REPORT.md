# PPO
# PPO Reinforcement Learning Training Validation Report

## / Project Overview

PPO (Proximal Policy Optimization) 

## SUCCESS / Validation Results

### SUCCESS
- **Python**: 3.11.13
- **Conda**: summer_project_2025
- ****:
 - PyTorch: 2.5.1
 - NumPy: 2.3.2
 - Pandas: 2.3.1
 - Scikit-learn: 1.6.1
 - Matplotlib: 3.10.3

### SUCCESS
- 
- `tests/` 
- `docs/` 
- Jupyter `notebooks/` 
- `utils/` 
- import

### PPO SUCCESS
- ** (PipelineEnv)**: 
- ** (PPOPolicy)**: 
- ** (PPOTrainer)**: 15

### 
- ****: 15
- ****: -0.733
- ****: 0.442
- ****: 0.000
- ****: -1.000
- ****: +0.232 (: -0.857 : -0.625)
- ****: SUCCESS !

### 
:
1. `logs/ppo_test_curves.png` - 
2. `logs/detailed_ppo_curves.png` - 
3. `logs/ppo_curves_20250724_165327.png` - 

### 
:
- N0 () N2 ()
- N0 N2 N1 () 
- N0 N2 N3 () N1 N4 ()
- : N0 N2 N3 N1 N4 N5 ()

## START / Key Achievements

1. ****: import
2. **PPO**: PPO
3. ****: PPO
4. ****: 
5. ****: NaN

## / Technical Details

### (Observation State)
```python
obs = {
 'fingerprint': [MAE, R2, feature_count], # 
 'node_visited': [bool] * 5, # 
 'action_mask': [float] * 5 # 
}
```

### (Action Space)
```python
action = {
 'node': int, # (0-5)
 'method': int, # 
 'params': float # (0-1)
}
```

### (Reward Mechanism)
- RÂ²MAE
- 
- 

## / Resolved Technical Issues

1. **NumPy DLL**: 
2. **Pillow**: Matplotlib
3. **Import**: 
4. ****: 
5. ****: NaN
6. ****: step

## / Learning Curve Analysis

:
- ****: ( -1.0)
- ****: 
- ****: 

## / Conclusion

SUCCESS ****: PPO:

1. ****
2. ****
3. ****
4. ****

## / File Locations

- ****: `train_ppo_safe.py`
- ****: `test_ppo_simple.py`
- ****: `logs/detailed_ppo_curves.png`
- ****: `check_env.py`
- ****: `run.py`

---

 ****: PPO

*: 2025724*
