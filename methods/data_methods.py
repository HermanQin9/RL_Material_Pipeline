# methods/data_methods.py
"""
æ•°æ®å¤„ç†æ–¹æ³•æ¨¡å— / Data Processing Methods Module

This module contains all data processing methods for the ML pipeline including
data fetching, feature matrix construction, imputation, feature selection, and scaling.
æœ¬æ¨¡å—åŒ…å«æœºå™¨å­¦ä¹ æµæ°´çº¿çš„æ‰€æœ‰æ•°æ®å¤„ç†æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®è·å–ã€ç‰¹å¾çŸ©é˜µæ„å»ºã€ç¼ºå¤±å€¼å¡«å……ã€ç‰¹å¾é€‰æ‹©å’Œæ•°æ®ç¼©æ”¾ã€‚

Main functions:
- fetch_and_featurize: è·å–å¹¶ç‰¹å¾åŒ–æ•°æ® / Fetch and featurize data
- feature_matrix: æ„å»ºç‰¹å¾çŸ©é˜µ / Construct feature matrix  
- impute_data: ç¼ºå¤±å€¼å¡«å…… / Missing data imputation
- feature_selection: ç‰¹å¾é€‰æ‹© / Feature selection
- scale_features: ç‰¹å¾ç¼©æ”¾ / Feature scaling
"""
from __future__ import annotations
import logging
import os
import joblib
from pathlib import Path
import pickle
from typing import Any, Dict, Tuple

import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from tqdm import tqdm
from mp_api.client import MPRester
from matminer.featurizers.structure import DensityFeatures, GlobalSymmetryFeatures
from matminer.featurizers.composition import ElementProperty
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler

from config import API_KEY, CACHE_FILE, PROC_DIR, BATCH_SIZE, N_TOTAL, TARGET_PROP, MODEL_DIR

# åˆå§‹åŒ– Logger
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

__all__ = [
    "fetch_and_featurize",
    "prepare_node_input",
    "validate_state_keys", 
    "split_labels",
    "update_state",
    # other dataâ€‘node functions â€¦
]

# Features will be used
FEATURE_METHODS = [
    (ElementProperty.from_preset("magpie"), "composition"),
    (DensityFeatures(), "structure"),
    (GlobalSymmetryFeatures(), "structure"),
]

# Set the cache and processed data paths
CACHE_PATH = Path(CACHE_FILE)
PROC_PATH = Path(PROC_DIR)

# Change to the public function
def split_by_fe(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """æŒ‰æ˜¯å¦å« Fe å°†æ•°æ®åˆ‡åˆ†ä¸º train/test"""
    mask = df["composition"].apply(lambda c: c is not None and c.as_dict().get("Fe", 0) > 0)
    train_df = df[~mask].reset_index(drop=True)
    test_df = df[mask].reset_index(drop=True)
    return train_df, test_df


def safe_featurize(feat_obj, x):
    """å®‰å…¨çš„ç‰¹å¾åŒ–å‡½æ•°ï¼Œå¤„ç†å¯èƒ½çš„é”™è¯¯"""
    if x is None:
        return [np.nan] * len(feat_obj.feature_labels())
    try:
        return feat_obj.featurize(x)
    except (TypeError, ValueError, AttributeError) as e:
        # å½“é‡åˆ° NoneType é”™è¯¯æˆ–å…¶ä»–ç‰¹å¾åŒ–é”™è¯¯æ—¶ï¼Œè¿”å› NaN
        logger.warning(f"Featurization failed for {type(feat_obj).__name__}: {str(e)[:100]}")
        return [np.nan] * len(feat_obj.feature_labels())

def apply_featurizers(df: pd.DataFrame) -> pd.DataFrame:
    """å¯¹ DataFrame æ‰¹é‡åº”ç”¨ featurizers å¹¶æ‹¼æ¥ç»“æœ"""
    parts: list[pd.DataFrame] = [df]
    for feat_obj, col in FEATURE_METHODS:
        labels = feat_obj.feature_labels()
        logger.info(f"Applying {type(feat_obj).__name__} to column '{col}' ({len(df)} samples)")
        # ä½¿ç”¨å®‰å…¨çš„ç‰¹å¾åŒ–å‡½æ•°
        array = Parallel(n_jobs=-1)(delayed(safe_featurize)(feat_obj, v) for v in df[col])
        parts.append(pd.DataFrame(array, columns=labels)) # type: ignore
        logger.info(f"  Generated {len(labels)} features")
    return pd.concat(parts, axis=1)

# Node 0: è·å–æ•°æ®ã€ç‰¹å¾åŒ–å¹¶åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
def fetch_and_featurize(_: Any = None, cache: bool = True) -> Dict[str, Any]:
    """N0: è·å–æ•°æ®ã€ç‰¹å¾åŒ–å¹¶åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†"""
    logger.info("N0 start: cache=%s", cache)

    full_df = fetch_data(cache=cache)
    featurized_df = featurize_data(full_df)
    train_df, test_df = split_data(featurized_df)

    logger.info("N0 complete: train=%d, test=%d", len(train_df), len(test_df))
    return {
        "train_data": train_df,
        "test_data": test_df,
        "full_data": featurized_df,
        "X_train": None, "X_val": None, "X_test": None,
        "y_train": None, "y_val": None, "y_test": None,
    }

def get_value(d, key, default=None):
    # æ”¯æŒ dictã€å¯¹è±¡ï¼ˆå¦‚ dataclass æˆ–ä¸€èˆ¬æœ‰å±æ€§çš„å¯¹è±¡ï¼‰
    if isinstance(d, dict):
        return d.get(key, default)
    elif hasattr(d, key):
        return getattr(d, key, default)
    else:
        return default

# Make the N0 into 3 functions:
def fetch_data(cache: bool = True) -> pd.DataFrame:
    if cache and CACHE_PATH.exists():
        logger.info("Loading cache from %s", CACHE_PATH)
        with open(CACHE_PATH, "rb") as f:
            cached = pickle.load(f)
        if isinstance(cached, dict):
            return cached["full_data"]
        elif isinstance(cached, pd.DataFrame):
            return cached
        else:
            raise TypeError(f"Unsupported cache type: {type(cached)}")

    logger.info("Fetching data from MP API")
    dfs = []
    fetched = 0
    with MPRester(API_KEY) as mpr:
        docs_iter = mpr.materials.summary.search(
            fields=["material_id", "structure", "elements", "formula_pretty", TARGET_PROP],
            chunk_size=BATCH_SIZE,
            num_chunks=N_TOTAL // BATCH_SIZE + 1,
        )
        for docs in tqdm(docs_iter, desc="MP fetch", ncols=80):
            docs = docs if isinstance(docs, list) else [docs]
            valid = [d for d in docs if getattr(d, TARGET_PROP, None) is not None]
            if not valid:
                continue
            df_b = pd.DataFrame([
                {
                    "material_id": get_value(d, "material_id"),
                    "structure": get_value(d, "structure"),
                    "elements": get_value(d, "elements"),
                    "formula_pretty": get_value(d, "formula_pretty"),
                    TARGET_PROP: get_value(d, TARGET_PROP),
                } for d in valid
            ]).dropna(subset=["structure"]).reset_index(drop=True)
            df_b["composition"] = df_b["structure"].apply(lambda s: getattr(s, 'composition', None) if s is not None else None)
            dfs.append(df_b)
            fetched += len(df_b)
            if fetched >= N_TOTAL:
                break

    if not dfs:
        raise RuntimeError("No data fetched from API")

    full_df = pd.concat(dfs, ignore_index=True)[:N_TOTAL]

    # ç¼“å­˜
    CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
    with open(CACHE_PATH, "wb") as f:
        pickle.dump(full_df, f)

    return full_df

def featurize_data(df: pd.DataFrame) -> pd.DataFrame:
    df_feat = apply_featurizers(df)
    df_feat.to_csv(PROC_PATH / "all_data_feat.csv", index=False)
    return df_feat

def split_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    train_df, test_df = split_by_fe(df)
    if train_df.empty or test_df.empty:
        raise RuntimeError("Train/Test split resulted in empty dataframe!")
    return train_df, test_df



# N1: Impution Node
# Incase of the missinf values, we will use the imputer to fill them.

def impute_data(data, strategy='mean', params=None):
    strategies = {
        'mean': impute_mean,
        'median': impute_median,
        'knn': impute_knn,
        'none': impute_none
    }
    if strategy not in strategies:
        raise ValueError(f"Unknown strategy: {strategy}")
    return strategies[strategy](data, **(params or {}))

def drop_allnan(df):
    return df.dropna(axis=1, how='all') if isinstance(df, pd.DataFrame) else df

def apply_imputer(imp, data):
    """Forï¼ˆX_train, X_val, X_testï¼‰use the same imputerï¼Œformat in dict"""
    X_train = imp.fit_transform(drop_allnan(data['X_train']))
    X_val   = imp.transform(drop_allnan(data['X_val']))
    X_test_raw = data.get('X_test')
    X_test  = imp.transform(drop_allnan(X_test_raw)) if X_test_raw is not None else None
    assert not np.isnan(X_train).any(), "NaN remains after impute!"
    return {
        'X_train': X_train,
        'X_val': X_val,
        'X_test': X_test,
        'y_train': data.get('y_train'),
        'y_val': data.get('y_val'),
        'y_test': data.get('y_test'),
        'imputer': imp
    }

# Imputation methods
def impute_mean(data, **params):
    """Mean imputation. Extra params are ignored safely."""
    logger.info("Imputing using mean")
    imp = SimpleImputer(strategy='mean')
    return apply_imputer(imp, data)

def impute_median(data, **params):
    """Median imputation. Extra params are ignored safely."""
    logger.info("Imputing using median")
    imp = SimpleImputer(strategy='median')
    return apply_imputer(imp, data)

def impute_knn(data, n_neighbors=5, **params):
    """
    KNN imputation with optional mapping from a normalized param in [0,1]
    to integer neighbors in [1, 10].

    Priority:
    1) If explicit n_neighbors is provided in params, use it.
    2) Else if 'param' provided, map p->[1..10].
    3) Else use the function argument default (n_neighbors).
    """
    # 1) Explicit override via params
    if 'n_neighbors' in params:
        try:
            n_neighbors = int(params['n_neighbors'])
        except Exception:
            pass

    # 2) Map from normalized 'param' if provided and n_neighbors not explicitly set
    if 'param' in params and 'n_neighbors' not in params:
        try:
            p = float(params['param'])
            # clip to [0,1]
            p = 0.0 if p < 0 else (1.0 if p > 1 else p)
            # Map to 1..10 (inclusive)
            n_neighbors = 1 + int(round(p * 9))
        except Exception:
            # keep existing n_neighbors if mapping fails
            pass

    # Safety clamp
    n_neighbors = max(1, int(n_neighbors))

    logger.info("Imputing using KNN, neighbors=%d%s",
                n_neighbors,
                f" (mapped from param={params.get('param'):.3f})" if 'param' in params and 'n_neighbors' not in params else "")
    imp = KNNImputer(n_neighbors=n_neighbors)
    return apply_imputer(imp, data)

def impute_none(data, **params):
    """No imputation. Extra params are ignored safely."""
    logger.info("No imputation performed")
    return {
        'X_train': data['X_train'], 
        'X_val': data['X_val'], 
        'X_test': data.get('X_test'),
        'y_train': data.get('y_train'),
        'y_val': data.get('y_val'),
        'y_test': data.get('y_test')
    }



# N2: Feature Matrix Node éœ€è¦è®¾ç½®verboseå¼€å…³
def select_feature_columns(df, nan_thresh=0.5):
    exclude = ['material_id', 'structure', 'elements', 'formula_pretty', 'composition', TARGET_PROP]
    return [
        c for c in df.columns if c not in exclude and df[c].isna().mean() < nan_thresh
    ]

def numeric_feature_matrix(df, cols):
    return df[cols].apply(pd.to_numeric, errors='coerce')

def split_train_val(X, y, ratio=0.8):
    split_idx = int(ratio * len(X))
    return X.iloc[:split_idx], X.iloc[split_idx:], y[:split_idx], y[split_idx:]

def select_numeric_cols(df):
    return df.select_dtypes(include=[np.number]).columns

def feature_matrix(data, nan_thresh=0.5, train_val_ratio=0.8, verbose=True):
    train_df = data['train_data']
    test_df = data['test_data']

    feat_cols = select_feature_columns(train_df, nan_thresh)
    X_train_full = numeric_feature_matrix(train_df, feat_cols)
    X_test_full = numeric_feature_matrix(test_df, feat_cols) if len(test_df) > 0 else None

    y_full = train_df[TARGET_PROP].values
    X_train, X_val, y_train, y_val = split_train_val(X_train_full, y_full, train_val_ratio)
    X_test = X_test_full

    num_cols = select_numeric_cols(X_train)
    X_train = X_train[num_cols]
    X_val   = X_val[num_cols]
    X_test  = X_test[num_cols] if X_test is not None else None
    y_test  = test_df[TARGET_PROP].values if TARGET_PROP in test_df.columns else None

    if verbose:
        print(f"[N2] X_train shape: {X_train_full.shape}, NaN count: {X_train_full.isna().sum().sum()}")
        print("[DEBUG] feat_cols:", feat_cols)
        print("[DEBUG] X_train shape:", X_train.shape)
        print("[DEBUG] X_val shape:", X_val.shape)
        print("[DEBUG] X_train head:\n", X_train.head())
        print("[DEBUG] X_val head:\n", X_val.head())

    return {
        'X_train': X_train,
        'X_val': X_val,
        'X_test': X_test,
        'feature_names': list(num_cols),
        'y_train': y_train,
        'y_val': y_val,
        'y_test': y_test
    }


# Node 3: Feature selection | Node 3: Feature selection

def feature_selection(data, strategy='none', params=None):
    """
    ç‰¹å¾é€‰æ‹©å…¥å£å‡½æ•°ã€‚
    Feature selection entry function.
    """
    strategies = {
        'none': no_selection,
        'variance': variance_selection,
        'univariate': univariate_selection,
        'pca': pca_selection
    }
    if strategy not in strategies:
        raise ValueError(f"Unknown strategy: {strategy}")
    return strategies[strategy](data, **(params or {}))

def apply_selector(selector, data, y_required=False):
    """
    ç”¨äº N3/N4ï¼šç»Ÿä¸€ä½œç”¨äº train/val/testï¼Œé€‚é… selector/scalerã€‚
    Apply sklearn selector/scaler/transformer to train/val/test sets.

    Args:
        selector: æ‹Ÿåˆå¯¹è±¡ (selector/scaler/transformer)
        data: è¾“å…¥æ•°æ® dict
        y_required: æ˜¯å¦å¼ºåˆ¶ä¼  yï¼ˆæ¯”å¦‚ç‰¹å¾é€‰æ‹©ï¼‰

    Returns:
        X_train, X_val, X_test: å„ split å¤„ç†åçŸ©é˜µ
    """
    if y_required and data.get('y_train') is not None:
        X_train = selector.fit_transform(data['X_train'], data['y_train'])
    else:
        X_train = selector.fit_transform(data['X_train'])
    X_val   = selector.transform(data['X_val'])
    X_test  = selector.transform(data['X_test']) if data.get('X_test') is not None else None
    return X_train, X_val, X_test

def filter_feature_names(data, selector, prefix=''):
    """
    æ ¹æ®æ”¯æŒå‘é‡æˆ–ä¸»æˆåˆ†è‡ªåŠ¨ç”Ÿæˆæ–°çš„ç‰¹å¾åã€‚
    Auto-generate new feature names according to support mask or PCA components.
    """
    if hasattr(selector, 'get_support') and 'feature_names' in data:
        mask = selector.get_support()
        return [fname for fname, m in zip(data['feature_names'], mask) if m]
    elif prefix:
        num_comps = selector.n_components_ if hasattr(selector, 'n_components_') else selector.n_components
        return [f"{prefix}{i+1}" for i in range(num_comps)]
    else:
        return data.get('feature_names')

def no_selection(data, **params):
    """
    ä¸è¿›è¡Œä»»ä½•ç‰¹å¾é€‰æ‹©ï¼Œç›´æ¥è¿”å›ã€‚
    No feature selection, just return the original features.
    """
    return {
        'X_train': data['X_train'],
        'X_val': data['X_val'],
        'X_test': data.get('X_test'),
        'feature_names': data.get('feature_names'),
        'y_train': data.get('y_train'), 'y_val': data.get('y_val'), 'y_test': data.get('y_test')
    }

def variance_selection(data, var_ratio=0.01, **params):
    """
    æ–¹å·®é˜ˆå€¼ç‰¹å¾é€‰æ‹©ã€‚
    Feature selection by variance threshold.
    """
    if np.isnan(data['X_train']).any():
        raise RuntimeError('X_train contains NaN before feature selection!')
    threshold = params.get('param', var_ratio)
    if isinstance(threshold, float) and threshold < 0:
        threshold = 0.0
    selector = VarianceThreshold(threshold=threshold)
    X_train, X_val, X_test = apply_selector(selector, data)
    np.save(os.path.join(PROC_DIR, "selector_mask.npy"), selector.get_support())
    new_feature_names = filter_feature_names(data, selector)
    return {
        'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
        'y_train': data.get('y_train'), 'y_val': data.get('y_val'), 'y_test': data.get('y_test'),
        'selector': selector, 'feature_names': new_feature_names
    }

# æœ‰ç›‘ç£å­¦ä¹ ï¼Œéœ€è¦ç”¨åˆ°y | Supervised learning, requires the use of y
def univariate_selection(data, k=20, **params):
    """
    å•å˜é‡ç‰¹å¾é€‰æ‹© (Fæ£€éªŒ, ç›¸å…³æ€§è¯„åˆ†)ã€‚
    Univariate feature selection (F-regression, correlation scores).
    """
    if np.isnan(data['X_train']).any():
        raise RuntimeError('X_train contains NaN before feature selection!')
    X_tr = data['X_train']
    X_tr_arr = X_tr if isinstance(X_tr, np.ndarray) else np.array(X_tr)
    y_tr = data.get('y_train')
    if 'param' in params:
        p = params['param']
        num_feats = X_tr_arr.shape[1]
        k_val = max(1, int(round(p * (num_feats - 1))) + 1)
    else:
        k_val = k
    selector = SelectKBest(score_func=f_regression, k=min(k_val, X_tr_arr.shape[1]))
    # ç»Ÿä¸€é€šè¿‡ apply_selector è°ƒç”¨ï¼Œå¹¶æŒ‡å®š y_required=True
    X_train, X_val, X_test = apply_selector(
        selector,
        {
            'X_train': X_tr_arr,
            'X_val': np.array(data['X_val']),
            'X_test': np.array(data['X_test']) if data.get('X_test') is not None else None,
            'y_train': y_tr
        },
        y_required=True  # å…³é”®ï¼select_univariate å¿…é¡»ä¼  y
    )
    new_feature_names = filter_feature_names(data, selector)
    return {
        'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
        'y_train': data.get('y_train'), 'y_val': data.get('y_val'), 'y_test': data.get('y_test'),
        'selector': selector, 'feature_names': new_feature_names
    }

def pca_selection(data, n_components=0.95, **params):
    """
    ä¸»æˆåˆ†åˆ†æ (PCA) ç‰¹å¾é™ç»´ã€‚
    Principal component analysis (PCA) for feature reduction.
    """
    if np.isnan(data['X_train']).any():
        raise RuntimeError('X_train contains NaN before feature selection!')
    X_tr = data['X_train']
    X_tr_arr = X_tr if isinstance(X_tr, np.ndarray) else np.array(X_tr)
    if 'param' in params:
        p = params['param']
        if p <= 0:
            comp_val = 0.01
        elif p <= 1:
            comp_val = p
        else:
            comp_val = int(p)
    else:
        comp_val = n_components
    pca = PCA(n_components=comp_val)
    X_train = pca.fit_transform(X_tr_arr)
    X_val   = pca.transform(np.array(data['X_val']))
    X_test  = pca.transform(np.array(data['X_test'])) if data.get('X_test') is not None else None
    new_feature_names = filter_feature_names(data, pca, prefix='PC')
    return {
        'X_train': X_train, 'X_val': X_val, 'X_test': X_test,
        'y_train': data.get('y_train'), 'y_val': data.get('y_val'), 'y_test': data.get('y_test'),
        'selector': pca, 'feature_names': new_feature_names
    }

# Node 4 : Feature Scaling

# å…¨å±€å”¯ä¸€ apply_selector å·²åœ¨æ­¤æ–‡ä»¶å®šä¹‰
# def apply_selector(selector, data, y_required=False): ...

def scale_features(data, strategy='standard', params=None):
    """
    ç‰¹å¾ç¼©æ”¾/æ ‡å‡†åŒ–ç»Ÿä¸€å…¥å£ã€‚
    Unified scaling interface for pipeline.
    """
    strategies = {
        'none': scale_none,
        'standard': scale_standard,
        'robust': scale_robust,
        'minmax': scale_minmax
    }
    if strategy not in strategies:
        raise ValueError(f"Unknown scaling strategy: {strategy}")
    return strategies[strategy](data, **(params or {}))

def scale_standard(data, **params):
    """
    æ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰
    Standardization (StandardScaler)
    """
    scaler = StandardScaler()
    X_train, X_val, X_test = apply_selector(scaler, data)
    joblib.dump(scaler, os.path.join(MODEL_DIR, "scaler.joblib"))
    return {
        'X_train': X_train,
        'X_val': X_val,
        'X_test': X_test,
        'y_train': data.get('y_train'),
        'y_val': data.get('y_val'),
        'y_test': data.get('y_test'),
        'scaler': scaler
    }

def scale_robust(data, **params):
    """
    ç¨³å¥ç¼©æ”¾ï¼ˆRobustScalerï¼‰
    Robust scaling (RobustScaler)
    """
    X_train, X_val, X_test = apply_selector(RobustScaler(), data)
    joblib.dump(RobustScaler(), os.path.join(MODEL_DIR, "scaler_robust.joblib"))
    return {
        'X_train': X_train,
        'X_val': X_val,
        'X_test': X_test,
        'y_train': data.get('y_train'),
        'y_val': data.get('y_val'),
        'y_test': data.get('y_test'),
        'scaler': RobustScaler()
    }

def scale_minmax(data, **params):
    """
    MinMaxæ ‡å‡†åŒ–ï¼ˆMinMaxScalerï¼‰
    Min-Max scaling (MinMaxScaler)
    """
    X_train, X_val, X_test = apply_selector(MinMaxScaler(), data)
    joblib.dump(MinMaxScaler(), os.path.join(MODEL_DIR, "scaler_minmax.joblib"))
    return {
        'X_train': X_train,
        'X_val': X_val,
        'X_test': X_test,
        'y_train': data.get('y_train'),
        'y_val': data.get('y_val'),
        'y_test': data.get('y_test'),
        'scaler': MinMaxScaler()
    }

def scale_none(data, **params):
    """
    ä¸è¿›è¡Œç¼©æ”¾ã€‚
    No scaling (identity transform)
    """
    return {
        'X_train': data['X_train'],
        'X_val': data['X_val'],
        'X_test': data.get('X_test'),
        'y_train': data.get('y_train'),
        'y_val': data.get('y_val'),
        'y_test': data.get('y_test')
    }


# ========================= è¾…åŠ©å‡½æ•° / Helper Functions =========================

def prepare_node_input(node_key: str, state: dict, verbose: bool = False) -> dict:
    """
    ä¸ºæŒ‡å®šèŠ‚ç‚¹å‡†å¤‡è¾“å…¥æ•°æ®ã€‚
    Prepare input data for the specified node.
    """
    if verbose:
        print(f"\n[èŠ‚ç‚¹ {node_key}] è¾“å…¥ keys: {list(state.keys())}")
        if state.get('X_train') is not None:
            print(f"[èŠ‚ç‚¹ {node_key}] X_train shape: {state['X_train'].shape}")
        if state.get('y_train') is not None:
            print(f"[èŠ‚ç‚¹ {node_key}] y_train preview: {state['y_train'][:5]}")

    if node_key == 'N0':
        # N0 æ•°æ®è·å–èŠ‚ç‚¹ä¸éœ€è¦è¾“å…¥ / N0 data fetch node needs no input
        return {}
    
    elif node_key == 'N2':
        # N2 ç‰¹å¾çŸ©é˜µèŠ‚ç‚¹éœ€è¦åŸå§‹æ•°æ® / N2 feature matrix node needs raw data
        required_keys = ['train_df', 'test_df']
        validate_state_keys(state, required_keys, node_key)
        return {
            'train_data': state['train_df'],
            'test_data': state['test_df']
        }

    elif node_key == 'N1':
        # N1 ç¼ºå¤±å€¼å¤„ç†èŠ‚ç‚¹éœ€è¦ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾
        required_keys = ['X_train', 'X_val', 'y_train', 'y_val']
        validate_state_keys(state, required_keys, node_key)
        return {
            'X_train': state['X_train'],
            'X_val': state['X_val'],
            'X_test': state.get('X_test'),
            'y_train': state.get('y_train'),
            'y_val': state.get('y_val'),
            'y_test': state.get('y_test')
        }

    elif node_key in ('N3', 'N4'):
        # N3/N4 ç‰¹å¾é€‰æ‹©ä¸ç¼©æ”¾ï¼Œä¼ é€’å…¨éƒ¨ X/y
        required_keys = ['X_train', 'X_val','X_test', 'y_train', 'y_val','y_test']
        validate_state_keys(state, required_keys, node_key)
        X_train = state['X_train']
        X_val = state['X_val']
        y_train = state['y_train']
        y_val = state['y_val']
        # å¤„ç†ç¼ºå¤±å€¼æ ·æœ¬ï¼ˆå¦‚æœ‰ï¼‰
        bad_train = np.any(pd.isna(X_train), axis=1)
        bad_val = np.any(pd.isna(X_val), axis=1)
        train_idx = np.where(bad_train)[0]
        val_idx = np.where(bad_val)[0]
        if len(train_idx) > 0 or len(val_idx) > 0:
            if verbose:
                print(f" before {node_key}: drop {len(train_idx)} train / {len(val_idx)} val samples due to missing values")
            X_train = np.delete(X_train, train_idx, axis=0)
            y_train = np.delete(y_train, train_idx, axis=0)
            X_val = np.delete(X_val, val_idx, axis=0)
            y_val = np.delete(y_val, val_idx, axis=0)
        return {
            'X_train': X_train,
            'X_val': X_val,
            'X_test': state.get('X_test'),
            'y_train': y_train,
            'y_val': y_val,
            'y_test': state.get('y_test'),
            'feature_names': state.get('feature_names'),
        }

    elif node_key == 'N5':
        # N5 æ¨¡å‹è®­ç»ƒèŠ‚ç‚¹ï¼šç¡®ä¿æ ‡ç­¾å·²åˆ†å‰² / N5 training node: ensure labels are split
        if 'y_train' not in state:
            if verbose:
                print("    ğŸ“ åˆ†å‰²æ ‡ç­¾æ•°æ® / Splitting label data...")
            split_labels(state)
        return {
            'X_train': state.get('X_train'),
            'X_val': state.get('X_val'),
            'X_test': state.get('X_test'),
            'y_train': state.get('y_train'),
            'y_val': state.get('y_val'),
            'y_test': state.get('y_test'),
            'feature_names': state.get('feature_names')
        }

    else:
        # å…¶ä»–èŠ‚ç‚¹ä½¿ç”¨å®Œæ•´çŠ¶æ€ / Other nodes use full state
        return state


def validate_state_keys(state: Dict[str, Any], required_keys: list, node_key: str) -> None:
    """
    éªŒè¯çŠ¶æ€å­—å…¸ä¸­æ˜¯å¦åŒ…å«å¿…éœ€çš„é”®ã€‚
    Validate that the state dict contains required keys.
    """
    missing_keys = [key for key in required_keys if key not in state]
    if missing_keys:
        raise KeyError(f"èŠ‚ç‚¹ {node_key} ç¼ºå°‘å¿…éœ€çš„çŠ¶æ€é”®: {missing_keys} / Node {node_key} missing required state keys: {missing_keys}")


def split_labels(state: Dict[str, Any]) -> None:
    """
    åˆ†å‰²æ ‡ç­¾æ•°æ®ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚
    Split label data into train and validation sets.
    """
    if 'train_df' not in state:
        raise KeyError("Stateä¸­ç¼ºå°‘train_df / Missing train_df in state")
    y_full = state['train_df'][TARGET_PROP].values
    split_idx = int(0.8 * len(y_full))
    state['y_train'], state['y_val'] = y_full[:split_idx], y_full[split_idx:]
    # å¦‚æœæœ‰æµ‹è¯•é›†ï¼Œä¹Ÿæå–æµ‹è¯•æ ‡ç­¾ / If test set exists, extract test labels too
    if state.get('test_df') is not None:
        state['y_test'] = state['test_df'][TARGET_PROP].values


def update_state(node_key: str,
                 node_output: Dict[str, Any],
                 state: Dict[str, Any],
                 verbose: bool = False) -> None:
    """Merge node output and safeguard state consistency."""
    # ---------- 0) é€šç”¨ï¼šåˆå¹¶è¾“å‡º ----------
    state.update(node_output or {})

    # ---------- 1) N0: æ•°æ®æŠ“å–èŠ‚ç‚¹ ----------
    if node_key == 'N0':
        # 1.1 å–å‡ºä¸‰ä»½ DataFrame
        train_df = node_output.get('train_data')
        test_df  = node_output.get('test_data')
        full_df  = node_output.get('full_data')

        # 1.2 ä¸¥æ ¼æ ¡éªŒ
        for name, df_ in [('train_data', train_df),
                          ('test_data',  test_df),
                          ('full_data',  full_df)]:
            if df_ is None or not isinstance(df_, pd.DataFrame):
                raise RuntimeError(f"N0 è¾“å‡ºç¼ºå°‘æœ‰æ•ˆçš„ {name}")

        # 1.3 å†™å› state
        state['train_df'] = train_df
        state['test_df']  = test_df
        state['full_df']  = full_df

        # 1.4 âš ï¸ æ¸…ç†å ä½ y_*ï¼ˆé¿å… len(None)ï¼‰
        for k in ('y_train', 'y_val', 'y_test',
                  'X_train', 'X_val', 'X_test'):   # â† åŠ ä¸Šè¿™ä¸‰é¡¹
            if state.get(k) is None:
                state.pop(k, None)
    # ---------- 2) N2: ç‰¹å¾çŸ©é˜µèŠ‚ç‚¹ ----------
    elif node_key == 'N2':
        if node_output.get('y_train') is None or node_output.get('y_val') is None:
            raise RuntimeError('N2 è¾“å‡ºç¼ºå°‘ y_train æˆ– y_val')
    
    elif node_key in ('N1', 'N3', 'N4'):
        #  ç¡®è®¤ä¸­é—´èŠ‚ç‚¹è¾“å‡ºåŒ…å«å®Œæ•´çš„è®­ç»ƒ/éªŒè¯ç‰¹å¾å’Œæ ‡ç­¾ / Validate intermediate node outputs
        if node_output.get('X_train') is None or node_output.get('X_val') is None:
            raise RuntimeError(f"{node_key} è¾“å‡ºç¼ºå°‘ X_train æˆ– X_val")
        if node_output.get('y_train') is None or node_output.get('y_val') is None:
            raise RuntimeError(f"{node_key} è¾“å‡ºç¼ºå°‘ y_train æˆ– y_val")

    # ---------- 3) N5: è®­ç»ƒèŠ‚ç‚¹ ----------
    elif node_key == 'N5':
        if 'y_val_pred' not in node_output:
            raise RuntimeError('N5 è¾“å‡ºç¼ºå°‘/ is lack of  y_val_pred')
 

